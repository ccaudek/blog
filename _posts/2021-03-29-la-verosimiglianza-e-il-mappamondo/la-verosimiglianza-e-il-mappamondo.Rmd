---
title: "La verosimiglianza e il pettirosso"
description: |
  La funzione di verosimiglianza spiegata da Richard McElreath.
author:
  - name: Corrado Caudek
    url: https://caudekblog.netlify.app
date: 03-29-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 2
draft: false
creative_commons: CC BY
categories:
  - R
  - Psicometria
preview: preview.png
bibliography: bibliography.bib
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Di solito, quando parliamo di distribuzioni di probabilità, assumiamo di conoscere i valori dei parametri. Nel mondo reale, però, di solito è il contrario.  Quello che abbiamo sono dei dati. I parametri sono quello che vogliamo sapere.

Iniziamo con un esempio. Supponiamo di studiare i pettirossi. Sappiamo che, tra fine aprile e inizio maggio, la femmina di pettirosso depone all'interno del nido quattro o sei uova. Dopo la schiusa, ai piccoli bastano 15/20 giorni per essere in grado di volare e abbandonare il nido. Ma solo pochi sopravvivono a causa dell'alta mortalità infantile.

```{r FigureBF, echo=FALSE, fig.height=3, fig.width=6, fig.cap="", fig.align="center"}
knitr::include_graphics("pettirosso.jpg", dpi = 20)
```

Immaginiamo di avere osservato una femmina di pettirosso che cova cinque uova. Dei cinque neonati tre sopravvivono e prendono il volo 14 giorni dopo la schiusa delle uova. 

Cosa ci dicono questi dati della probabilità di un pettirosso neonato di sopravvivere, per questa particolare femmina?  Cosa ci dicono questi dati della probabilità di sopravvivenza nella popolazione dei pettirossi neonati?

Non conosciamo la probabilità di sopravvivenza dei pettirossi neonati nella popolazione. Ma possiamo calcolare la _verosimiglianza_ di ottenere i dati che abbiamo osservato per valori diversi del parametro sconosciuto _p_ (la probabilità di sopravvivenza):

```{r}
dbinom(x=3, size=5, prob=0.1)
```

Se cambiamo il valore del parametro _p_ la verosimiglianza dei dati cambia:

```{r}
dbinom(x=3, size=5, prob=0.4)
```

```{r}
dbinom(x=3, size=5, prob=0.9)
```

Dai calcoli precedenti vediamo che è più plausibile osservare 3 "successi" in 5 prove se la probabilità di successo è 0.4 piuttosto che 0.1 o 0.9.

Questo modo di ragionare è così comune in statistica che ha un nome speciale: si chiama __verosimiglianza__. La verosimiglianza descrive la plausibilità di osservare i dati al variare dei valori del parametro di un modello statistico. Ciò ci conduce alla seguente definizione.


# Definizione

> La funzione di verosimiglianza è la funzione di massa o di densità di probabilità dei dati vista come una funzione dei parametri sconosciuti.


# Un esempio

Per capire meglio cosa significa la definizione precedente ci focalizzeremo sul caso di un'unico parametro sconosciuto (come nell'esempio precedente del pettirosso) e prenderemo in esame la distribuzione Binomiale. Inizio caricando i pacchetti che userò in questo tutorial.

```{r}
library("tidyverse", warn.conflicts = FALSE)
library("bayesplot")
```

Nella discussione seguente non ci occuperemo più di pettirossi ma esamineremo un esempio discusso da @mcelreath2020statistical. Supponiamo di tenere in mano un mappamondo gonfiabile e di chiederci: "qual’è la proporzione della superficie terreste ricoperta d’acqua?" Sembra una domanda a cui è difficile rispondere. Ma ci viene in mente questa idea brillante: lanciamo in aria il mappamondo e, quando lo riprendiamo, osserviamo se la superfice del mappamondo sotto il nostro dito indice destro rappresenta acqua o terra. Possiamo ripetere questa procedura più volte, così da ottenere un campione causale di diverse porzioni della superficie dal mappamondo. Eseguiamo il nostro esperimento lanciando in aria il mappamondo nove volte e osserviamo i seguenti risultati: A, T, A, A, A, T, A, T, A, dove “A” indica acqua e “T” indica terra. 

# Modello statistico

Qual è il modello statistico che potrebbe avere generato i dati che abbiamo osservato? Per l’esempio del mappamondo possiamo dire quanto segue:

- la proporzione del pianeta Terra ricoperta d’acqua è _p_;
- un singolo lancio del mappamondo ha una probabilità _p_ di produrre l’osservazione “acqua” (A);
- i lanci del mappamondo sono indipendenti (nel senso che il risultato di un lancio non influenza i risultati degli altri lanci).

Le caratteristiche descritte sopra definiscono il processo generativo dei dati che sta alla base della distribuzione Binomiale. Nel caso presente, il parametro sconosciuto è _p_.

Sappiamo dunque che legge Binomiale illustra la relazione tra i dati osservati e il parametri sconosciuto _p_:

$$
p(x \mid p) = \binom{n}{k} p^k (1-p)^{n-k}
$$

# Coefficiente binomiale

Il coefficiente binomiale è

$$
\binom{n}{k} = \frac{n!}{k! (n-k)!}
$$
In `R` la funzione che restituisce come risultato il fattoriale di $a$ è
`factorial()`. Per esempio:

```{r}
factorial(3)
```

Ma il risultato del coefficiente binomiale viene calcolato direttamente dalla funzione `choose()`. Per esempio, consideriamo il caso in cui _n_ = 9 e _k_ = 6:

```{r}
choose(9, 6)
```
ovvero

```{r}
factorial(9) / (factorial(6) * factorial(3))
```

Dunque, possiamo implementare la legge binomiale in una funzione `R` nel modo seguente:

```{r}
binomial <- function(k, n, p) {
  choose(n, k) * p^k * (1 - p)^(n - k)
}
```

Ovviamemente, tale funzione è già presente in `R` ed è data da `dbinom()`. Controlliamo:

```{r}
binomial(6, 9, 0.2)
```

```{r}
dbinom(6, 9, 0.2)
```


# La funzione di verosimiglianza

Adesso poniamoci il problema di capire come si crea la figura che ho preso dal seguente [script](https://bookdown.org/content/4857/small-worlds-and-large-worlds.html#building-a-model) la quale implementa in `R` la figura riportata da McElreath (2020).

Inziamo a definire un vettore che contiene i dati:

```{r}
d <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w"))
d <- d %>% 
  mutate(n_trials  = 1:9,
         n_success = cumsum(toss == "w"))
d
```

```{r}
sequence_length <- 50

d %>%
  expand(
    nesting(
      n_trials, toss, n_success
    ),
    p_water = seq(
      from = 0, to = 1,
      length.out = sequence_length
    )
  ) %>%
  group_by(p_water) %>%
  mutate(
    lagged_n_trials = lag(n_trials, k = 1),
    lagged_n_success = lag(n_success, k = 1)
  ) %>%
  ungroup() %>%
  mutate(
    prior = ifelse(
      n_trials == 1, .5,
      dbinom(
        x = lagged_n_success,
        size = lagged_n_trials,
        prob = p_water
      )
    ),
    likelihood = dbinom(
      x = n_success,
      size = n_trials,
      prob = p_water
    ),
    strip = str_c("n = ", n_trials)
  ) %>%
  # the next three lines allow us to normalize the prior and the likelihood,
  # putting them both in a probability metric
  group_by(n_trials) %>%
  mutate(
    prior = prior / sum(prior),
    likelihood = likelihood / sum(likelihood)
  ) %>%
  # plot!
  ggplot(aes(x = p_water)) +
  geom_line(aes(y = prior),
    linetype = 2
  ) +
  geom_line(aes(y = likelihood)) +
  scale_x_continuous("proportion water", breaks = c(0, .5, 1)) +
  scale_y_continuous("plausibility", breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~strip, scales = "free_y")
```

La curva rappresentata in ciascun pannello della figura precedente è la funzione di verosimiglianza calcolata utilizzando la legge Binomiale, ma considierando campioni diversi di dati. Nel pannello _n = 1_ è rappresentata la verosimiglianza dei possibili valori del parametro sconosciuto _p_ se abbiamo osservato solo un successo in 1 prova dell'esperimento casuale. Nel secondo pannello, _n = 2_ è rappresentata la verosimiglianza che si ottiene avendo osservato un successo e un insuccesso in due prove successive dell'esperimento casuale. E così via.

Iniziamo dal pannello _n = 1_. 

La definizione della funzione di verosimiglianza ci dice che dobbiamo usare la legge Binomiale __tenendo costanti i dati__. Questo significa che __dobbiamo variare__ i valori del parametro _p_. Quali valori può assumere _p_?  Tutti i valori nell'intervallo [0, 1]. Ne considereremo qui 50:

```{r}
p <- seq(0, 1, length.out = 50)
p
```

Quali sono i nostri dati?  Se abbiamo ottenuto "acqua" avendo lanciato una volta in aria il mappadondo, allora i nostri dati sono: _k_ = 1, con _n_ = 1. Inseriamo dunque tali dati nella formula della Binomiale:

```{r}
l <- binomial(1, 1, p)
l
```

Creaimo un diagramma con il risultato ottenuto.

```{r}
like <- data.frame(
  l = l
)
like %>% 
  ggplot(aes(x = p)) +
  geom_line(aes(y = l),
    linetype = 1
  ) +
  labs(
    y = "plausibility",
    x = "proportion water"
  ) +
  theme(panel.grid = element_blank()) 
```

La figura ci dice che, avendo osservato un successo in una prova, la plausibilità che _p_ sia uguale a 1 è massima.  La plausibilità di _p_ diminuisce allonandoci dal valore possibile _p_ = 1, fino ad arrivare al valore minimo, ovvero 0, in corrispondenza dell'ipotesi secondo la quale _p_ = 0. È infatti impossibile che _p_ = 0 dato che abbiamo osservato un successo. Quindi deve essere vero che _p_ $>$ 0.

Adesso lanciamo il mappamondo una seconda volta.  Osserviamo "terra".  Ripetiamo la procedura descritta sopra.

```{r}
l <- binomial(1, 2, p)
like <- data.frame(
  l = l
)
like %>% 
  ggplot(aes(x = p)) +
  geom_line(aes(y = l),
    linetype = 1
  ) +
  labs(
    y = "plausibility",
    x = "proportion water"
  ) +
  theme(panel.grid = element_blank()) 
```

La figura ci dice che ci sono due valori del parametro _p_ che sono impossibili: _p_ = 0, perché abbiamo osservato un successo e _p_ = 1, in quanto abbiamo osservato un insuccesso. In tali condizioni, è inuitivo che il valore _p_ più plausibile sia nell'intorno di 0.5. Infatti, la figura ci dice proprio questo.

Evidentemente non abbiamo bisogno di usare la nostra funzione `binomial()` per ottenere i risultati descritti sopra. Gli stessi risultati si ottengono con la funzione `dbinom()`:

```{r}
l <- dbinom(1, 2, p)
like <- data.frame(
  l = l
)
like %>% 
  ggplot(aes(x = p)) +
  geom_line(aes(y = l),
    linetype = 1
  ) +
  labs(
    y = "plausibility",
    x = "proportion water"
  ) +
  theme(panel.grid = element_blank()) 
```

Ripetendo questo ragionamento, ovvero ripercorrendo i vari passi dell'esempio fino ad ottenere 6 successi in 9 prove, riusciamo a costruire la figura con nove pannelli riportata sopra.


# Verosimiglianza per una Normale

Poniamoci ora il problema di calcolare la verosimiglianza per i parametri sconosciuti di una distribuzione Normale avendo a disposizione un campione casuale di osservazioni. 

Iniziamo con il ricordare cosa significano le nozioni di "probabilità" e "densità di probabilità" nel caso di una variabile aleatoria continua.
La _densità di probabilità_ può essere intesa come una misura di _probabilità relativa_, nel senso che valori della variabile aleatoria in intervalli a cui sono associate probabilità maggiori avranno anche densità di probabilità maggiori. In termini formali si dice che la probabilità è l'integrale della densità di probabilità in un intervallo. Per esempio, la classica curva campanulare associata alla distribuzione Normale è una misura della _densità di probabilità_, mentre la _probabilità_ corrisponde all'area sottesa alla curva in un determinato intervallo di valori della variabile aleatoria.

Se noi assegnamo un modello statistico ad una variabile aleatoria, allora ciascuna realizzazione della variabile aleatoria (chiamiamola $x_i$) avrà una densità di probabilità definita dal modello probabilistico (chiamiamola $f(x_i)$). Se assumiamo che i valori del nostro campione sono statisticamente indipendenti (ovvero, se la probabilità di osservare un certo valore del nostro campione non dipende dagli altri valori che fanno parte del campione), allora la __verosimiglianza__ di osservare l'intero campione (chiamiamola $L(x)$) è definita come il prodotto delle densità di probabilità di ciascuno dei valori del campione, ovvero 

$$
L(x) = \prod_{i=1}^n f(x_i),
$$
dove $n$ è l'ampiezza del campione.

In base al modello statistico che abbiamo scelto, ovvero quello della distribuzione Normale, la densità di una singola osservazione è data da

$$
 f(x) = \frac{1}{\sigma\sqrt{2\pi}} 
  \exp\left( -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{\!2}\,\right).
$$

La legge Normale è implementata in `R` nell'istruzione seguente: 

```{r, eval=FALSE}
dnorm(x, mean = mu, sigma = sigma)
```

La verosimiglianza del campione sarà dunque uguale a

$$
L(x) = \prod_{i=1}^n \frac{1}{\sigma\sqrt{2\pi}} 
  \exp\left( -\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{\!2}\,\right).
$$

Il problema di procedere in questo modo, però, è che il risultato numerico di questo prodotto diventa molto piccolo, e i computer hanno difficoltà a manipolare numeri così piccoli. Tale problema può essere risolto prendendo il logaritmo. Sappiao che, per le proprietà dei logaritmi, il logaritmo di un prodotto è uguale alla somma dei logaritmi. Per esempio,

```{r}
log(3 * 4 * 5)
log(3) + log(4) + log(5)
```

Dunque, se vogliamo costruire la funzione di log-verosimiglianza per un campione di $n$ osservazioni sarà sufficiente sommare le $n$ log-verosimiglianze:

$$
log\big(L(x)\big) = \sum_{i=1}^n log \big(f(x_i) \big).
$$

In `R`, il logaritmo della densità di probabilità Normale è dato da:

```{r, eval=FALSE}
dnorm(x, mean = mu, sigma = sigma, log = TRUE)
```

L'argomento `log=TRUE` nella funzione `dnorm()` specifica appunto che vogliamo il logaritmo della densità Normale. 

Faciamo un esempio e consideriamo i dati discussi nelle dispense:

```{r}
d <- data.frame(
  x = c(26, 35, 30, 25, 44, 30, 33, 43, 22, 43, 24, 19, 39, 31, 25, 
        28, 35, 30, 26, 31, 41, 36, 26, 35, 33, 28, 27, 34, 27, 22)
  )
```

Per semplificare il problema, poniamoci l'obiettivo di stimare solo uno dei due parametri sconosciuti della distribuzione Normale, ovvero $\mu$, tenendo $\sigma$ uguale alla deviazione standard del campione:

```{r}
true_sigma <- sd(d$x)
```

Definiamo in `R` il logaritomo della funzione di verosimiglianza, ovvero la funzione di log-verosimiglianza:

```{r}
log_likelihood <- function(x, mu, sigma=true_sigma) {
  sum(dnorm(x, mu, sigma, log=TRUE))
}
```

La situazione è dunque simile a quella che abbiamo discusso nel caso della Binomiale.  La complicazione è che dobbiamo fare una somma di 30 addendi, ma tale somma viene calcolata all'interno della funzione `log_likelihood()`.

Notiamo che della funzione `log_likelihood()` richiede tre argomenti: il primo argomento `x` corrisponde al vettore che contiene i dati; il secondo argomento `mu` è il parametro sconosciuto; il terzo argomento, $\sigma$, nell'esempio che stiamo discutendo viene mantenuto costante.

Come abbiamo fatto nel caso della verosimiglianza di una Binomiale, definiamo una serie di valori per il parametro sconosciuto `mu`:

```{r}
nrep <- 1e5
mu <- seq(
  mean(d$x) - sd(d$x), 
  mean(d$x) + sd(d$x), 
  length.out = nrep
)
```

Per l'esempio presente esamineremo un grande numero di valori possibili per il parametro sconosciuto $\mu$, ovvero 1e5.

Siamo ora nelle condizioni di trovare i valori della log-verosimiglianza:

```{r}
ll <- rep(NA, nrep)
for (i in 1:nrep) {
  ll[i] <- log_likelihood(d$x, mu[i], true_sigma)
}
```

Esaminiamo uno dei valori possibili della log-verosimiglianza. Consideriamo un valore qualsiasi del parametro _mu_, per esempio, 28, e svolgiamo i calcoli. Dobbiamo calcolare il valore che viene ritornato da ´dnorm(..., log=TRUE)´ per ciascuno dei valori `x` del campione, tenendo costanti i valori `sigma = s` e `mu = 28`:

```{r}
mu_test <- 28
dnorm(d$x, mu_test, true_sigma, log=TRUE)
```

La somma di questi valori è data da

```{r}
sum(dnorm(d$x, mu_test, true_sigma, log=TRUE))
```

Nello script precedente, questi calcoli sono stati svolti per tutti gli 1e5 valori ipotizzati per `mu`. Verifichiamo. Chiediamoci a quale indice del vettore `mu` corrisponde il valore di `mu_test` = 28.  Nel vettore che abbiamo creato, il valore più prossimo a 28 corrisponde all'elemento nella posizione 

```{r}
mu_index <- which.min(abs(mu - mu_test))
mu_index
```
ovvero 

```{r}
mu[mu_index]
```

Chiediamoci qual è il valore dell'elemento di indice pari a `mu_index` all'interno del vettore `ll`:

```{r}
ll[mu_index]
```

Tale valore è identico a quello trovato con i calcoli che abbiamo svolto in precedenza.


# La log-verosimiglianza

Disegnamo ora la funzione di log-verosimiglianza per il parametro sconosciuto $\mu$:

```{r}
data.frame(mu, ll) %>% 
ggplot(aes(x=mu, y=ll)) +
  geom_line() +
  vline_at(mean(d$x), color="red", linetype="dashed") +
  labs(
    y="Log-verosimiglianza",
    x=c("Parametro \u03BC")
  ) +
  theme(panel.grid = element_blank()) 
```

La funzione di log-verosimiglianza ha un massimo. Il valore $\mu$ corrispondente a tale massimo è lo stimatore di massima verosimiglianza per la media sconosciuta della popolazione. 

Sappiamo che tale stimatore non è altro che la media del campione.  Controlliamo:

```{r}
mu[which.max(ll)]
```

infatti

```{r}
mean(d$x)
```



