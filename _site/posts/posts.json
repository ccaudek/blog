[
  {
    "path": "posts/2021-04-09-intervalli-di-fiducia/",
    "title": "Intervalli di fiducia",
    "description": "La descrizione dell'intertezza in termini frequentisti.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-04-09",
    "categories": [],
    "contents": "\nL’approccio frequentista è basato sull’idea di probabilità quale frequenza relativa. Tale approccio immagina che un determinato esperimento casuale venga ripetuto infinite volte e si chiede come si distribuiscono i risultati ottenuti. Consideriamo qui, quale esempio, l’esperimento casuale che corrisponde all’estrazione casuale di un campione di \\(n\\) osservazioni da una popolazione e del calcolo della media di quel campione. Dato che si immagina che l’esperimento casuale venga ripetuto infinite volte, dobbiamo immaginare l’esistenza di infiniti campioni casuali di \\(n\\) osservazioni. Dato che ciascuno di tali campioni è costituito da osservazioni diverse, ognuno di essi avrà una media diversa. Tale fenomeno è detto variabilità campionaria. Se usiamo il linguaggio dell’approccio frequentista diremo che la distribuzione dei tutti gli infiniti possibili valori della statistica in questione (nel caso dell’esempio, la media del campione) nell’universo dei campioni si chiama .\nSe lo psicologo usa la media del campione quale stima della media della popolazione, ovviamente commetterà un errore, dato che la statistica campionaria è sempre diversa dal parametro. Il problema dello psicologo è quello di valutare l’entità di tale errore, ovvero quello di valutare il livello di incertezza inerente alla sua stima.\nPer valutare l’incertezza della stima lo psicologo fa riferimento alla variabilità campionaria. Se la deviazione standard della distribuzione campionaria della statistica è piccola, questo significa che ogni campione casuale di ampiezza \\(n\\) produrrà una statistica \\(\\bar{X}\\) simile al parametro \\(\\mu\\) della popolazione. In tali condizioni (utilizzando i dati di un singolo campione) ci sarà una piccola incertezza relativamente al valore del parametro, perché, in media, \\(\\bar{X}\\) è simile a \\(\\mu\\). Se invece la deviazione standard della distribuzione campionaria di \\(\\bar{X}\\) è grande, i campioni casuali di ampiezza \\(n\\) produrranno, in media, una statistica \\(\\bar{X}\\) molto lontana dal parametro \\(\\mu\\). Utilizzando i dati di un singolo campione, in tali condizioni lo psicologo sarà molto incerto relativamente al vero valore del parametro (in quanto, in media, \\(\\bar{X}\\) è molto diverso da \\(\\mu\\)).\nLa deviazione standard della distribuzione campionaria, detta errore standard, viene dunque utilizzata per quantificare l’incertezza relativamente alla stima di un parametro. Solitamente, l’approccio frequentista quantifica l’incertezza della stima nei termini di una funzione dell’errore standard chiamata intervallo fiduciale. Lo scopo di questo capitolo è quello di introdurre la nozione di errore standard in modo tale da potere fornire un’interpretazione alla nozione di intervallo fiduciale.\nL’errore standard\nIniziamo con un esempio e chiediamoci: com’è possibile misurare la variabilità della proporzione di studenti promossi, se prendiamo in considerazione tutti i possibili appelli d’esame di Psicometria a Firenze (quelli passati, quelli presenti e anche quelli futuri)? Sappiamo che la statistica più utile per quantificare la variabilità di una variabile è la deviazione standard. Questo fatto fornisce la risposta anche alla domanda che ci siamo posti ora: in linea di principio, potremmo usare la deviazione standard per descrivere di quanto variano, in media, i valori delle proporzioni di studenti promossi in tutti i possibili appelli d’esame di Psicometria a Firenze. Un problema che dobbiamo affrontare, però, riguarda il fatto che la distribuzione di valori a cui facciamo riferimento è una distribuzione di valori virtuali, non è un insieme di dati che abbiamo osservato. Per calcolare la deviazione standard, dunque, dobbiamo procedere in modo diverso da quanto abbiamo fatto in precedenza. Iniziamo con un po’ di terminologia. La stima della deviazione standard della distribuzione di una statistica campionaria (nell’esempio disciusso, la proporzione) viene detta errore standard. Queste considerazioni ci conducono alla seguente definizione.\n\nSi dice  la deviazione standard dei valori una statistica campionaria nell’universo dei campioni.\n\nIl calcolo dell’errore standard è solitamente lasciato ad un software.\nL’errore standard è molto importante perché descrive l’accuratezza della nostra stima. Se l’errore standard è piccolo questo ci dice che, se osserviamo un campione diverso da quello corrente, allora ci aspettiamo che la statistica in esame abbia un valore simile a quello corrente. Un grande errore standard, invece, ci dice che non dobbiamo assegnare troppa fiducia alla stima ottenuta nel campione a disposizione perché, in un altro campione, si otterrà una stima molto diversa, e, in media, i valori ottenuti in campioni diversi saranno lontani dal vero valore del parametro sconosciuto (ovvero, nell’esempio considerato, dalla media di tutte le proporzioni che si possono ottenere).\nIl calcolo dell’errore standard è solitamente lasciato ad un software. Ma come si arriva ad una quantificazione dell’errore standard? Forniamo qui solo una descrizione intuitiva della procedura che viene seguita e forniamo la seguente definizione.\n\nL’errore standard può essere inteso come una misura (del reciproco) della curvatura della verosimiglianza in corrispondenza della stima di massima verosimiglianza per un parametro \\(\\theta\\).\n\nLa prima delle due definizioni precedente descrive la logica soggiacente alla procedura di calcolo dell’errore standard, mentre la seconda definizione ci dice qual è il significato dell’errore standard.\nPer capire come si traduce in pratica la definizione precedente, esaminiamo la figura~. Nel pannello di sinistra è riprodotta la funzione di verosimiglianza nel caso di 7 successi in 10 prove Bernoulliane. Nel pannello centrale è riportata la verosimiglianza per 70 successi in 100 prove e nel pannello di destra abbiamo la verosimiglianza nel caso di 700 successi in  prove. Quello che la figura~ ci dice è che, al crescere del numero di prove, diminuisce la nostra incertezza relativamente al valore del parametro \\(\\pi\\) (probabilità di successo, ovvero, la media di tutte le proporzioni campionarie). Nel caso di un piccolo numero di prove, la verosimiglianza ha una piccola curvatura e ci fornisce una modesta quantità di informazione concernente il parametro non osservabile \\(\\pi\\) – in altri termini, la verosimiglianza definisce un intervallo piuttosto ampio di valori \\(\\pi\\) la cui plausibilità relativa è piuttosto grande. Con un grande numero di prove, invece, la verosimiglianza ha un  molto più marcato che associa livelli relativamente alti di plausibilità ad un intervallo molto più piccolo di valori \\(\\pi\\). In altre parole, maggiore è la curvatura della verosimiglianza, maggiore è la quantità di informazione che il campione fornisce rispetto al valore del parametro sconosciuto che vogliamo stimare.\nAd esempio, esaminiamo qui sotto la funzione di verosimiglianza nel caso di 7 successi in 10 prove Bernoulliane (pannello di sinistra), di 70 successi in 100 prove (pannello centrale) e di 700 successi in 1000 prove (pannello di destra).\n\n\nlibrary(\"tidyverse\")\nsuppressPackageStartupMessages(library(\"bayesplot\"))\ntheme_set(bayesplot::theme_default(base_size=16))\nsuppressPackageStartupMessages(library(\"patchwork\"))\nlibrary(\"ggExtra\")\n\nlog_likelihood <- function(x, mu, sigma = true_sigma) {\n  sum(dnorm(x, mu, sigma, log = TRUE))\n}\n\np <- seq(0, 1, length.out = 1e3)\n\nlike <- dbinom(7, 10, p)\n\np1 <- ggplot(\n  data.frame(p, like),\n  aes(x = p, y = like)\n) +\n  geom_line() +\n  vline_at(0.7, color = \"red\", linetype=\"dashed\") +\n  labs(\n    y = \"Verosimiglianza\",\n    x = c(\"\\u03C0\")\n  ) \n\nlike <- dbinom(70, 100, p)\n\np2 <- ggplot(\n  data.frame(p, like),\n  aes(x = p, y = like)\n) +\n  geom_line() +\n  vline_at(0.7, color = \"red\", linetype=\"dashed\") +\n  labs(\n    y = \"\",\n    x = c(\"\\u03C0\")\n  ) \n\nlike <- dbinom(700, 1000, p)\n\np3 <- ggplot(\n  data.frame(p, like),\n  aes(x = p, y = like)\n) +\n  geom_line() +\n  vline_at(0.7, color = \"red\", linetype=\"dashed\") +\n  labs(\n    y = \"\",\n    x = c(\"\\u03C0\")\n  ) \n\np1 + p2 + p3\n\n\n\n\nIn termini formali, la curvatura è la derivata seconda di una funzione e, appunto, calcolando la derivata seconda della funzione di verosimiglianza possiamo trovare l’errore standard di una statistica. Nel caso presente, l’errore standard della proporzione campionaria è\n\\[\n\\sigma_{\\hat{\\pi}} = \\sqrt{\n\\frac{p (1-p)}{n},\n}\n\\]\ndove \\(p\\) è la proporzione campionaria e \\(n\\) è il numero di osservazioni. Questa quantità si interpreta come qualunque deviazione standard: nello specifico, ci dice quanto varia in media la proporzione campionari se consideriamo campioni diversi. Si noti che, avendo \\(n\\) al denominatore, la formula riproduce l’intuizione che abbiamo descritto mediante la figura precedente: quando \\(n\\) è grande l’errore standard è piccolo e, viceversa, quando \\(n\\) è piccolo l’errore standard è grande. In altri termini, quando l’errore standard di una stima è piccolo, possiamo attribuire un grande livello di fiducia al valore della stima del parametro. Invece, un grande errore standard ci suggerisce ad essere cauti in qualunque inferenza che potremmo trarre dalla stima che abbiamo ottenuto.\nIntervallo di fiducia per la media\nI casi più frequenti nelle indagini svolte nell’ambito della psicologia riguardano i test relativi ad una singola media oppure al confronto tra le medie di due campioni. Iniziamo con il caso di una singola media.\nPopolazione con varianza nota\nSia \\(X_1,X_2\\dots,X_n\\) un campione casuale estratto da una popolazione di legge normale di media \\(\\mu\\) e varianza \\(\\sigma^2\\). La media campionaria, essendo una combinazione lineare di \\(n\\) variabili aleatorie normali, è anch’essa una variabile normale (si veda la ~): \\(\\bar{X} \\sim \\mathcal{N}(\\mu, \\sigma/n)\\). La media campionaria standardizzata\n\\[\n\\begin{equation}\n\\frac{\\bar{X} - \\mu}{\\sigma} \\sqrt{n}\\sim \\mathcal{N}(0, 1)\\notag\n\\end{equation}\n\\]\nsegue dunque una distribuzione normale con media zero e deviazione standard unitaria.\nFissato il livello fiduciario \\(\\gamma = 1 - \\alpha\\) (tipicamente 0.95, corrispondente a \\(\\alpha = 0.05\\)), indichiamo con \\(z\\) il quantile di ordine \\(1 - \\alpha/2\\) della distribuzione normale standard in modo che\n\\[\n\\begin{equation}\nP(-z \\leq Z \\leq z) = 1 - \\alpha.\\notag\n\\end{equation}\n\\]\nOtteniamo dunque\n\\[\n\\begin{equation}\nP\\bigg(-z \\leq \\frac{\\bar{X} - \\mu}{\\sigma} \\sqrt{n} \\leq z\\bigg) = 1 - \\alpha.\\notag\n\\end{equation}\n\\]\nApplicando qualche manipolazione algebrica, la diseguaglianza precedente si può scrivere nel modo seguente:\n\\[\n\\begin{align}\nP\\bigg(-z {\\frac{\\sigma}{\\sqrt{n}}} \\leq  \\bar{X} - \\mu \\leq z \\frac{\\sigma}{\\sqrt{n}}\\bigg) &= 1 - \\alpha\\notag\\\\\nP\\bigg(-\\bar{X}-z {\\frac{\\sigma}{\\sqrt{n}}} \\leq -\\mu \\leq -\\bar{X} + z \\frac{\\sigma}{\\sqrt{n}}\\bigg) &= 1 - \\alpha\\notag\\\\\nP\\bigg(\\bar{X}+z \\frac{\\sigma}{\\sqrt{n}} \\geq \\mu \\geq  \\bar{X} -z \\frac{\\sigma}{\\sqrt{n}}\\bigg) &= 1 - \\alpha.\\notag\n\\end{align}\n\\]\nSe definiamo\n\\[\n\\begin{equation}\n\\hat{a} \\triangleq \\bar{X}-z \\frac{\\sigma}{\\sqrt{n}},\n\\quad \\hat{b} \\triangleq \\bar{X} +z \\frac{\\sigma}{\\sqrt{n}},\n\\end{equation}\n\\]\navremo che\n\\[\n\\begin{equation}\nP(\\hat{a} \\leq \\mu \\leq \\hat{b}) = 1 - \\alpha.\\notag\n\\end{equation}\n\\]\nL’intervallo \\([\\hat{a}, \\hat{b}]\\) è detto intervallo di fiducia per una stima della media della popolazione al livello fiduciario \\(\\gamma = 1 -\\alpha\\).\nPopolazione con varianza incognita\nIn ogni applicazione concreta, lo sperimentatore estrae un solo campione \\(x_1, \\dots, x_n\\) dalla popolazione e la varianza \\(\\sigma^2\\), in aggiunta alla media \\(\\mu\\) da determinare, è sconosciuta. In tal caso, per effettuare una stima intervallare di \\(\\mu\\) ci si basa sulla densità \\(t\\) di Student. In base ad un Teorema che riguarda la distribuzione \\(t\\), possiamo scrivere\n\\[\n\\begin{equation}\nP\\bigg(-t^{\\ast} \\leq \\frac{\\bar{X} - \\mu}{s} \\sqrt{n} \\leq t^{\\ast}\\bigg) = 1 -\\alpha,\n\\end{equation}\n\\]\ndove \\(s\\) è lo stimatore non distorto di \\(\\sigma\\) e \\(t^{\\ast} \\triangleq t_{n-1,1-\\alpha/2}\\) è il quantile di ordine \\(1 - \\alpha/2\\) della distribuzione \\(t_{n-1}\\). Pertanto, il limite inferiore \\(\\hat{a}\\) e il limite superiore \\(\\hat{b}\\) dell’intervallo di fiducia diventano, rispettivamente, uguali a:\n\\[\n\\begin{equation}\n\\hat{a} \\triangleq \\bar{X} -t^{\\ast} \\frac{s}{\\sqrt{n}},\n\\quad \\hat{b} \\triangleq \\bar{X} + t^{\\ast} \\frac{s}{\\sqrt{n}}.\n\\end{equation}\n\\]\nSi noti che, nel caso di una popolazione con varianza incognita, i limiti fiduciari si ottengono dall’equazione~ sostituendo \\(\\sigma\\), ora incognito, con \\(s\\) (per una ampiezza campionaria \\(n\\) qualsiasi), e il coefficiente \\(z\\) con \\(t_{n-1,1-\\alpha/2}\\).\nEsempio. Murray, Murphy, von Hippel, Trivers, e Haselton (2017) si pongono il problema dei bias di giudizio che l’evoluzione potrebbe avere imposto sul funzionamento del sistema cognitivo. In particolare, esaminano l’ipotesi secondo la quale i maschi tendono a sovrastimare l’interesse sessuale delle donne (Haselton & Buss, 2000) perché per la loro inclusive fitness è meno costoso inferire un interesse sessuale assente che non notare un interesse sessuale presente.\nAi partecipanti del loro esperimento veniva chiesto di stimare che cosa una donna direbbe (say question) rispetto alle sue intenzioni sessuali (cioè, sulla propria intenzione di avere un rapporto sessuale con un uomo) e quali potrebbero essere le sue vere intenzioni sessuali (want question) quando essa è coinvolta in vari comportamenti romantici con un uomo (es., tenersi per mano).\nSenza entrare nei dettagli, in una condizione sperimentale, gli autori si aspettano di non trovare differenze tra le condizioni say e want. Riportiamo qui i calcoli per l’intervallo di fiducia sulla differenza tra le stime fornite dai 207 partecipanti alle domande relative alle condizioni say e want.\nI dati sono appaiati e dunque calcoliamo la differenza tra le due condizioni, per ogni partecipante (i dati sono disponibili al seguente indirizzo: ). La media delle differenze è \\(\\bar{X}\\) = 0.063, con una deviazione standard pari a 1.338. L’intervallo di fiducia del 95% risulta dunque essere uguale a\n\\[\n\\bar{X} \\pm t_{.975, 206} \\frac{s}{\\sqrt{n}}  = 0.063 \\pm 1.971 \\frac{1.338}{\\sqrt{207}} = [-0.12,   0.25],\n\\]\ncome riportato dagli autori.\nDato che l’intervallo di confidenza include lo zero, si è soliti concludere che i dati non forniscono evidenze che la statistica in questione (la differenza tra le medie dei campioni) sia diversa da zero – ovvero, nel caso presente, che non vi è una differenza tra le condizioni say e want.\nAmpiezza campionaria e distribuzione della popolazione\nLa formula dell’intervallo di fiducia è stata ricavata nell’ipotesi che la popolazione sia normalmente distribuita e vale anche per piccoli campioni (\\(n < 30\\)) estratti casualmente da questa. Per una popolazione con distribuzione diversa da quella normale, invece, le stime intervallari per la media della popolazione si possono ancora ottenere se la numerosità del campione è sufficientemente elevata (\\(n \\geq 30\\)), calcolando i coefficienti fiduciari \\(z_{\\gamma}\\) che compaiono nelle formule precedenti, dopo aver sostituito in queste formule la deviazione standard incognita \\(\\sigma\\) con il valore empirico della statistica campionaria \\(s\\).\nIn alcuni casi, la distribuzione delle statistiche campionarie approssima la Normale e, in tali casi, l’intervallo fiduciale al 95% è dato da\n\\[\n\\begin{equation}\n\\hat{\\theta} \\pm 1.96 \\cdot \\text{SE}, \\notag\n\\end{equation}\n\\]\novvero, dalla stima del parametro \\(\\pm\\) 1.96 volte l’errore standard.\nConoscendo l’errore standard, è dunque molto semplice calcolare l’intervallo fiduciale. Meno semplice, invece, è interpretare l’intervallo fiduciale nel modo corretto. Per capire quale sia l’interpretazione corretta dell’intervallo fiduciale, iniziamo a definire il concetto di livello di copertura.\nIl livello di copertura\nSi indica con \\(1-\\alpha\\) il livello di copertura fornito dall’intervallo fiduciale. Il termine probabilità di copertura si riferisce alla probabilità che la procedura per la costruzione degli intervalli di fiducia produca un intervallo che contiene (o copre) il valore reale del parametro di interesse. Esiste infatti sempre una probabilità pari ad \\(\\alpha\\) che i dati campionari producano un intervallo che non contiene il valore reale del parametro di interesse.\nRicordiamo che l’approccio frequentista interpreta la probabilità di un evento come la proporzione di volte in cui tale evento si verifica avendo osservato molte ripetizioni indipendenti di un esperimento casuale. Nel caso presente, l’evento in questione è la risposta alla domanda l’intervallo fiduciale contiene il valore del parametro? mentre l’esperimento casuale corrisponde al calcolo dell’intervallo fiduciale per la statistica in question in un campione casuale di ampiezza \\(n\\). L’interpretazione frequentista della nozione di livello di copertura può essere chiarita mediante la seguente simulazione.\nPrendiamo in considerazione la distribuzione dell’altezza degli adulti maschi nella popolazione. Sappiamo che l’altezza degli individui segue la distribuzione normale. Sappiamo inoltre che, per esempio, l’altezza media di un italiano adulto maschio è di \\(175\\) cm, con una varianza di \\(49\\) cm\\(^2\\). Utilizziamo queste informazioni per realizzare la seguente simulazione in R. Nella simulazione prevediamo 100 ripetizioni dell’esperimento casuale che consiste nell’estrazione di un campione di ampiezza \\(n = 20\\) dalla popolazione distribuita come \\(\\mathcal{N}(175, 7)\\). Per ciascun campione casuale così trovato utilizzeremo poi la funzione t.test() per calcolare l’intervallo fiduciale al 95%. Salveremo quindi nella matrice sampling_distribution il limite inferiore e il limite superiore dell’intervallo fiduciale trovato in ciascuno dei 100 campioni.\n\n\nlibrary(\"tidyverse\")\nlibrary(\"scales\")\nset.seed(1235)\nnrep <- 100\nsampling_distribution <- matrix(NA, nrow = nrep, ncol = 2)\npoint_estimate <- rep(NA, nrep)\nsample_size <- 20\nmu <- 175\nsigma <- 7\n\nfor (i in 1:nrep) {\n  y <- rnorm(sample_size, mu, sigma)\n  temp <- t.test(y, conf.level = 0.95)\n  sampling_distribution[i, ] <- temp$conf.int\n  point_estimate[i] <- temp$estimate\n}\n\n\n\nCreiamo poi un data.frame a cui aggiungiamo una colonna che riporta i valori delle medie campionarie.\n\n\ncolnames(sampling_distribution) <- c(\"lcl\", \"ucl\")\nsampling_distribution <- \n  as.data.frame(sampling_distribution)\nsampling_distribution$mean <- as.numeric(point_estimate)\nsampling_distribution$replicate <- 1:nrep\nsampling_distribution$captured <- factor(ifelse(\n  sampling_distribution$lcl <= mu & sampling_distribution$ucl >= mu, \n  1, 0\n))\nlevels(sampling_distribution$captured) <- c('No', 'Si')\n\n\n\nUtilizzando ggplot() creiamo una figura che riporta i 100 intervalli fiduciali al 95% che abbiamo ottenuto, uno per ciascuno dei 100 diversi campioni casuali estratti dalla distribuzione delle altezze.\n\n\np <- ggplot(sampling_distribution) +\n  geom_point(\n    aes(\n      x = point_estimate, y = replicate, color = captured)\n  ) +\n  geom_segment(aes(\n    y = replicate, yend = replicate, x = lcl, xend = ucl,\n    color = captured\n  )) +\n  geom_vline(\n    xintercept = 175, linetype = 2, color = \"white\"\n  ) +\n  labs(\n    x = \"Stima puntuale\",\n    y = \"Campioni simulati\"\n  ) +\n  guides(color=guide_legend(\"Parametro contenuto nell'intervallo\")) \np + papaja::theme_apa() +\n  theme(legend.position = \"bottom\") \n\n\n\n\nLa figura precedente riporta i 100 intervalli fiduciali del 95% calcolati nella simulazione descritta sopra e distingue tra intervalli fiduciali che contengono il valore del parametro e intervalli che non lo contengono. Se ripetiamo la simulazione \\(\\num{10000}\\) volte troviamo un livello di copertura (ovvero, una proporzione di intervalli fiduciali del 95% che contengono il parametro) pari a 0.9468. Questo valore è molto prossimo al livello nominale \\(1 - \\alpha = 0.95\\).\nIntervallo di fiducia per la differenza di due medie\nSi considerino due popolazioni distribuite normalmente e aventi la stessa varianza. Le due popolazioni sono dunque distribuite come due variabili aleatorie indipendenti \\(X \\sim \\mathcal{N}(\\mu_1, \\sigma^2)\\), \\(Y \\sim \\mathcal{N}(\\mu_2, \\sigma^2)\\). Ci poniamo il problema di stimare la differenza \\(\\mu_1 - \\mu_2\\) fra le medie delle due popolazioni.\nEstraiamo un campione casuale da ciascuna popolazione. Siano\n\\[\n\\begin{equation}\nX_1,\\dots,X_{n_1} \\sim \\mathcal{N}(\\mu_1,\\sigma_1), \\quad Y_1,\\dots,Y_{n_2} \\sim \\mathcal{N}(\\mu_2,\\sigma_2),\\notag\n\\end{equation}\n\\] due variabili aleatorie indipendenti e siano\n\\[\n\\begin{equation}\n\\bar{X} = \\frac{1}{n_1}\\sum_{i=1}^{n_1}X_i, \\quad s_1^2 = \\frac{1}{n_1-1}\\sum_{i=1}^{n_1}(X_i-\\bar{X})^2,\\notag\n\\end{equation}\n\\] \\[\n\\begin{equation}\n\\bar{Y} = \\frac{1}{n_2}\\sum_{i=1}^{n_2}Y_i, \\quad s_2^2 = \\frac{1}{n_2-1}\\sum_{i=1}^{n_2}(Y_i-\\bar{Y})^2,\\notag\n\\end{equation}\n\\]\nle relative medie (\\(\\bar{X}, \\bar{Y}\\)) e varianze (\\(s_1^2, s_2^2\\)) campionarie. La varianza \\(\\sigma^2\\), comune a entrambe le popolazioni, può essere stimata utilizzando congiuntamente entrambi i campioni. Infatti, può essere dimostrato che la variabile aleatoria\n\\[\n\\begin{equation}\ns_p^2 \\triangleq  \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2 -2},\n\\end{equation}\n\\]\ndetta stima combinata della varianza è uno stimatore corretto di \\(\\sigma^2\\). Può essere inoltre dimostrato che la variabile aleatoria\n\\[\n\\begin{equation}\nT_n = \\frac{(\\bar{X} - \\bar{Y}) - (\\mu_1-\\mu_2)}{\\sqrt{s_p^2 \\big(\\frac{1}{n_1} + \\frac{1}{n_2}\\big) }}\n\\end{equation}\n\\]\nsi distribuisce come una variabile \\(t\\)-Student con \\(\\nu = n_1 + n_2 -2\\) gradi di libertà.\nSeguendo la procedura descritta nella sezione precedente, possiamo dunque concludere che l’intervallo di fiducia al livello di \\(\\gamma = 1 -\\alpha\\) per la differenza tra le medie \\(\\mu_1 - \\mu_2\\) è dato da\n\\[\n\\begin{equation}\n(\\bar{X} -\\bar{Y}) - t^{\\ast} \\cdot s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} < \\mu_1 - \\mu_2 < (\\bar{X} - \\bar{Y}) + t^{\\ast} \\cdot s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}},\n\\end{equation}\n\\]\ndove \\(t^{\\ast} = t_{\\nu, 1-\\alpha/2}\\) è il quantile di ordine \\(1- \\alpha/2\\) della distribuzione \\(t\\)-Student con \\(\\nu = n_1 + n_2 - 2\\) gradi di libertà.\nDimostrazione. È facile trovare il valore atteso e la varianza (si veda il denominatore dell’equazione ) della differenza fra due medie campionarie. Siano \\(X_1,\\dots,X_{n_1}\\) e \\(Y_1,\\dots,Y_{n_2}\\) due variabili aleatorie indipendenti tali che \\(\\mathbb{E}(X) = \\mu_X\\) e \\(var(X) = \\sigma^2\\); inoltre, \\(\\mathbb{E}(Y) = \\mu_Y\\) e \\(var(Y) = \\sigma^2\\). Consideriamo qui il caso in cui entrambe le variabili aleatorie \\(X\\) e \\(Y\\) hanno la stessa varianza \\(\\sigma^2\\). Il valore atteso della differenza tra le medie \\(\\bar{X} - \\bar{Y}\\) per campioni di ampiezza \\(n_X\\) e \\(n_Y\\) è\n\\[\n\\begin{equation}\n\\mathbb{E}(\\bar{X}-\\bar{Y}) = \\mathbb{E}(\\bar{X})-\\mathbb{E}(\\bar{Y}) = \\mu_X -\\mu_Y.\n\\end{equation}\n\\]\nNell’ipotesi che \\(X\\) e \\(Y\\) siano indipendenti, la varianza della differenza delle medie \\(\\bar{X} - \\bar{Y}\\) per campioni di ampiezza \\(n_X\\) e \\(n_Y\\) è\n\\[\n\\begin{align}\nvar(\\bar{X}-\\bar{Y}) &= var(\\bar{X}) + var(\\bar{Y})- 2 cov(\\bar{X},\\bar{Y}) \\notag\\\\\n&= var(\\bar{X}) + var(\\bar{Y})  \\notag\\\\\n &=  \\frac{\\sigma^2}{n_X} + \\frac{\\sigma^2}{n_Y} \\notag\\\\ \n &= \\sigma^2 \\Big(\\frac{1}{n_X} + \\frac{1}{n_Y} \\Big).\n\\end{align}\n\\]\nInterpretazione\nStudenti e ricercatori tendono ad interpretare gli intervalli fiduciali dicendo che “c’è una probabilità del 95% che la vera media della popolazione si trovi all’interno dell’intervallo fiduciale”. Questa è un’interpretazione semplice e cattura l’idea del senso comune secondo la quale una probabilità di 0.95 significa: “sono sicuro al 95%”. Sfortunatamente, l’interpretazione precedente è sbagliata. La precedente interpretazione richiede che la probabilità venga descritta in termini soggettivi e corrisponde a dire: sono fiducioso al 95% che l’intervallo così costruito contenga la media della popolazione, perché questa è la mia opinione. Nella vita di tutti i giorni un tale punto di vista va benissimo, ma parlare di opinioni soggettive e di fiducia è un’idea Bayesiana. Non c’è niente di male con l’idea che la nozione probabilità del 95% possa riferirsi a un’opinione personale. Tuttavia, gli intervalli fiduciali sono una procedura statistica di stampo frequentista, non Bayesiano. Se usiamo degli strumenti statistici frequentisti per costruire l’intervallo fiduciale non possiamo attribuire ad esso un’interpretazione Bayesiana, ma dobbiamo interpretare tale intervallo di valori in maniera coerente con l’impianto teorico frequentista – anche perché gli intervalli di fiducia frequentisti e gli intervalli di credibilità Bayesiani sono numericamente diversi!\nSe l’interpretazione presentata sopra non è corretta, allora qual è l’interpretazione corretta dell’intervallo fiduciale? Dobbiamo ricordare ciò che abbiamo detto sulla probabilità frequentista: in base all’approccio frequentista la probabilità di un evento è alla proporzione di volte con la quale un evento si è verificato in una sequenza di esperimenti casuali. È necessario ripetere tante volte un esperimento casuale, anche solo in maniera ipotetica (come nella simulazione descritta sopra), altrimenti non è possibile parlare di probabilità. L’interpretazione frequentista di un intervallo fiduciale deve avere a che fare con la ripetizione di un esperimento casuale e può essere formulata nel modo seguente.\n\nSe ripetessimo tante volte l’esperimento casuale che consiste nell’estrarre un campione casuale dalla popolazione e nel calcolare l’intervallo fiduciale al 95%, allora nel 95% dei casi gli intervalli così calcolati conterrebbero il vero valore del parametro.\n\nPiù in generale, se si estraggono successivamente più campioni indipendenti dalla stessa popolazione e se si determinano i relativi intervalli fiduciali seguendo la procedura indicata dalla statistica frequentista, allora il \\(100 (1-\\alpha)\\)% degli intervalli così calcolati conterrà il vero valore del parametro incognito.\nQuesta idea è illustrata nella figura che riporta i risultati della simulazione sul livello di copertura, la quale mostra 100 intervalli fiduciali costruiti per stimare l’altezza media di un italiano adulto maschio sulla base di campioni casuali di ampiezza \\(n = 30\\). Alcuni di questi intervalli fiduciali contengono il valore del parametro, altri non lo contengono. Se la simulazione venisse ripetuta infinite volte si scoprirebbe che esattamente il 95% degli intervalli così calcolati conterrebbe il valore del parametro (e il 5% non lo conterrebbe), dato che, per costruire gli intervalli fiduciali abbiamo usato \\(\\alpha = 0.05\\).\nQuesta è l’interpretazione corretta che deve essere attribuita alla nozione di intervallo fiduciale al livello \\(100 (1-\\alpha)\\)%. È però risaputo come i ricercatori (non solo gli studenti!) spesso attribuiscono agli intervalli fiduciali un’interpretazione errata, come abbiamo descritto sopra. Non poche volte nelle riviste specialistiche si leggono affermazioni del tipo: la probabilità che la media della popolazione \\(\\mu\\) sia contenuta nell’intervallo \\([\\hat{a}, \\hat{b}]\\) è 0.95, mentre in realtà si dovrebbe scrivere: la procedura tramite la quale l’intervallo \\([\\hat{a}, \\hat{b}]\\) è stato calcolato include \\(\\mu\\) nel 95% dei casi.\nIn conclusione, la simulazione appena eseguita ci suggerisce quanto segue. Calcolare un intervallo di fiducia al 95% significa utilizzare una procedura che, per ciascun campione, ci consente di trovare due valori: il limite inferiore e il limite superiore dell’intervallo fiduciario. Se consideriamo un numero molto grande di intervalli di fiducia calcolati in questo modo, la proporzione di intervalli che effettivamente contengono il vero valore della media della popolazione sarà uguale a 0.95. Questo è il significato di intervallo di fiducia al 95%.\nLa differenza fondamentale è che le affermazioni di tipo bayesiano sono delle affermazioni probabilistiche sulla media della popolazione (cioè, descrivono nostra incertezza relativamente al valore della media della popolazione), mentre affermazioni di questo tipo non sono consentite nell’interpretazione frequentista della probabilità. Nell’interpretazione frequentista, la media della popolazione è fissa e nessuna interpretazione ‘probabilistica’ è sensata a questo proposito.\nGli estremi dell’intervallo di fiducia, invece, sono delle quantità aleatorie che dipendono da un esperimento casuale: ogni volta che osserviamo un nuovo campione il limite inferiore e il limite superiore dell’intervallo di fiducia assumeranno valori diversi. Pertanto è sensato pensare che la procedura di costruzione dell’intervallo di fiducia possa essere ripetuta. È in riferimento a tali ripetizioni che l’approccio frequentista assegna una probabilità agli intervalli di fiducia: la probabilità è la frequenza relativa (in queste infinite ripetizioni) che un certo evento si verifica (dove l’evento è ovviamente il fatto che l’intervallo include il valore del parametro). Pertanto, dal punto di vista frequentista, è lecito parlare della probabilità che l’intervallo di fiducia (una variabile aleatoria) contenga il parametro; non è invece lecito dire alcunché sulla probabilità che il parametro (un evento non ripetibile) sia compreso nell’intervallo di fiducia.\nQuesta non è solo una differenza ‘semantica’. Come ho accennato sopra, le procedure di calcolo per gli intervalli di fiducia frequentisti sono diverse dalle procedure di calcolo per gli intervalli di credibilità bayesiani. %In maniera corrispondente, nei due casi anche le interpretazioni sono diverse – e dobbiamo scegliere quella giusta! %Un altro modo per descrivere questa situazione è quello di dire che ciò che vorremmo conoscere è \\(P(\\theta \\mid \\text{dati})\\), mentre in realtà quello che l’approccio frequentista ci fornisce è \\(P(\\text{dati} \\mid \\theta)\\). %Se vengono utilizzati i metodi della statistica bayesiana è invece possibile costruire un  che corrisponde a \\(P(\\theta \\mid \\text{dati})\\).\nFraintendimenti\nHoekstra, Morey, Rouder e Wagenmakers (2014) notano che, essendo ampiamente riconosciuti i limiti del test dell’ipotesi nulla, per l’inferenza statistica viene spesso consigliato l’utilizzo degli intervalli di fiducia. Per esempio, l’American Psychological Association Publication Manual fa riferimento agli intervalli di fiducia affermando che essi rappresentano “in general, the best reporting strategy” (APA, 2001, p. 22; APA, 2009, p. 34). Hoekstra et al. (2014) fanno notare, però, che tali raccomdandazioni hanno dei limiti, in quanto non tengono in considerazione la difficoltà che hanno i ricercatori a fornire agli intervalli di fiducia l’interpretazione corretta. A sostegno di questo punto di vista, Hoekstra et al. (2014) hanno svolto uno studio nel quale si sono posti due domande:\nin che misura gli intervalli di fiducia vengono interpretati in maniera sbagliata da studenti e ricercatori?\nle interpretazioni errate degli intervalli di fiducia diminuiscono con l’esperienza nell’ambito della ricerca?\nPrima di presentare lo studio, Hoekstra et al. ricordano quale sia l’interpretazione corretta degli intervalli di fiducia. Il lettore può mettere in relazione la seguente citazione con ciò che è stato discusso in precedenza.\n\nA CI is a numerical interval constructed around the estimate of a parameter. Such an interval does not, however, directly indicate a property of the parameter; instead, it indicates a property of the procedure, as is typical for a frequentist technique. Specifically, we may find that a particular procedure, when used repeatedly across a series of hypothetical data sets (i.e., the sample space), yields intervals that contain the true parameter value in 95% of the cases. When such a procedure is applied to a particular data set, the resulting interval is said to be a 95% CI. The key point is that the CIs do not provide for a statement about the parameter as it relates to the particular sample at hand; instead, they provide for a statement about the performance of the procedure of drawing such intervals in repeated use. Hence, it is incorrect to interpret a CI as the probability that the true value is within the interval (, Berger & Wolpert, 1988). As is the case with \\(p\\)-values, CIs do not allow one to make probability statements about parameters or hypotheses.\n\nNello studio, Hoekstra et al. hanno sottoposto il questionario riportato di seguito ad un campione di 596 partecipanti. Il campione includeva 442 studenti di psicologia del primo anno che seguivano un corso introduttivo di statistica presso l’università di Amsterdam, 34 studenti di master e 120 ricercatori (cioè dottorandi e docenti universitari).\n\nProfessor Bumbledorf conducts an experiment, analyzes the data, and reports: “The 95% confidence interval for the mean ranges from 0.1 to 0.4!” Please mark each of the statements below as ‘true’ or ‘false’.\n\nThe probability that the true mean is greater than 0 is at least 95%.\nThe probability that the true mean equals 0 is smaller than 5%.\nThe “null hypothesis” that the true mean equals 0 is likely to be incorrect.\nThere is a 95% probability that the true mean lies between 0.1 and 0.4.\nWe can be 95% confident that the true mean lies between 0.1 and 0.4.\nIf we were to repeat the experiment over and over, then 95% of the time the true mean falls between 0.1 and 0.4.\nLe sei affermazioni precedenti sono tutte errate.\nI risultati dello studio di Hoekstra et al. (2014) mostrano però che i partecipanti si sono dichiarati d’accordo con il seguente numero medio di item (su 6): 3.51 (99% CI = [3.35, 3.68]) per gli studenti del primo anno, 3.24 (99% CI = [2.40, 4.07]) per gli studenti di master e 3.45 (99% CI = [3.08, 3.82]) per i ricercatori. Gli intervalli di fiducia al 95% si sovrappongono per le tre categorie di rispondenti il che significa che, a tale livello di fiducia, non c’è ragione di ritenere che vi siano delle differenze tra i tre gruppi di rispondenti. In altre parole, questi dati suggeriscono che i ricercatori tendono a condividere con gli studenti di psicologia del primo anno le stesse opinioni (errate!) relativamente agli intervallo fiduciali.\nLe interpretazioni errate degli intervalli di fiducia sono dunque molto diffuse e l’esperienza pratica nel mondo della ricerca non contribuisce ad una comprensione migliore di tale concetto. In generale, i risultati della ricerca di Hoekstra et al. (2014), e di altre che hanno prodotto risultati simili, mettono in discussione l’utilità degli intervalli fiduciali frequentisti (dato che molto poche persone hanno una comprensione adeguata di tale concetto), favorendo invece l’uso degli intervallo di credibilità Bayesiani ai quali è più facile fornire un’interpretazione corretta, perché tale interpretazione coincide con le nostre intuizioni.\n\n\n\n",
    "preview": "posts/2021-04-09-intervalli-di-fiducia/intervalli-di-fiducia_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-04-09T09:28:27+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-04-07-la-distribuzione-campionaria-della-media/",
    "title": "L'incertezza della stima",
    "description": "Una semplicissima simulazione sulla distribuzione campionaria della media.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-04-07",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\nPresento qui uno script R per un semplice esercizio sulla distribuzione campionaria della media. Lo scopo è quello di chiarire il concetto di distribuzione campionaria della media.\nCarico il pacchetto prob:\n\n\nlibrary(\"prob\")\n\n\n\nNell’esempio useremo una popolazione finita costituita da un numero molto piccolo di elementi:\n\n\nx <- c(2, 1, 6, 4)\nx\n\n\n[1] 2 1 6 4\n\nLa media \\(\\mu\\) della popolazione è\n\n\nmean(x)\n\n\n[1] 3.25\n\nPer calcolare la varianza definisco la funzione my_var() in quanto voglio dividere per n anzicché per n - 1:\n\n\nmy_var <- function(x) {\n  var(x) * (length(x) - 1) / length(x)\n}\n\n\n\nLa varianza della popolazione, \\(\\sigma^2\\), è\n\n\nmy_var(x)\n\n\n[1] 3.6875\n\nMi pongo ora il problema di estrarre dalla popolazione tutti i possibili campioni di ampiezza n = 2. A questo fine userò la funzione prob::urnsamples():\n\n\nss <- unique(urnsamples(x, size = 2, replace = TRUE, ordered = TRUE))\n\n\n\nL’elenco di tutti i possibili campioni di ampiezza n = 2, ovvero 16 campioni, è il seguente:\n\n\nss\n\n\n   X1 X2\n1   2  2\n2   1  2\n3   6  2\n4   4  2\n5   2  1\n6   1  1\n7   6  1\n8   4  1\n9   2  6\n10  1  6\n11  6  6\n12  4  6\n13  2  4\n14  1  4\n15  6  4\n16  4  4\n\nCalcolo ora la media di ciascuno dei 16 campioni:\n\n\nsampling_dist_mean <- rowMeans(ss)\nsampling_dist_mean \n\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16 \n2.0 1.5 4.0 3.0 1.5 1.0 3.5 2.5 4.0 3.5 6.0 5.0 3.0 2.5 5.0 4.0 \n\nQuesti 16 numeri costituiscono la distribuzione campionaria della media per campioni di ampiezza n = 2 nel caso della popolazione di riferimento che stiamo esaminando. Infatti, questo è l’elenco dei valori della media di tutti i possibili campioni di ampiezza n = 2 che si possono estrarre dalla popolazione.\nLo scopo di questa simulazione è quello di mettere in relazione le proprietà della popolazione con le proprietà della distribuzione campionaria della media.\nLa media della distribuzione campionaria delle medie dei campioni\n\n\nmean(sampling_dist_mean)\n\n\n[1] 3.25\n\nè uguale alla media della popolazione.\nLa varianza della distribuzione campionaria delle medie dei campioni\n\n\nmy_var(sampling_dist_mean)\n\n\n[1] 1.84375\n\nè uguale a\n\\[\n\\sigma^2_{\\bar{X}} = \\frac{\\sigma^2}{n}\n\\]\novvero\n\n\nmy_var(x) / 2\n\n\n[1] 1.84375\n\nIl significato della varianza della distribuzione distribuzione campionaria delle medie dei campioni è il seguente:\n\nla deviazione standard della distribuzione campionaria quantifica l’errore medio che compiamo quando usiamo la media del campione quale stima della media della popolazione.\n\n\n\n\n",
    "preview": "posts/2021-04-07-la-distribuzione-campionaria-della-media/preview.png",
    "last_modified": "2021-04-07T21:02:06+02:00",
    "input_file": {},
    "preview_width": 696,
    "preview_height": 658
  },
  {
    "path": "posts/2021-03-30-analisi-delle-componenti-principali/",
    "title": "Analisi delle componenti principali",
    "description": "Ovvero, la proiezione dei punti nella direzione di massima dispersione dei dati.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-30",
    "categories": [
      "R"
    ],
    "contents": "\n\nContents\nI dati\nAutovalori e autovettori\nInterpretazione geometrica\n\nLe componenti principali\nI valori di PC1\nLA PCA con R\nRappresentazione grafica\nInterpretazione\nAutovalori\n\n\nI dati\nSimuliamo i dati di due variabili correlate tra loro. Per semplicità, le due variabili sono standardizzate:\n\n\nlibrary(\"car\")\nlibrary(\"tidyverse\", warn.conflicts = FALSE)\nlibrary(broom)  # for augment(), tidy()\nset.seed(123456)\n\nnpoints <- 20\nx <- as.numeric(scale(rnorm(npoints, 0, 1)))\ny <- as.numeric(scale(3 * x + rnorm(npoints, 0, 2)))\n\nY <- as.matrix(\n  data.frame(x, y)\n)\n\n\n\nRacchiudiamo le osservazioni con un’ellisse (nel caso presente, il contorno di isodensità al 95%).\n\n\ncar:::dataEllipse(\n  Y[, 1], Y[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3)\n)\n\n\n\n\nAutovalori e autovettori\nCalcoliamo autovettori e autovalori con R:\n\n\ns <- cov(Y)\nee <- eigen(s)\nee\n\n\neigen() decomposition\n$values\n[1] 1.8291033 0.1708967\n\n$vectors\n          [,1]       [,2]\n[1,] 0.7071068 -0.7071068\n[2,] 0.7071068  0.7071068\n\nInterpretazione geometrica\nLa lunghezza dei semiassi maggiori e minori dell’ellisse è proporzionale alla radice quadrata dei due autovalori.\nL’asse maggiore è la linea passante per il punto specificato dal primo autovettore e l’asse minore è la linea passante per il punto specificato dal secondo autovettore.\nUsiamo queste informazioni per aggiungere le due frecce rosse nella figura:\n\n\ncar:::dataEllipse(\n  Y[, 1], Y[, 2],\n  levels = 0.95,\n  lty = 2,\n  ylim = c(-3, 3),\n  xlim = c(-3, 3)\n)\n\nk <- 2.65 \narrows(\n  0,0,\n  k * sqrt(ee$values[1]) * ee$vectors[1],\n  k * sqrt(ee$values[1]) * ee$vectors[2],\n  code = 2,\n  col = \"red\",\n  lwd = 2\n)\n\narrows(\n  0, 0,\n  k * sqrt(ee$values[2]) * ee$vectors[1],\n  k * sqrt(ee$values[2]) * -ee$vectors[2],\n  code = 2,\n  col = \"red\",\n  lwd = 2\n)\n\n\n\n\nLe componenti principali\nAvendo calcolato gli autovalori e gli autovettori di una matrice di varianze/covarianze è facile eseguire i calcoli dell’analisi delle componenti principali.\nLa prima componente principale corrisponde alla proiezione ortogonale dei punti del diagramma a dispersione sull’asse coincidente con l’asse maggiore dell’ellisse rappresentata nella figura.\nLa seconda componente principale corrisponde invece alla proiezione dei punti sull’asse ortogonale a quello descritto in precedenza.\nI valori di PC1\nIl primo autovettore è dato da:\n\n\nfirst_eigenvector <- ee$vectors[, 1]\nfirst_eigenvector\n\n\n[1] 0.7071068 0.7071068\n\nUsiamo l’algebra matriciale per calcolare la proiezione di un punto su un vettore. A tale scopo, definiamo la seguente funzione:\n\n\northo_proj <- function(x, y, eigenvector) {\n  cbind(x, y) %*% eigenvector \n}\n\n\n\nNel caso presente non è necessario dividere per la lunghezza dell’autovettore perché, per convenzione, gli autovettori specificati da R hanno lunghezza unitaria.\nSiamo ora nelle condizioni di calcolare la proiezione dei 20 punti dell’esempio considerato sull’asse specificato dal primo autovettore:\n\n\npc1 <- ortho_proj(Y[1:20, 1], Y[1:20, 2], first_eigenvector)\n\n\n\nTali proiezioni costituiscono la prima componente principale.\nÈ facile verificare che questo è vero. Infatti, la varianza della prima componente principale (che abbiamo appena calcolato)\n\n\nvar(pc1) \n\n\n         [,1]\n[1,] 1.829103\n\nè uguale al primo autovalore\n\n\nee$values[1]\n\n\n[1] 1.829103\n\ncome ci aspettiamo che sia.\nLA PCA con R\n\n\npca_fit <- Y %>%       \n  prcomp()\n\n\n\nAggiungiamo le componenti principali ai dati di partenza:\n\n\npc <- pca_fit %>%\n  augment(Y)\nhead(pc)\n\n\n# A tibble: 6 x 5\n  .rownames      x       y .fittedPC1 .fittedPC2\n  <fct>      <dbl>   <dbl>      <dbl>      <dbl>\n1 1          0.302 -0.0997     -0.143     0.284 \n2 2         -0.781  0.104       0.478    -0.626 \n3 3         -0.858 -0.600       1.03     -0.182 \n4 4         -0.426 -0.178       0.427    -0.175 \n5 5          1.69   1.64       -2.35      0.0329\n6 6          0.303  0.0734     -0.266     0.162 \n\nVerifichiamo i calcoli eseguiti in precedenza:\n\n\ncor(pc1, pc$.fittedPC1)\n\n\n     [,1]\n[1,]   -1\n\nIl segno è arbitrario, il che significa che il risultato è corretto.\nRappresentazione grafica\nPossiamo ottenere una rappresentazione grafica delle due componenti principali nel modo seguente:\n\n\npca_fit %>%\n  # add PCs to the original dataset\n  augment(Y) %>%\n  ggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_point() +\n  labs(\n    x = \"PC1\",\n    y = \"PC2\"\n  ) +\n  papaja::theme_apa()\n\n\n\n\nInterpretazione\n\n\narrow_style <- arrow(\n  angle = 20, length = grid::unit(8, \"pt\"),\n  ends = \"first\", type = \"closed\"\n)\npca_fit %>%\n  # extract rotation matrix\n  tidy(matrix = \"rotation\") %>%\n  pivot_wider(\n    names_from = \"PC\", values_from = \"value\",\n    names_prefix = \"PC\"\n  ) %>%\n  ggplot(aes(PC1, PC2)) +\n  geom_segment(\n    xend = 0, yend = 0,\n    arrow = arrow_style\n  ) +\n  geom_text(aes(label = column), hjust = 1) +\n  xlim(-1.5, 0.5) + ylim(-1, 1) + \n  coord_fixed() +\n  papaja::theme_apa()\n\n\n\n\nLe due variabili x e y contribuiscono negativamente a PC1, nella stessa misura. Dunque PC1 è l’equivalente di una media aritmetica delle due variabili.\nPC2 rappresenta invece la differenza tra le dimensioni x e y.\nAutovalori\nUna rappresentazione grafica della varianza spiegata dagli autovalori è fornita qui sotto:\n\n\npca_fit %>%\n  # extract eigenvalues\n  tidy(matrix = \"eigenvalues\") %>%\n  ggplot(aes(PC, percent)) + \n  geom_col() + \n  scale_x_continuous(\n    # create one axis tick per PC\n    breaks = 1:2\n  ) +\n  scale_y_continuous(\n    name = \"Varianza spiegata\",\n    # format y axis ticks as percent values\n    label = scales::label_percent(accuracy = 1)\n  ) +\n  papaja::theme_apa()\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-03-30-analisi-delle-componenti-principali/analisi-delle-componenti-principali_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-31T05:49:44+02:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 1152
  },
  {
    "path": "posts/2021-03-29-la-verosimiglianza-e-il-mappamondo/",
    "title": "La verosimiglianza e il pettirosso",
    "description": "La funzione di verosimiglianza spiegata da Richard McElreath.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-29",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\n\nContents\nDefinizione\nUn esempio\nModello statistico\nCoefficiente binomiale\nLa funzione di verosimiglianza\nVerosimiglianza per una Normale\nLa log-verosimiglianza\n\nDi solito, quando parliamo di distribuzioni di probabilità, assumiamo di conoscere i valori dei parametri. Nel mondo reale, però, di solito è il contrario. Quello che abbiamo sono dei dati. I parametri sono quello che vogliamo sapere.\nIniziamo con un esempio. Supponiamo di studiare i pettirossi. Sappiamo che, tra fine aprile e inizio maggio, la femmina di pettirosso depone all’interno del nido quattro o sei uova. Dopo la schiusa, ai piccoli bastano 15/20 giorni per essere in grado di volare e abbandonare il nido. Ma solo pochi sopravvivono a causa dell’alta mortalità infantile.\n\n\n\nImmaginiamo di avere osservato una femmina di pettirosso che cova cinque uova. Dei cinque neonati tre sopravvivono e prendono il volo 14 giorni dopo la schiusa delle uova.\nCosa ci dicono questi dati della probabilità di un pettirosso neonato di sopravvivere, per questa particolare femmina? Cosa ci dicono questi dati della probabilità di sopravvivenza nella popolazione dei pettirossi neonati?\nNon conosciamo la probabilità di sopravvivenza dei pettirossi neonati nella popolazione. Ma possiamo calcolare la verosimiglianza di ottenere i dati che abbiamo osservato per valori diversi del parametro sconosciuto p (la probabilità di sopravvivenza):\n\n\ndbinom(x=3, size=5, prob=0.1)\n\n\n[1] 0.0081\n\nSe cambiamo il valore del parametro p la verosimiglianza dei dati cambia:\n\n\ndbinom(x=3, size=5, prob=0.4)\n\n\n[1] 0.2304\n\n\n\ndbinom(x=3, size=5, prob=0.9)\n\n\n[1] 0.0729\n\nDai calcoli precedenti vediamo che è più plausibile osservare 3 “successi” in 5 prove se la probabilità di successo è 0.4 piuttosto che 0.1 o 0.9.\nQuesto modo di ragionare è così comune in statistica che ha un nome speciale: si chiama verosimiglianza. La verosimiglianza descrive la plausibilità di osservare i dati al variare dei valori del parametro di un modello statistico. Ciò ci conduce alla seguente definizione.\nDefinizione\n\nLa funzione di verosimiglianza è la funzione di massa o di densità di probabilità dei dati vista come una funzione dei parametri sconosciuti.\n\nUn esempio\nPer capire meglio cosa significa la definizione precedente ci focalizzeremo sul caso di un’unico parametro sconosciuto (come nell’esempio precedente del pettirosso) e prenderemo in esame la distribuzione Binomiale. Inizio caricando i pacchetti che userò in questo tutorial.\n\n\nlibrary(\"tidyverse\", warn.conflicts = FALSE)\nlibrary(\"bayesplot\")\n\n\n\nNella discussione seguente non ci occuperemo più di pettirossi ma esamineremo un esempio discusso da McElreath (2020). Supponiamo di tenere in mano un mappamondo gonfiabile e di chiederci: “qual’è la proporzione della superficie terreste ricoperta d’acqua?” Sembra una domanda a cui è difficile rispondere. Ma ci viene in mente questa idea brillante: lanciamo in aria il mappamondo e, quando lo riprendiamo, osserviamo se la superfice del mappamondo sotto il nostro dito indice destro rappresenta acqua o terra. Possiamo ripetere questa procedura più volte, così da ottenere un campione causale di diverse porzioni della superficie dal mappamondo. Eseguiamo il nostro esperimento lanciando in aria il mappamondo nove volte e osserviamo i seguenti risultati: A, T, A, A, A, T, A, T, A, dove “A” indica acqua e “T” indica terra.\nModello statistico\nQual è il modello statistico che potrebbe avere generato i dati che abbiamo osservato? Per l’esempio del mappamondo possiamo dire quanto segue:\nla proporzione del pianeta Terra ricoperta d’acqua è p;\nun singolo lancio del mappamondo ha una probabilità p di produrre l’osservazione “acqua” (A);\ni lanci del mappamondo sono indipendenti (nel senso che il risultato di un lancio non influenza i risultati degli altri lanci).\nLe caratteristiche descritte sopra definiscono il processo generativo dei dati che sta alla base della distribuzione Binomiale. Nel caso presente, il parametro sconosciuto è p.\nSappiamo dunque che legge Binomiale illustra la relazione tra i dati osservati e il parametri sconosciuto p:\n\\[\np(x \\mid p) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\nCoefficiente binomiale\nIl coefficiente binomiale è\n\\[\n\\binom{n}{k} = \\frac{n!}{k! (n-k)!}\n\\] In R la funzione che restituisce come risultato il fattoriale di \\(a\\) è factorial(). Per esempio:\n\n\nfactorial(3)\n\n\n[1] 6\n\nMa il risultato del coefficiente binomiale viene calcolato direttamente dalla funzione choose(). Per esempio, consideriamo il caso in cui n = 9 e k = 6:\n\n\nchoose(9, 6)\n\n\n[1] 84\n\novvero\n\n\nfactorial(9) / (factorial(6) * factorial(3))\n\n\n[1] 84\n\nDunque, possiamo implementare la legge binomiale in una funzione R nel modo seguente:\n\n\nbinomial <- function(k, n, p) {\n  choose(n, k) * p^k * (1 - p)^(n - k)\n}\n\n\n\nOvviamemente, tale funzione è già presente in R ed è data da dbinom(). Controlliamo:\n\n\nbinomial(6, 9, 0.2)\n\n\n[1] 0.002752512\n\n\n\ndbinom(6, 9, 0.2)\n\n\n[1] 0.002752512\n\nLa funzione di verosimiglianza\nAdesso poniamoci il problema di capire come si crea la figura che ho preso dal seguente script la quale implementa in R la figura riportata da McElreath (2020).\nInziamo a definire un vettore che contiene i dati:\n\n\nd <- tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\"))\nd <- d %>% \n  mutate(n_trials  = 1:9,\n         n_success = cumsum(toss == \"w\"))\nd\n\n\n# A tibble: 9 x 3\n  toss  n_trials n_success\n  <chr>    <int>     <int>\n1 w            1         1\n2 l            2         1\n3 w            3         2\n4 w            4         3\n5 w            5         4\n6 l            6         4\n7 w            7         5\n8 l            8         5\n9 w            9         6\n\n\n\nsequence_length <- 50\n\nd %>%\n  expand(\n    nesting(\n      n_trials, toss, n_success\n    ),\n    p_water = seq(\n      from = 0, to = 1,\n      length.out = sequence_length\n    )\n  ) %>%\n  group_by(p_water) %>%\n  mutate(\n    lagged_n_trials = lag(n_trials, k = 1),\n    lagged_n_success = lag(n_success, k = 1)\n  ) %>%\n  ungroup() %>%\n  mutate(\n    prior = ifelse(\n      n_trials == 1, .5,\n      dbinom(\n        x = lagged_n_success,\n        size = lagged_n_trials,\n        prob = p_water\n      )\n    ),\n    likelihood = dbinom(\n      x = n_success,\n      size = n_trials,\n      prob = p_water\n    ),\n    strip = str_c(\"n = \", n_trials)\n  ) %>%\n  # the next three lines allow us to normalize the prior and the likelihood,\n  # putting them both in a probability metric\n  group_by(n_trials) %>%\n  mutate(\n    prior = prior / sum(prior),\n    likelihood = likelihood / sum(likelihood)\n  ) %>%\n  # plot!\n  ggplot(aes(x = p_water)) +\n  geom_line(aes(y = prior),\n    linetype = 2\n  ) +\n  geom_line(aes(y = likelihood)) +\n  scale_x_continuous(\"proportion water\", breaks = c(0, .5, 1)) +\n  scale_y_continuous(\"plausibility\", breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~strip, scales = \"free_y\")\n\n\n\n\nLa curva rappresentata in ciascun pannello della figura precedente è la funzione di verosimiglianza calcolata utilizzando la legge Binomiale, ma considierando campioni diversi di dati. Nel pannello n = 1 è rappresentata la verosimiglianza dei possibili valori del parametro sconosciuto p se abbiamo osservato solo un successo in 1 prova dell’esperimento casuale. Nel secondo pannello, n = 2 è rappresentata la verosimiglianza che si ottiene avendo osservato un successo e un insuccesso in due prove successive dell’esperimento casuale. E così via.\nIniziamo dal pannello n = 1.\nLa definizione della funzione di verosimiglianza ci dice che dobbiamo usare la legge Binomiale tenendo costanti i dati. Questo significa che dobbiamo variare i valori del parametro p. Quali valori può assumere p? Tutti i valori nell’intervallo [0, 1]. Ne considereremo qui 50:\n\n\np <- seq(0, 1, length.out = 50)\np\n\n\n [1] 0.00000000 0.02040816 0.04081633 0.06122449 0.08163265 0.10204082\n [7] 0.12244898 0.14285714 0.16326531 0.18367347 0.20408163 0.22448980\n[13] 0.24489796 0.26530612 0.28571429 0.30612245 0.32653061 0.34693878\n[19] 0.36734694 0.38775510 0.40816327 0.42857143 0.44897959 0.46938776\n[25] 0.48979592 0.51020408 0.53061224 0.55102041 0.57142857 0.59183673\n[31] 0.61224490 0.63265306 0.65306122 0.67346939 0.69387755 0.71428571\n[37] 0.73469388 0.75510204 0.77551020 0.79591837 0.81632653 0.83673469\n[43] 0.85714286 0.87755102 0.89795918 0.91836735 0.93877551 0.95918367\n[49] 0.97959184 1.00000000\n\nQuali sono i nostri dati? Se abbiamo ottenuto “acqua” avendo lanciato una volta in aria il mappadondo, allora i nostri dati sono: k = 1, con n = 1. Inseriamo dunque tali dati nella formula della Binomiale:\n\n\nl <- binomial(1, 1, p)\nl\n\n\n [1] 0.00000000 0.02040816 0.04081633 0.06122449 0.08163265 0.10204082\n [7] 0.12244898 0.14285714 0.16326531 0.18367347 0.20408163 0.22448980\n[13] 0.24489796 0.26530612 0.28571429 0.30612245 0.32653061 0.34693878\n[19] 0.36734694 0.38775510 0.40816327 0.42857143 0.44897959 0.46938776\n[25] 0.48979592 0.51020408 0.53061224 0.55102041 0.57142857 0.59183673\n[31] 0.61224490 0.63265306 0.65306122 0.67346939 0.69387755 0.71428571\n[37] 0.73469388 0.75510204 0.77551020 0.79591837 0.81632653 0.83673469\n[43] 0.85714286 0.87755102 0.89795918 0.91836735 0.93877551 0.95918367\n[49] 0.97959184 1.00000000\n\nCreaimo un diagramma con il risultato ottenuto.\n\n\nlike <- data.frame(\n  l = l\n)\nlike %>% \n  ggplot(aes(x = p)) +\n  geom_line(aes(y = l),\n    linetype = 1\n  ) +\n  labs(\n    y = \"plausibility\",\n    x = \"proportion water\"\n  ) +\n  theme(panel.grid = element_blank()) \n\n\n\n\nLa figura ci dice che, avendo osservato un successo in una prova, la plausibilità che p sia uguale a 1 è massima. La plausibilità di p diminuisce allonandoci dal valore possibile p = 1, fino ad arrivare al valore minimo, ovvero 0, in corrispondenza dell’ipotesi secondo la quale p = 0. È infatti impossibile che p = 0 dato che abbiamo osservato un successo. Quindi deve essere vero che p \\(>\\) 0.\nAdesso lanciamo il mappamondo una seconda volta. Osserviamo “terra.” Ripetiamo la procedura descritta sopra.\n\n\nl <- binomial(1, 2, p)\nlike <- data.frame(\n  l = l\n)\nlike %>% \n  ggplot(aes(x = p)) +\n  geom_line(aes(y = l),\n    linetype = 1\n  ) +\n  labs(\n    y = \"plausibility\",\n    x = \"proportion water\"\n  ) +\n  theme(panel.grid = element_blank()) \n\n\n\n\nLa figura ci dice che ci sono due valori del parametro p che sono impossibili: p = 0, perché abbiamo osservato un successo e p = 1, in quanto abbiamo osservato un insuccesso. In tali condizioni, è inuitivo che il valore p più plausibile sia nell’intorno di 0.5. Infatti, la figura ci dice proprio questo.\nEvidentemente non abbiamo bisogno di usare la nostra funzione binomial() per ottenere i risultati descritti sopra. Gli stessi risultati si ottengono con la funzione dbinom():\n\n\nl <- dbinom(1, 2, p)\nlike <- data.frame(\n  l = l\n)\nlike %>% \n  ggplot(aes(x = p)) +\n  geom_line(aes(y = l),\n    linetype = 1\n  ) +\n  labs(\n    y = \"plausibility\",\n    x = \"proportion water\"\n  ) +\n  theme(panel.grid = element_blank()) \n\n\n\n\nRipetendo questo ragionamento, ovvero ripercorrendo i vari passi dell’esempio fino ad ottenere 6 successi in 9 prove, riusciamo a costruire la figura con nove pannelli riportata sopra.\nVerosimiglianza per una Normale\nPoniamoci ora il problema di calcolare la verosimiglianza per i parametri sconosciuti di una distribuzione Normale avendo a disposizione un campione casuale di osservazioni.\nIniziamo con il ricordare cosa significano le nozioni di “probabilità” e “densità di probabilità” nel caso di una variabile aleatoria continua. La densità di probabilità può essere intesa come una misura di probabilità relativa, nel senso che valori della variabile aleatoria in intervalli a cui sono associate probabilità maggiori avranno anche densità di probabilità maggiori. In termini formali si dice che la probabilità è l’integrale della densità di probabilità in un intervallo. Per esempio, la classica curva campanulare associata alla distribuzione Normale è una misura della densità di probabilità, mentre la probabilità corrisponde all’area sottesa alla curva in un determinato intervallo di valori della variabile aleatoria.\nSe noi assegnamo un modello statistico ad una variabile aleatoria, allora ciascuna realizzazione della variabile aleatoria (chiamiamola \\(x_i\\)) avrà una densità di probabilità definita dal modello probabilistico (chiamiamola \\(f(x_i)\\)). Se assumiamo che i valori del nostro campione sono statisticamente indipendenti (ovvero, se la probabilità di osservare un certo valore del nostro campione non dipende dagli altri valori che fanno parte del campione), allora la verosimiglianza di osservare l’intero campione (chiamiamola \\(L(x)\\)) è definita come il prodotto delle densità di probabilità di ciascuno dei valori del campione, ovvero\n\\[\nL(x) = \\prod_{i=1}^n f(x_i),\n\\] dove \\(n\\) è l’ampiezza del campione.\nIn base al modello statistico che abbiamo scelto, ovvero quello della distribuzione Normale, la densità di una singola osservazione è data da\n\\[\n f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \n  \\exp\\left( -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{\\!2}\\,\\right).\n\\]\nLa legge Normale è implementata in R nell’istruzione seguente:\n\n\ndnorm(x, mean = mu, sigma = sigma)\n\n\n\nLa verosimiglianza del campione sarà dunque uguale a\n\\[\nL(x) = \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}} \n  \\exp\\left( -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{\\!2}\\,\\right).\n\\]\nIl problema di procedere in questo modo, però, è che il risultato numerico di questo prodotto diventa molto piccolo, e i computer hanno difficoltà a manipolare numeri così piccoli. Tale problema può essere risolto prendendo il logaritmo. Sappiao che, per le proprietà dei logaritmi, il logaritmo di un prodotto è uguale alla somma dei logaritmi. Per esempio,\n\n\nlog(3 * 4 * 5)\n\n\n[1] 4.094345\n\nlog(3) + log(4) + log(5)\n\n\n[1] 4.094345\n\nDunque, se vogliamo costruire la funzione di log-verosimiglianza per un campione di \\(n\\) osservazioni sarà sufficiente sommare le \\(n\\) log-verosimiglianze:\n\\[\nlog\\big(L(x)\\big) = \\sum_{i=1}^n log \\big(f(x_i) \\big).\n\\]\nIn R, il logaritmo della densità di probabilità Normale è dato da:\n\n\ndnorm(x, mean = mu, sigma = sigma, log = TRUE)\n\n\n\nL’argomento log=TRUE nella funzione dnorm() specifica appunto che vogliamo il logaritmo della densità Normale.\nFaciamo un esempio e consideriamo i dati discussi nelle dispense:\n\n\nd <- data.frame(\n  x = c(26, 35, 30, 25, 44, 30, 33, 43, 22, 43, 24, 19, 39, 31, 25, \n        28, 35, 30, 26, 31, 41, 36, 26, 35, 33, 28, 27, 34, 27, 22)\n  )\n\n\n\nPer semplificare il problema, poniamoci l’obiettivo di stimare solo uno dei due parametri sconosciuti della distribuzione Normale, ovvero \\(\\mu\\), tenendo \\(\\sigma\\) uguale alla deviazione standard del campione:\n\n\ntrue_sigma <- sd(d$x)\n\n\n\nDefiniamo in R il logaritomo della funzione di verosimiglianza, ovvero la funzione di log-verosimiglianza:\n\n\nlog_likelihood <- function(x, mu, sigma=true_sigma) {\n  sum(dnorm(x, mu, sigma, log=TRUE))\n}\n\n\n\nLa situazione è dunque simile a quella che abbiamo discusso nel caso della Binomiale. La complicazione è che dobbiamo fare una somma di 30 addendi, ma tale somma viene calcolata all’interno della funzione log_likelihood().\nNotiamo che della funzione log_likelihood() richiede tre argomenti: il primo argomento x corrisponde al vettore che contiene i dati; il secondo argomento mu è il parametro sconosciuto; il terzo argomento, \\(\\sigma\\), nell’esempio che stiamo discutendo viene mantenuto costante.\nCome abbiamo fatto nel caso della verosimiglianza di una Binomiale, definiamo una serie di valori per il parametro sconosciuto mu:\n\n\nnrep <- 1e5\nmu <- seq(\n  mean(d$x) - sd(d$x), \n  mean(d$x) + sd(d$x), \n  length.out = nrep\n)\n\n\n\nPer l’esempio presente esamineremo un grande numero di valori possibili per il parametro sconosciuto \\(\\mu\\), ovvero 1e5.\nSiamo ora nelle condizioni di trovare i valori della log-verosimiglianza:\n\n\nll <- rep(NA, nrep)\nfor (i in 1:nrep) {\n  ll[i] <- log_likelihood(d$x, mu[i], true_sigma)\n}\n\n\n\nEsaminiamo uno dei valori possibili della log-verosimiglianza. Consideriamo un valore qualsiasi del parametro mu, per esempio, 28, e svolgiamo i calcoli. Dobbiamo calcolare il valore che viene ritornato da ´dnorm(…, log=TRUE)´ per ciascuno dei valori x del campione, tenendo costanti i valori sigma = s e mu = 28:\n\n\nmu_test <- 28\ndnorm(d$x, mu_test, true_sigma, log=TRUE)\n\n\n [1] -2.852865 -3.368322 -2.852865 -2.910138 -5.739425 -2.852865\n [7] -3.093412 -5.384332 -3.219412 -5.384332 -2.990320 -3.734870\n[13] -4.193054 -2.910138 -2.910138 -2.807047 -3.368322 -2.852865\n[19] -2.852865 -2.910138 -4.742875 -3.540141 -2.852865 -3.368322\n[25] -3.093412 -2.807047 -2.818501 -3.219412 -2.818501 -3.219412\n\nLa somma di questi valori è data da\n\n\nsum(dnorm(d$x, mu_test, true_sigma, log=TRUE))\n\n\n[1] -101.6682\n\nNello script precedente, questi calcoli sono stati svolti per tutti gli 1e5 valori ipotizzati per mu. Verifichiamo. Chiediamoci a quale indice del vettore mu corrisponde il valore di mu_test = 28. Nel vettore che abbiamo creato, il valore più prossimo a 28 corrisponde all’elemento nella posizione\n\n\nmu_index <- which.min(abs(mu - mu_test))\nmu_index\n\n\n[1] 27802\n\novvero\n\n\nmu[mu_index]\n\n\n[1] 28.00006\n\nChiediamoci qual è il valore dell’elemento di indice pari a mu_index all’interno del vettore ll:\n\n\nll[mu_index]\n\n\n[1] -101.6681\n\nTale valore è identico a quello trovato con i calcoli che abbiamo svolto in precedenza.\nLa log-verosimiglianza\nDisegnamo ora la funzione di log-verosimiglianza per il parametro sconosciuto \\(\\mu\\):\n\n\ndata.frame(mu, ll) %>% \nggplot(aes(x=mu, y=ll)) +\n  geom_line() +\n  vline_at(mean(d$x), color=\"red\", linetype=\"dashed\") +\n  labs(\n    y=\"Log-verosimiglianza\",\n    x=c(\"Parametro \\u03BC\")\n  ) +\n  theme(panel.grid = element_blank()) \n\n\n\n\nLa funzione di log-verosimiglianza ha un massimo. Il valore \\(\\mu\\) corrispondente a tale massimo è lo stimatore di massima verosimiglianza per la media sconosciuta della popolazione.\nSappiamo che tale stimatore non è altro che la media del campione. Controlliamo:\n\n\nmu[which.max(ll)]\n\n\n[1] 30.93327\n\ninfatti\n\n\nmean(d$x)\n\n\n[1] 30.93333\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. CRC press.\n\n\n\n\n",
    "preview": "posts/2021-03-29-la-verosimiglianza-e-il-mappamondo/pettirosso.jpg",
    "last_modified": "2021-04-03T08:22:05+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-29-un-po-di-algebra-matriciale/",
    "title": "Un po' di algebra matriciale",
    "description": "Usiamo R per le operazioni di base dell'algebra lineare.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-29",
    "categories": [
      "R"
    ],
    "contents": "\n\nContents\nVettori\nMoltiplicazione di matrici\nInversa di una matrice\nTraccia\n\n\n\nlibrary(\"matlib\")\n\n\n\nVettori\nDefiniamo il vettore a\n\n\na <- matrix(\n  c(1, 3, -2, 4), \n  nrow = 4, \n  byrow = TRUE\n)\na\n\n\n     [,1]\n[1,]    1\n[2,]    3\n[3,]   -2\n[4,]    4\n\ne il vettore b\n\n\nb <- matrix(\n  c(2, 0, 1, -1), \n  nrow = 4, \n  byrow = TRUE\n)\nb\n\n\n     [,1]\n[1,]    2\n[2,]    0\n[3,]    1\n[4,]   -1\n\nSomma e differenza si svolgono elemento per elemento, per cui:\n\n\na + b\n\n\n     [,1]\n[1,]    3\n[2,]    3\n[3,]   -1\n[4,]    3\n\ne\n\n\na - b\n\n\n     [,1]\n[1,]   -1\n[2,]    3\n[3,]   -3\n[4,]    5\n\nProdotto interno\nIl prodotto interno produce uno scalare:\n\n\nt(a) %*% b\n\n\n     [,1]\n[1,]   -4\n\nSi noti che, quando dobbiamo moltiplicare vettori riga e vettori colonna, ovvero quando dobbiamo moltiplicare matrici, usiamo l’operatore %*%.\nOvviamente qui l’ordine con il quale svolgiamo la moltiplicazione non conta:\n\n\nt(b) %*% a\n\n\n     [,1]\n[1,]   -4\n\nProdotto esterno\nIl prodotto esterno produce una matrice:\n\n\na %*% t(b)\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    0    1   -1\n[2,]    6    0    3   -3\n[3,]   -4    0   -2    2\n[4,]    8    0    4   -4\n\nQui invece l’ordine è importante:\n\n\nb %*% t(a)\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   -4    8\n[2,]    0    0    0    0\n[3,]    1    3   -2    4\n[4,]   -1   -3    2   -4\n\nMoltiplicazione di matrici\nDue matrici si possono moltiplicare quando sono conformabili (ovvero quando il numero di colonne della prima è uguale al numero di righe della seconda matrice). La matrice A è di ordine 2 \\(\\times\\) 4:\n\n\nA <- matrix(\n  c(1, 3, -2, 4, \n    4, 1,  0, 1), \n  nrow = 2, \n  byrow = TRUE\n)\nA\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3   -2    4\n[2,]    4    1    0    1\n\nLa matrice B è di ordine 4 \\(\\times\\) 3:\n\n\nB <- matrix(\n  c(1, 1, -2, \n    2, 1,  0,\n    1, 1, 0, \n    -2, 1, 0), \n  nrow = 4, \n  byrow = TRUE\n)\nB\n\n\n     [,1] [,2] [,3]\n[1,]    1    1   -2\n[2,]    2    1    0\n[3,]    1    1    0\n[4,]   -2    1    0\n\nIl risultato è una matrice 2 \\(\\times\\) 3:\n\n\nA %*% B\n\n\n     [,1] [,2] [,3]\n[1,]   -3    6   -2\n[2,]    4    6   -8\n\nInversa di una matrice\nIn R si usa la funzione solve() per trovare l’inversa di una matrice\n\n\nC <- matrix(\n  c(3, -2, \n    2, 2), \n  nrow = 2, \n  byrow = TRUE\n)\nC\n\n\n     [,1] [,2]\n[1,]    3   -2\n[2,]    2    2\n\n\n\nsolve(C)\n\n\n     [,1] [,2]\n[1,]  0.2  0.2\n[2,] -0.2  0.3\n\n\n\nround(solve(C) %*% C, 3)\n\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n\n\nround(C %*% solve(C), 3)\n\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nIl metodo della matrice aggiunta\nCalcoliamo il determinante\n\n\ndet(C)\n\n\n[1] 10\n\ne la matrice aggiunta (in inglese adjoint)\n\n\nadjoint <- function(A) det(A)*solve(A)\n\n\n\n\n\nadjoint(C)\n\n\n     [,1] [,2]\n[1,]    2    2\n[2,]   -2    3\n\nL’inversa diventa\n\n\n1 / det(C) * adjoint(C)\n\n\n     [,1] [,2]\n[1,]  0.2  0.2\n[2,] -0.2  0.3\n\nquindi\n\n\nround(C %*% (1 / det(C) * adjoint(C)), 3)\n\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\nTraccia\nLa traccia di una matrice è data dalla somma degli elementi sulla diagonale principale:\n\n\nsum(diag(C))\n\n\n[1] 5\n\nOppure\n\n\ntrace <- function(A) {\n  n <- dim(A)[1] # get dimension of matrix\n  tr <- 0 # initialize trace value\n  \n  # Loop over the diagonal elements of the supplied matrix and add the element to tr\n  for (k in 1:n) {\n    l <- A[k,k]\n    tr <- tr + l\n  }\n  return(tr[[1]])\n}\n\n\n\n\n\ntrace(C)\n\n\n[1] 5\n\n\n\n\n",
    "preview": "posts/2021-03-29-un-po-di-algebra-matriciale/preview.png",
    "last_modified": "2021-03-29T13:45:53+02:00",
    "input_file": {},
    "preview_width": 834,
    "preview_height": 338
  },
  {
    "path": "posts/2021-03-25-un-tutorial-sullalgoritmo-di-metropolis/",
    "title": "Catena di Markov Monte Carlo",
    "description": "Un tutorial sull'algoritmo di Metropolis.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-25",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\n\nContents\nIntroduzione\nObiettivo\nDistribuzioni a priori\nVerosimiglianza\nL’algoritmo di Metropolis\nImplementazione in R\nDistribuzione a priori non uniforme per \\(\\mu\\)\nUn campione più grande\n\n\nIntroduzione\nL’obiettivo di questo tutorial è dimostrare come funziona l’algoritmo Metropolis quando viene applicato a un caso specifico in cui la distribuzione a posteriori (o distribuzione target) è nota. Nella presente simulazione useremo una distribuzione target Normale di parametri \\(\\mu\\) = 3 e \\(\\sigma^2\\) = 1.\nSupponiamo però di non conoscere le proprietà della distribuzione target, ma di avere a disposizione solo un campione casuale di \\(n\\) osservazioni tratte da questa distribuzione. Il problema che ci poniamo è quello di stimare i parametri della distribuzione, ipotizzando che sia una Normale, alla luce dei dati osservati.\nI dati sono contenuti in un vettore chiamato \\(y\\), con \\(y_1, y_2, \\dots, y_n\\). In R abbiamo:\n\n\nset.seed(123)\ny <- rnorm(20, mean=3, sd=1)\n\n\n\nObiettivo\nIl nostro obiettivo è quello di trovare la distribuzione a posteriori congiunta dei parametri sconosciuti \\(\\mu\\) e \\(\\sigma^2\\) di una distribuzione Normale. Ipotizziamo infatti che i nostri dati provengano da una tale distribuzione e vogliamo trovare i parametri maggiormente plausibili della distribuzione, alla luce dei dati osservati.\nSe applichiamo il teorema di Bayes otteniamo la seguente espressione:\n\\[\np(\\mu, \\sigma^2 \\mid y) = \\frac{p(y \\mid \\mu, \\sigma^2) \\cdot p(\\mu, \\sigma^2)}{\\int\\int p(y \\mid \\mu, \\sigma^2) \\cdot p(\\mu, \\sigma^2) \\cdot d\\mu \\cdot d\\sigma^2}.\n\\]\nCi focalizziamo qui sul numeratore in quanto il denominatore è solo una costante:\n\\[\np(\\mu, \\sigma^2 \\mid y) \\propto p(y \\mid \\mu, \\sigma^2) \\cdot p(\\mu, \\sigma^2).\n\\]\nNon siamo tanto interessati alla distribuzione a posteriori congiunta dei parametri, quanto alle distribuzioni marginali a posteriori per ciascun parametro sconosciuto: \\(p(\\mu \\mid y)\\) e \\(p(\\sigma^2 \\mid y)\\).\nI vari termini dell’espressione precedente corrispondono alle quantità seguenti:\ndistribuzione a posteriori congiunta: \\(p(\\mu, \\sigma^2 \\mid y)\\),\nverosimiglianza, \\(p(y \\mid \\mu, \\sigma^2)\\),\ndistribuzione a priori, \\(p(\\mu, \\sigma^2)\\).\nEsplicitiamo ora ciascuno dei termini precedenti.\nDistribuzioni a priori\nSe assumiamo che \\(\\mu\\) e \\(\\sigma^2\\) sono indipendenti, possiamo riscrivere la distribuzione a priori congiunta come il prodotto di due distribuzioni a priori:\n\\[\np(\\mu, \\sigma^2) = p(\\mu) \\cdot p(\\sigma^2).\n\\]\nPer iniziare, assumiamo una distribuzione a priori uniforme per entrambi i parametri \\(\\mu\\) e \\(\\sigma^2\\), ipotizzando un qualche intervallo ragionevole:\n\\[\np(\\mu) = Unif(-50, 50),\\\\\np(\\sigma^2) = Unif(0, 100).\n\\]\nCreaiamo una funzione R che ritorna le densità a priori per \\(\\mu\\) e \\(\\sigma^2\\). Per comodità le esprimiamo su una scala logaritmica:\n\n\nprior_mu <- function(mu) {\n  dunif(mu, min=-50, max=50, log=TRUE)\n}\n  \nprior_sigma2 <- function(sigma2) {\n  dunif(sigma2, min=0, max=100, log=TRUE)\n}\n\n\n\nEsaminiamo le distribuzioni a priori:\n\n\npar(mfrow = c(1, 2))\n\nmus <- seq(from=-50, to=50, length.out = 100)\nsigmas2 <- seq(from=0, to=100, length.out = 100)\n\n# Densità di mu\nplot(\n  mus, \n  prior_mu(mus), \n  type = 'l', \n  col = \"red\", \n  xlab = expression(mu), \n  ylab = \"Densità\"\n)\n\n# Densità di sigma2\nplot(\n  sigmas2, \n  prior_sigma2(sigmas2), \n  type = 'l', \n  col = \"red\", \n  xlab = expression(sigma^2), \n  ylab = \"Densità\"\n)\n\n\n\n\nVerosimiglianza\nPer \\(n\\) osservazioni indipendenti tratte da una Normale, la verosimiglianza è data dalla seguente espressione:\n\\[\np(y \\mid \\mu, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(y_i-\\mu)^2}{2\\sigma^2}}.\n\\]\nDato che le osservazioni sono indipendenti, la verosimiglianza che si verifichino simultaneamente è data dal prodotto della verosimiglianza di ciascuna separata osservazione. Ma qual è la verosimiglianza che una singola osservazione si verifichi? Ciascuna osservazione è una realizzazione di una Normale di parametri \\(\\mu\\) e \\(\\sigma^2\\), dunque la plausibilita di \\(X=x\\) è data dalla curva di densità di una Normale di parametri \\(\\mu\\) e \\(\\sigma^2\\). Il nostro problema è che non conosciamo \\(\\mu\\) e \\(\\sigma^2\\), ma questo non è un problema. Sappiamo infatti che la funzione di verosimiglianza descrive la verosimiglianza relativa dei dati considerati tutti i possibili valori che i parametri possono assumere.\nSe sappiamo calcolare la funzione di verosimiglianza per una singola osservazione \\(y\\), è facile trovare la verosimiglianza congiunta per tutte le osservazioni del campione. Se il campione è casuale, allora le osservazioni sono indipendenti ed è sufficiente fare il prodotto delle singole verosimiglianze. La seguente funzione R ci consente di calcolare tale verosimiglianza congiunta per tutte le osservazioni del campione:\n\n\nlikelihood <- function(data, mu, sigma){\n  prod(dnorm(data, mean=mu, sd=sigma))\n}\n\n\n\nPer creare una rappresentazione grafica del risultato che abbiamo ottenuto, consideriamo una sequenza di valori possibili per il parametro \\(\\mu\\) e teniamo costante il valore \\(\\sigma^2\\). In questo modo otterremo una curva anziché una superficie. Per la presente simulazione utilizziamo i valori indicati qui sotto:\n\n\n# Valori per il parametro mu\nmus <- seq(from=-5, to=10, length.out = 1e3)\n# Teniamo costante il valore sigma2\nstd <- sqrt(4)\n# Utilizziamo i dati del campione\ndata <- y\n# Calcoliamo i valori della funzione di verosimiglianza per mu\nlikelihood_vector <- 0\nfor (i in 1:length(mus)) {\n  likelihood_vector[i] <- likelihood(data, mu=mus[i], sigma=std)\n}\n\n\n\nCreaimo un grafico della verosimiglianza:\n\n\nplot(\n  mus,\n  likelihood_vector,\n  type = 'l',\n  col = \"red\",\n  xlab = expression(mu),\n  ylab = \"Verosimiglianza\"\n)\n\n\n\n\nSi noti che i valori di verosimiglianza sono molto piccoli. Per evitare problemi numerici nei calcoli usiamo i logaritmi. Di conseguenza, possiamo ri-scrivere il prodotto della distribuzione Normale come una somma di elementi.\n\n\nloglikelihood <- function(data, mu, sigma){\n  sum(dnorm(data, mean=mu, sd=sigma, log=TRUE))\n}\n\n\n\nCreaiamo il diagramma per la log-verosimiglianza:\n\n\nmus <- seq(from=-5, to=10, length.out = 1e3)\nstd <- sqrt(4)\ndata <- y\nloglikelihood_vector <- 0\n\nfor(i in 1:length(mus)){\n  loglikelihood_vector[i] <- loglikelihood(data, mu=mus[i], sigma=std)\n}\n\nplot(\n  mus, \n  loglikelihood_vector,\n  type='l',\n  col=\"red\",\n  xlab=expression(mu),\n  ylab=\"Log-verosimiglianza\")\n\n\n\n\nL’algoritmo di Metropolis\nSiamo ora pronti per applicare l’algoritmo Metropolis in modo da poter campionare dalla distribuzione a posteriori congiunta dei parametri \\(\\mu\\) e \\(\\sigma^2\\). Quello che facciamo in ciascuna interazione è trovare una coppia di valori proposti per i parametri sconosciuti \\(\\mu\\) e \\(\\sigma^2\\) per poi se accettare o meno la proposta fatta. L’algoritmo di Metropolis può essere descritto nel modo seguente.\na. Inizializziamo il vettore dei parametri \\(\\theta^0 = (\\mu^0, sigma^0)\\) (per semplificare la notazione facciamo qui riferimento alla deviazione standard anziché alla varianza).\nb. Scegliamo a caso due valori proposti \\(\\mu^p\\) e \\(\\sigma^p\\) estraendoli dalla distribuzione proposta. La distribuzione proposta è una distribuzione simmetrica centrata sul valore del parametro nel passo precedente della catena. Qui utilizzo, quale distribuzione proposta, una distribuzione Normale.\nc. Calcoliamo il rapporto tra le densità\n\\[\nr = \\frac{p(\\mu^p, \\sigma^p \\mid y)}{p(\\mu^{i-1}, \\sigma^{i-1} \\mid y)} = \\frac{p(y \\mid \\mu^p, \\sigma^p)p(\\mu^p)p(\\sigma^p)}{p(y \\mid \\mu^{i-1}, \\sigma^{i-1})p(\\mu^{i-1})p(\\sigma^{i-1})}.\n\\]\nIl termine al numeratore rappresenta il prodotto tra la verosimiglianza dei dati alla luce dei valori proposti dei parametri e le distribuzioni a priori assegnate ai valori proposti dei parametri, nell’ipotesi che \\(\\mu\\) e \\(\\sigma\\) siano indipendenti. In altri termini, il numeratore corrisponde alla densità a posteriori del valore proposto dei parametri, alla luce dei dati. Al denominatore abbiamo la densità a posteriori del valore dei parametri nel passo precedente della catena, alla luce dei dati.\nIl rapporto tra queste due densità (ovvero tra le ordinate delle due funzioni a posteriori) si chiede la seguente domanda: alla luce dei dati, risultano più plausibili i valori proposti dei parametri o quelli del passo precedente della catena?\nSe il rapporto è maggiore di 1 allora i valori proposti vengono sempre accettati in quanto la densità a posteriori calcolata con i nuovi parametri proposti è maggiore della densità a posteriore calcolata con i valori dei parametri definiti nell’interazione precedente della catena.\nAltrimenti, decidiamo di tenere i valori proposti con una probabilità minore di 1, ovvero non sempre, ma soltanto con una probabilità definita dal metodo descritto nei punti d, e. In altri termini, la probabilità con la quale i valori proposti sono accettati è uguale al rapporto \\(r\\): se \\(r\\) è uguale a 0.10, ad esempio, questo significa che la densità a posteriori calcolata con i valori proposti è 10 volte più piccola della densità a posteriori calcolata con i valori dell’interazione precedente. In queste circostanze, i valori proposti verranno accettati solo il 10% delle volte. L’effetto di questo algoritmo è quello di consentirci di “campionare” la distribuzione a posteriori in modo tale che la scelta dei valori accettati sia proporzionale alla densità dei valori nella distribuzione a posteriori.\nIn termini logaritmici, il rapporto \\(r\\) descritto sopra diventa:\n\\[\n\\begin{align}\nlog(r) &= log \\Bigg( \\frac{p(\\mu^p, \\sigma^p \\mid y)}{p(\\mu^{i-1}, \\sigma^{i-1} \\mid y)}\\Bigg) = log\\Big(p(y \\mid \\mu^p, \\sigma^p)p(\\mu^p)p(\\sigma^p)\\Big) - \\notag\\\\\n&log\\Big(p(y \\mid \\mu^{i-1}, \\sigma^{i-1})p(\\mu^{i-1})p(\\sigma^{i-1})\\Big),\\notag\n\\end{align}\n\\]\novvero\n\\[\n\\begin{align}\nlog(r) = \\Bigg(&log\\Big(p(y \\mid \\mu^p, \\sigma^p)\\Big) + log\\Big( p(\\mu^p)\\Big) + log \\Big( p(\\sigma^p)\\Big)\\Bigg) - \\notag\\\\\n\\Bigg(&log\\Big(p(y \\mid \\mu^{i-1}, \\sigma^{i-1})\\Big) + log\\Big( p(\\mu^{i-1})\\Big) + log \\Big( p(\\sigma^{i-1})\\Big)\\Bigg). \\notag\n\\end{align}\n\\]\nd. Scegliamo un u a caso da una distribuzione uniforme, \\(Unif(0, 1)\\).\ne. Decidiamo se tenere i valori proposti dei parametri: se \\(u < min(1, r)\\) allora\n\\[\n\\theta^{i+1} = \\theta^p,\n\\]\naltrimenti\n\\[\n\\theta^{i+1} = \\theta^i.\n\\]\nf. Ritorniamo al punto b dell’algoritmo di Metropolis e ripetiamo il processo fino a raggiungere il numero prefissato di iterazioni.\nImplementazione in R\nUsiamo i dati contenuti nel vettore y:\n\n\ndata <- y\n\n\n\nDefiniamo la funzione di log-verosimiglianza:\n\n\njoint <- function(data, mu, sigma){\n  loglikelihood(data, mu, sigma) + prior_mu(mu) + prior_sigma2(sigma^2)\n}\n\n\n\nScriviamo una funzione R per l’algoritmo di Metropolis:\n\n\nrun_metropolis <- function(data, startvalue, iterations, sigma_proposal_mu) {\n  \n  # Matrice dove salvare i due parametri della catena\n  # numero di righe = numero di iterazioni + 1 \n  # numbero delle colonne = numero dei parametri (ovvero 2)\n  chain <- array(dim = c(iterations+1, 2))\n  \n  # Inizializzo la prima riga della catena con i valori iniziali (arbitrari) dei \n  # parametri\n  chain[1, ] = startvalue\n  \n  # variabili che consentono di salvare il numero di valori accettati e rigettati\n  n_accept <- 0\n  n_reject <- 0\n  \n  # Loop con l'algoritmo di Metropolis \n  for (i in 1:iterations) {\n    \n    # Il valore proposto mu è estratto a caso da una Normale con media uguale al \n    # valore mu dell'interazione precedente e deviazione standard pari a \n    # sigma_proposal_mu.\n    proposal_mu <- rnorm(1, mean=chain[i, 1], sd=sigma_proposal_mu)\n  \n    # Il valore proposto sigma è estratto a caso da una distribuzione \n    # Uniforme(-0.5, 0.5) che viene sommato al valore del parametro sigma \n    # nell'interazione precedente \n    proposal_sigma <- chain[i,2] + runif(1, min = -0.5, max=0.5)\n    # Logaritmo del rapporto tra le densità a posteriori: al mumeratore la \n    # densità calcolata con i parametri prposti, al denominatore quella \n    # calcolata con i parametri dell'interazione precedente.\n    r_ratio <- joint(data, proposal_mu, proposal_sigma) - \n      joint(data, chain[i, 1],chain[i, 2])\n    \n    # Qui viene affronatato un problema prettamente numerico: se per qualche \n    # ragione il rapporto r_ratio non è un valore numerico, allora si ripete il \n    # presente passo della catena.\n    if (exp(r_ratio) == \"NaN\") {\n      chain[i+1,] <- chain[i, ]\n      n_reject <- n_reject+1\n    }\n    else\n    # Si estrae un numero casuale tra 0 e 1 e lo si confronta con il valore di \n    # r_ratio. Dal momento che abbiamo preso il logaritmo di r_ratio, è \n    # necessario esponenziare per riportare r_ratio sulla scala corretta.\n      if (runif(1, 0, 1) < min(1, exp(r_ratio))) {\n        chain[i+1, ] <- c(proposal_mu, proposal_sigma)\n        # Si conta il numero di volte in cui i parametri proposti vengono \n        # accettati -- un valore sensato del tasso di accettazione è di circa il\n        # 20%\n        n_accept <- n_accept+1\n      }\n    \n    else {\n      # Rigetto i valori proposti e, per il prossimo passo della catena, tengo \n      # i valori correnti dei parametri  \n      chain[i+1,] = chain[i,]\n      n_reject = n_reject+1\n    }\n  }\n  # Creo una lista in cui salverò i valori dei parametri nella catena e il\n  # numero dei valori dei parametri che vengono accettati dall'algoritmo di \n  # Metropolis e il numero dei valori che vengono rifiutati \n  list_c = list(chain, n_accept, n_reject)\n  \n  return(list_c)\n}\n\n\n\nPrima di lanciare la simulazione definiamo i parametri necessari:\n\n\n# Numero di iterazioni\niterations <- 1e6\n# Valori iniziali dei parametri mu e sigma\nstartvalue <- c(1, 1)\n# I dati\ndata <- y\n# Per manipolare il tasso di accettazione, trovo empiricamente un valore \n# per la varianza della distribuzione proposta del parametro mu.\nsigma_proposal_mu <- 1.0\n\n\n\nLancio ora l’algoritmo di Metropolis e salvo l’output nella lista ´results_m´:\n\n\nresults_m <- \n  run_metropolis(data, startvalue, iterations, sigma_proposal_mu)\n\n\n\nCalcolo il tasso di accettazione:\n\n\nacpt_rate <- results_m[[2]] / (results_m[[3]])\nacpt_rate\n\n\n[1] 0.1981716\n\nIl primo elemento della lista chain_n è una matrice che contiene i valori che sono stati campionati dall’algoritmo di Metropolis dalla distribuzione a posteriori dei parametri:\n\n\nchain_m <- results_m[[1]]\n\n\n\nUna rappresentazione grafica dei risultati prodotti dall’algoritmo di Metropolis è fornita qui di seguito.\n\n\npar(mfrow=c(2, 2))\n\nplot(chain_m[, 1], type=\"l\", ylab=expression(mu), xlab=\"Iterazione\")\nabline(h=mean(chain_m[-5000, 1]), col=\"red\")\nabline(h=3, col=\"blue\", lty=2)\n\nplot(chain_m[, 2], type=\"l\", ylab=expression(sigma), xlab=\"Iterazione\")\nabline(h=mean(chain_m[-5000,2]), col=\"red\")\nabline(h=1, col=\"blue\", lty=2)\nlegend(\n  8000, 5.6, c(\"Media della catena\",\"Valore vero della media\"), \n  col=c(\"red\",\"blue\"), lty=c(1,2), cex=1, border=\"black\", bty=\"o\", \n  box.lwd=1, box.col=\"black\", bg=\"white\"\n)\n\nplot(density(chain_m[, 1], bw=0.05), type=\"l\", xlab=expression(mu), main=\"\")\nabline(v=mean(chain_m[-5000,1]), col=\"red\", lty=1)\nabline(v=3,col=\"blue\",lty=2)\n\nplot(density(chain_m[-5000,2]), type=\"l\", xlab=expression(sigma), main=\"\")\nabline(v=mean(chain_m[-5000,2]), col=\"red\", lty=1)\nabline(v=1, col=\"blue\", lty=2)\n\n\n\n\nLa stima a posteriori per il parametro \\(\\mu\\) è:\n\n\nmean(chain_m[-5000, 1])\n\n\n[1] 3.141238\n\nLa stima a posteriori per il parametro \\(\\sigma^2\\) è:\n\n\nmean(chain_m[-5000, 2])\n\n\n[1] 1.043072\n\nIn conclusione, l’algoritmo ha funzionato: sulla base delle informazioni fornite dai dati siamo riusciti a trovare delle stime ragionevoli dei parametri della distribuzione Normale da cui il campione di dati è stato estratto.\nDistribuzione a priori non uniforme per \\(\\mu\\)\nPoniamoci ora il problema di studiare l’influenza delle distribuzoni a priori. Nella simulazione precedente abbiamo utilizzato delle distribuzioni a priori uniformi. Ma che cosa succede se vengono utilizzate delle distribuzioni a priori centrata su valori molto lontani dai valori veri dei parametri?\nPer rispondere a questa domanda, lasciamo inalterata la distribuzione a priori di \\(\\sigma^2\\) e scegliamo la seguente distribuzione a priori per il parametro \\(\\mu\\): \\(\\mathcal{N}(\\mu=20, \\sigma=1)\\). In altri termini, affermiamo che, prima di guardare i dati, i valori plausibili per il parametro \\(\\mu\\) sono valori nell’interno di 20. In realtà, sappiamo che, nella simulazione, il valore corretto è \\(\\mu\\) = 3. Pertanto, definendo una distribuzione a priori per \\(\\mu\\) molto lontana dal “valore vero” di \\(\\mu\\), ci chiediamo che effetto avrà questa credenza a priori sulla distribuzione a posteriori.\nRicordiamo che, come nella simulazione precedente, anche ora il campione è molto piccolo: \\(n\\) = 20.\n\n\nprior2_mu <- function(mu) {\n  dnorm(mu, 30, 2, log=TRUE)\n}\n  \nprior_sigma2 <- function(sigma2) {\n  dunif(sigma2, min=0, max=100, log=TRUE)\n}\n\n\n\nCalcoliamo la log-verosimiglianza\n\n\ndata <- y\njoint <- function(data, mu, sigma){\n  loglikelihood(data, mu, sigma) + prior2_mu(mu) + prior_sigma2(sigma^2)\n}\n\n\n\ne lanciamo l’algoritmo di Metropolis:\n\n\nsigma_proposal_mu <- 6.0\nresults_m <- \n  run_metropolis(data, startvalue, iterations, sigma_proposal_mu)\n\n\n\nIl tasso di accettazione è ragionevole:\n\n\nacpt_rate <- results_m[[2]] / (results_m[[3]])\nacpt_rate\n\n\n[1] 0.2327735\n\nEsaminiamo i risultati:\n\n\nchain_m <- results_m[[1]]\n\n\n\nLa stima a posteriori per il parametro \\(\\mu\\) è:\n\n\nmean(chain_m[-5000, 1])\n\n\n[1] 17.53029\n\nLa stima a posteriori per il parametro \\(\\sigma^2\\) è:\n\n\nmean(chain_m[-5000, 2])\n\n\n[1] 9.619287\n\nIn conclusione, se il campione è piccolo, le stime a posteriori sono fortemente influenzate dalle distribuzioni a priori dei parametri. Quindi, una distribuzione a priori informativa ma molto lontana dal “valore vero” può influenzare fortemente le conclusioni a cui giungiamo. In questo caso le conclusioni risultano fortemente distorte.\nUn campione più grande\nAumentiamo ora l’ampiezza del campione:\n\n\nset.seed(123)\ny2 <- rnorm(200, mean=3, sd=1)\n\n\n\nUtilizziamo le stesse distribuzioni a priori della simulazione precedente:\n\n\nprior2_mu <- function(mu) {\n  dnorm(mu, 30, 2, log=TRUE)\n}\n  \nprior_sigma2 <- function(sigma2) {\n  dunif(sigma2, min=0, max=100, log=TRUE)\n}\n\n\n\nCalcoliamo la log-verosimiglianza per i nuovi dati\n\n\ndata <- y2\n\njoint <- function(data, mu, sigma) {\n  loglikelihood(data, mu, sigma) + prior2_mu(mu) + prior_sigma2(sigma^2)\n}\n\n\n\ne lanciamo l’algoritmo di Metropolis:\n\n\nsigma_proposal_mu <- 0.002\nresults_m <- \n  run_metropolis(data, startvalue, iterations, sigma_proposal_mu)\n\n\n\nIl tasso di accettazione è ragionevole:\n\n\nacpt_rate <- results_m[[2]] / (results_m[[3]])\nacpt_rate\n\n\n[1] 0.1822413\n\nEsaminiamo i risultati:\n\n\nchain_m <- results_m[[1]]\n\n\n\nLa stima a posteriori per il parametro \\(\\mu\\) è:\n\n\nmean(chain_m[-5000, 1])\n\n\n[1] 2.975561\n\nLa stima a posteriori per il parametro \\(\\sigma^2\\) è:\n\n\nmean(chain_m[-5000, 2])\n\n\n[1] 0.9684834\n\nIn conclusione, all’aumentare della grandezza del campione la verosimiglianza si rafforza e la distribuzione a posteriori dei parametri risulta influenzata in maniera molto minore dalla distribuzione a priori. Per cui, se abbiamo tante informazioni, ovvero un campione grande di dati, non importa quali distribuzione a priori scegliamo: la soluzione a posteriori converge sul risultato corretto.\n\n\n\n",
    "preview": "posts/2021-03-25-un-tutorial-sullalgoritmo-di-metropolis/un-tutorial-sullalgoritmo-di-metropolis_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-26T07:09:07+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-24-la-campana-di-gauss/",
    "title": "La campana di Gauss",
    "description": "Una prima occhiata alla distribuzione gaussiana.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-24",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\n\nContents\nFunzione gaussiana\nLa funzione di ripartizione\nQuantili e densità\n\nLa probabilità\nIl valore atteso\nLa varianza\nL’interpretazione dei parametri\n\nCarichiamo i pacchetti che ci servono.\n\n\nlibrary(\"tidyverse\", warn.conflicts = FALSE)\nlibrary(\"gghighlight\")\n\n\n\nFunzione gaussiana\nUna funzione gaussiana è una funzione della seguente forma:\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \n  \\exp\\left( -\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{\\!2}\\,\\right)\n\\]\nladdove \\(\\mu\\) e \\(\\sigma\\) sono i paramemtri della distribuzione.\nIn R la formula diventa:\n\n\ngauss <- function(x, mu, sigma) {\n  1 / sqrt(2 * pi * sigma^2) * exp(-(x - mu)^2 / (2 * sigma^2)) \n}\n\n\n\nL’area è unitaria, per qualsiasi valore \\(\\mu\\) e \\(\\sigma\\):\n\n\narea <- integrate(\n  gauss, \n  mu = 100,\n  sigma = 15,\n  lower = -Inf, \n  upper = Inf)$value\narea\n\n\n[1] 1\n\n\n\narea <- integrate(\n  gauss, \n  mu = 0,\n  sigma = 1,\n  lower = -Inf, \n  upper = Inf)$value\narea\n\n\n[1] 1\n\nIl fatto che la distribuzione gaussiana dipende dai parametri \\(\\mu\\) e \\(\\sigma\\) significa che, al variare del valore dei parametri varia la forma della curva di densità: la variazione di \\(\\mu\\) trasla la curva di densità in maniera rigida sull’asse \\(\\mathbb{R}\\); la variazione di \\(\\sigma\\) allarga e appiattisce la curva di densità.\nDisegnamo la funzione di densità usando, come parametri, \\(\\mu\\) = 100 e \\(\\sigma\\) = 15 – ovvero, specifichiamo la distribuzione del QI.\n\n\nmu <- 100\nsigma <- 15\nx <- seq(55, 145, length.out = 1e3)\nplot(x, gauss(x, mu, sigma), type = 'l', ylab = \"Densità\")\n\n\n\n\nLo stesso risultato si ottene con\n\n\nplot(x, dnorm(x, 100, 15), type = 'l', ylab = \"Densità\")\n\n\n\n\nLa funzione di ripartizione\n\n\ncurve(\n  pnorm(x), \n  xlim = c(-3.5, 3.5), \n  ylab = \"Probabilità\", \n  main = \"Funzione cumulativa della normale standardizzata\"\n)\n\n\n\n\nQuantili e densità\nDefiniamo i seguenti quantili e calcoliamo la densità corrispondente per il caso della normale standardizzata:\n\n\nquants <- c(-1.96, 0, 1.96)\ngauss(quants, 0, 1)\n\n\n[1] 0.05844094 0.39894228 0.05844094\n\nLo stesso risultato si ottene con\n\n\ndnorm(quants, 0, 1)\n\n\n[1] 0.05844094 0.39894228 0.05844094\n\nLa probabilità\nCalcoliamo ora le probabilità, ovvero le aree. Iniziamo con la probabilità \\(P(X < 115)\\) per il QI.\n\n\ndf <- data.frame(x = seq(55, 165, length.out = 100)) %>% \n  mutate(y = dnorm(x, mean=100, sd=15))\n\nggplot(df, aes(x, y)) + \n  geom_area(fill = \"sky blue\") + \n  gghighlight(x < 115) +\n  labs(\n    x = \"QI\",\n    y = \"Densità\"\n  ) +\n  papaja::theme_apa()\n\n\n\n\nIl risultato cercato è\n\n\nintegrate(\n  gauss, \n  mu = 100,\n  sigma = 15,\n  lower = -Inf, \n  upper = 115)\n\n\n0.8413447 with absolute error < 3.8e-06\n\novvero\n\n\npnorm(115, 100, 15)\n\n\n[1] 0.8413447\n\nConsideriamo ora l’area sottesa alla funzione di densità nell’intervallo \\(\\mu \\pm 1.96 \\sigma\\).\n\n\ndf <- data.frame(x = seq(55, 165, length.out = 100)) %>% \n  mutate(y = dnorm(x, mean=100, sd=15))\n\nggplot(df, aes(x, y)) + \n  geom_area(fill = \"sky blue\") + \n  gghighlight(x > 100 - 1.96*15 & x < 100 + 1.96*15) +\n  labs(\n    x = \"QI\",\n    y = \"Densità\"\n  ) +\n  papaja::theme_apa()\n\n\n\n\nLa probabilità cercata è\n\n\nintegrate(\n  gauss, \n  mu = 100,\n  sigma = 15,\n  lower = 100 - 1.96*15, \n  upper = 100 + 1.96*15)\n\n\n0.9500042 with absolute error < 1e-11\n\novvero\n\n\npnorm(100 + 1.96 * 15, 100, 15) - pnorm(100 - 1.96 * 15, 100, 15)\n\n\n[1] 0.9500042\n\nIl valore atteso\nIl valore atteso di una variabile aleatoria continua \\(X\\) è\n\\[\n\\mathbb{E}(X) = \\mu_X = \\int xf_X(x)dx.\n\\]\nPer calcolare il valore numerico dell’integrale con R definiamo la seguente funzione:\n\n\ng <- function(x) x * gauss(x, 100, 15)\n\n\n\nPossiamo ora usare integrate() per trovare la soluzione che cerchiamo:\n\n\nEX <- integrate(\n  g,\n  lower = -Inf,\n  upper = Inf\n)$value\nEX\n\n\n[1] 100\n\nLa varianza\nLa varianza di una variabile aleatoria continua \\(X\\) è\n\\[\nvar(X) = \\mu_X = \\int (x - \\mu)^2f_X(x)dx.\n\\] In R definiamo la seguente funzione\n\n\nh <- function(x) x^2 * gauss(x, 100, 15)\n\n\n\ne poi calcoliamo l’integrale\n\n\nVarX <- integrate(\n  h,\n  lower = -Inf,\n  upper = Inf\n)$value - EX^2 \nVarX\n\n\n[1] 225\n\nsqrt(VarX)\n\n\n[1] 15\n\nL’interpretazione dei parametri\nIn conclusione, la distribuzione gaussiana dipende da due parametri: \\(\\mu\\) e \\(\\sigma^2\\). Tali parametri corrispondono al valore atteso (cioè alla media) e alla varianza (cioè alla dispersione dei valori attorno al massimo della curva) della distribuzione.\n\n\n\n",
    "preview": "posts/2021-03-24-la-campana-di-gauss/la-campana-di-gauss_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-03-24T14:22:05+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-23-esercizi-sulla-distribuzione-binomiale/",
    "title": "Esercizi sulla distribuzione binomiale",
    "description": "Sequenze di prove Bernoulliane.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-23",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\nLa funzione dbinom() fornisce le probabilità per possibili valori di una variabile aleatoria binomiale. Come minimo richiede tre argomenti. Il primo argomento è un vettore di quantili (i possibili valori della variabile aleatoria X). Il secondo e il terzo argomento sono i parametri che definiscono la distribuzione binomiale, vale a dire, n (il numero di prove Bernoulliane indipendenti) e p (la probabilità di successo in ciascuna prova). Ad esempio, per una distribuzione binomiale con n = 5, p = 0.5, i valori possibili per X sono 0,1,2,3,4,5. La funzione dbinom(x, n, p) ritornerà i valori di probabilità \\(P(X = x)\\) per \\(x = 0, 1, 2, 3, 4, 5\\). Ovvero\n\n\nn <- 5\np <- 0.5\nx <- 0:n\ndbinom(x, n, p)\n\n\n[1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125\n\nIl grafico della distribuzione di probabilità binomiale può essere visualizzato come indicato nella figura seguente:\n\n\nx <- 0:12\nprob <- dbinom(x, 12, 0.5)\nbarplot(\n  prob,\n  col = \"red\",\n  ylim = c(0,.2),\n  names.arg=x,\n  main=\"Distribuzione Binomiale (n=12, p=0.5)\"\n)\n\n\n\n\nSi noti che la distribuzione binomiale è simmetrica quando p = 0,5. Per dimostrare che la distribuzione binomiale ha un’asimmetria negativa quando p è maggiore di 0.5, si consideri il seguente esempio:\n\n\nn <- 9\np <- .8 \nx <- 0:n\nprob <- dbinom(x, n, p)\nbarplot(\n  prob,\n  names.arg = x,\n  main=\"Distribuzione Binomiale (n=9, p=0.7)\",\n  col=\"lightblue\"\n)\n\n\n\n\nQuando p è minore di 0.5 la distribuzione binomiale è ha un’asimmetria positiva come indicato di seguito.\n\n\nn <- 9\np <- .2 \nx <- 0:n\nprob <- dbinom(x, n, p)\nbarplot(\n  prob,\n  names.arg = x,\n  main=\"Distribuzione Binomiale (n=9, p=0.3)\",\n  col=\"lightblue\"\n)\n\n\n\n\nIllustreremo ora l’uso della funzione di distribuzione cumulativa pbinom(). Questa funzione può essere utilizzata per calcolare la probabilità \\(P (X \\leq x)\\). Il primo argomento di questa funzione è un vettore di quantili (valori di x).\nAd esempio: si calcoli la probabilità \\(P(X \\leq 2)\\) nella distribuzione Bin(n=5, p=0.5):\n\n\npbinom(2, 5, 0.5)\n\n\n[1] 0.5\n\novvero\n\n\nsum(dbinom(0:2, 5, 0.5))\n\n\n[1] 0.5\n\nPer calcolare probabilità del tipo \\(P( a \\leq X \\leq b)\\) procediamo nel modo seguente:\n\n\n# P(3<= X <= 5) = P(X=3) + P(X=4) + P(X=5) in a Bin(n=9,p=0.6) dist\nsum(dbinom(c(3,4,5), 9, 0.6))\n\n\n[1] 0.4923556\n\nPossiamo presentare la distribuzione Binomiale in una tabella nel modo seguente:\n\n\nn <- 10\np <- 0.4\nx <- 0:n\nprob = dbinom(x, n, p) \ncdf = pbinom(x, n, p) \ndist_table = cbind(x, prob, cdf)\ndist_table\n\n\n       x         prob         cdf\n [1,]  0 0.0060466176 0.006046618\n [2,]  1 0.0403107840 0.046357402\n [3,]  2 0.1209323520 0.167289754\n [4,]  3 0.2149908480 0.382280602\n [5,]  4 0.2508226560 0.633103258\n [6,]  5 0.2006581248 0.833761382\n [7,]  6 0.1114767360 0.945238118\n [8,]  7 0.0424673280 0.987705446\n [9,]  8 0.0106168320 0.998322278\n[10,]  9 0.0015728640 0.999895142\n[11,] 10 0.0001048576 1.000000000\n\nLa funzione rbinom() viene usata per generare campioni casuali di una determinata ampiezza avendo specificato una distribuzione Binomiale di determinati parametri.\n\n\nx_val <- names(table(rbinom(1000, 8, .5)))\nbarplot(\n  as.vector(table(rbinom(1000, 8, 0.5))), names.arg = x_val,\n  main=\"Simulazione di una distribuzione Binomiale\\n di parametri n=8, p=0.5\"\n)\n\n\n\n\nEsempio 1\nIl database di uno psicologo mostra che dei pazienti affetti da ansia sociale, il 60% migliora dopo il trattamento. Qual è la probabilità che, se estraiamo a caso le cartelle di 10 pazienti, esattamente 6 di questi mostrino un miglioramento dopo il trattamento?\nTroviamo la soluzione applicando la funzione dbinom():\n\n\nn <-  10\np <- 0.60 \nx <- 6\ndbinom(x, n, p)\n\n\n[1] 0.2508227\n\n\nEsempio 2\nUn test è costituito da 6 domande con 4 alternative di risposta ciascuna. Se un rispondente completa il test ma si limita a tirare a caso, qual è la probabilità di almeno una risposta corretta?\n\n\n1 - dbinom(0, 6, 1/4)\n\n\n[1] 0.8220215\n\n\nEsempio 3\nSe una moneta bilanciata viene lanciata 8 volte, qual è la probabilità di ottenere più di cinque volte testa?\n\n\n1 - pbinom(5, 8, 0.5)\n\n\n[1] 0.1445312\n\n\nEsempio 4\nUno psicologo realizza un esperimento in cui la difficoltà delle prove viene stablita per ogni soggetto in maniera tale da produrre 3 errori in 5 prove. Si consideri un campione casuale di 10 prove. Si trovi la probabilità di osservare (a) esattamente 6 errori, (b) meno di 9 errori.\n\n\ndbinom(6, 10, .6)\n\n\n[1] 0.2508227\n\n\n\npbinom(8, 10, .6)\n\n\n[1] 0.9536426\n\n\nEsempio 5\nUn dado truccato produce il risultato ‘6’ in 8 lanci su 30. Il dado viene lanciato 12 volte. Qual è il valore atteso del numero di volte in cui si osserverà il risultato ‘6’? Qual è la deviazione standard del numero di volte in cui si osserva ‘6’?\n\n\n12 * 8/30\n\n\n[1] 3.2\n\n\n\nsqrt(12 * 8/30 * (1 - 8/30))\n\n\n[1] 1.531883\n\n\nEsempio 6\nSe gli eventi \\(A\\) e \\(B\\) sono indipendenti, e \\(A\\) ha una probabilità del 50% di verificarsi e \\(B\\) ha una probabilità del 30% di verificarsi, qual è la probabilità che vengano osservati insieme? Si risponda usando una simulazione in R.\n\n\nn <- 1e7\nA <- rbinom(n, 1, 0.5)\nB <- rbinom(n, 1, 0.3)\nmean(A & B) \n\n\n[1] 0.150216\n\n\nEsempio 7\nSe gli eventi \\(A\\) e \\(B\\) sono indipendenti, e \\(A\\) ha una probabilità del 40% di verificarsi e \\(B\\) ha una probabilità del 75% di verificarsi, qual è la probabilità che venga osservato uno di essi oppure l’altro? Si risponda usando una simulazione in R.\n\n\nn <- 1e7\nA <- rbinom(n, 1, 0.4)\nB <- rbinom(n, 1, 0.75)\nmean(A | B) \n\n\n[1] 0.8500116\n\n\nEsempio 8\nSupponiamo che \\(X\\) sia una variabile aleatoria Binomiale di parametri (10, 0.3) e \\(Y\\) sia un’altra variabile aleatoria Binomiale di parametri (10, 0.65); supponiamo che \\(X\\) e \\(Y\\) siano indipendenti. Qual è la probabilità che una di queste variabili sia minore o uguale a 5? Stimare questa probabilità utilizzando una simulazione e calcolare la probabilità esatta.\n\n\nn <- 1e7\nX <- rbinom(n, 10, 0.3)\nY <- rbinom(n, 10, 0.65)\nmean(X<=5 | Y<=5)\n\n\n[1] 0.9644439\n\noppure\n\n\nprob_X_less <- pbinom(5, 10, 0.3)\nprob_Y_less <- pbinom(5, 10, 0.65)\nprob_X_less + prob_Y_less - prob_X_less*prob_Y_less\n\n\n[1] 0.9644174\n\n\n\n\n",
    "preview": "posts/2021-03-23-esercizi-sulla-distribuzione-binomiale/esercizi-sulla-distribuzione-binomiale_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-24T07:32:08+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-22-esercizio-sulla-correlazione/",
    "title": "Esercizio sulla correlazione",
    "description": "Usiamo R per controllare l'algebra.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-22",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\nSi consideri il seguente problema.\nSupponiamo che \\(X1\\) e \\(X2\\) siano due variabili aleatorie indipendenti con media 0 e varianza \\(\\sigma^2\\). Qual è la correlazione di \\(X1 + 2\\cdot X2\\) e \\(4 \\cdot X1 − X2\\)?\nSoluzione\nScriviamo \\(U = X1 + 2 X2\\) e \\(V = 4X1 - X2\\).\nCalcoliamo la covarianza:\n\\[\n\\begin{align}\ncov(X1 + 2 X2, V) &= cov(X1, V) + 2 cov(X2, V) \\notag\\\\\n&= cov(X1, 4X1-X2) + 2 cov(X2, 4X1-X2) \\notag\\\\\n&= 4 cov(X1, X1) - cov(X1, X2) + 2[4 cov(X1, X2) - cov(X2, X2)].\\notag\n\\end{align}\n\\] Ricordando che \\(X1\\) e \\(X2\\) sono indipendenti e che hanno entrambe la stessa varianza, otteniamo\n\\[\ncov(U, V) = 4 \\sigma^2 - 2 \\sigma^2 = 2 \\sigma^2.\n\\] Calcoliamo le due varianze:\n\\[\nvar(U) = var(X1 + 2 X2) = var(X1) + 4 var(X2) = 5 \\sigma^2,\n\\] \\[\nvar(V) = var(4X1 - X2) = 16 var(X1) + var(X2) = 17 \\sigma^2.\n\\] Calcoliamo la correlazione:\n\\[\nr = \\frac{cov(U, V)}{\\sqrt{var(U)var(V)}} = \\frac{2 \\sigma^2}{\\sqrt{5 \\sigma^2 \\cdot 17 \\sigma^2}} = 0.2169.\n\\]\nUsiamo R per controllare il risultato simulando un numero molto grande di valori per le due variabili:\n\n\nX_1 <- rnorm(1e7)\nX_2 <- rnorm(1e7)\nU <- X_1 + 2*X_2\nV <- 4*X_1 - X_2\n\n\n\nCalcoliamo la covarianza:\n\n\nmean(U*V)\n\n\n[1] 1.999482\n\nCalcoliamo la correlazione:\n\n\nmean(U*V) / sqrt(var(U)*var(V))\n\n\n[1] 0.2168761\n\n\n\n\n",
    "preview": "posts/2021-03-22-esercizio-sulla-correlazione/preview.png",
    "last_modified": "2021-03-22T14:00:59+01:00",
    "input_file": {},
    "preview_width": 1426,
    "preview_height": 624
  },
  {
    "path": "posts/2021-03-20-valore-atteso-e-varianza-di-variabili-aleatorie-continue/",
    "title": "Valore atteso e varianza di variabili aleatorie continue",
    "description": "Usiamo `R` per il calcolo degli integrali.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-21",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\n\nContents\nTL; DR\nLa funzione di densità di probabilità\nLa probabilità \\(P(a \\leq Y \\leq b)\\)\nIl valore atteso\nLa varianza\n\nTL; DR\nIn questo post esamineremo il calcolo di\nprobabilità,\nvalore atteso,\nvarianza\nnel caso di variabili aleatorie continue.\nLa cattiva notizia è che la soluzione di tali problemi richiede la conoscenza del calcolo integrale. Oppure, se vogliamo evitare di avventurarci in quella direzione, possiamo trovare facilmente la soluzione utilizzando un software, come ad esempio R. Vedremo qui come si fa.\nLa funzione di densità di probabilità\nConsideriamo la variabile aleatoria continua \\(Y\\) avente una distribuzione di densità \\(f_Y(y)\\). La probabilità che \\(Y\\) assuma valori compresi tra \\(a\\) e \\(b\\), con \\(a<b\\), è\n\\[\nP(a \\leq Y \\leq b) = \\int_a^b f_Y(y) dy.\n\\]\nPer definizione, \\(P(-\\infty \\leq Y \\leq +\\infty) = 1\\) e quindi \\(\\int_{-\\infty}^{+\\infty} f_Y(y) dy = 1\\).\nSupponiamo che\n\\[\nf_Y (y) = \\frac{3}{y^4}, \\quad y > 1\n\\] Verifichiamo che \\(f_Y(y)\\) sia una funzione di densità. Definiamo la funzione \\(f_Y(y)\\) in R nel modo seguente:\n\n\nf <- function(y) 3 / y^4\n\n\n\nMediante le seguenti istruzioni possiamo ottenere una rappresentazione grafica di \\(f_X(x)\\). Iniziamo a creare un data.frame fittizio\n\n\nlibrary(\"tidyverse\", warn.conflicts = FALSE)\np <- ggplot(data = data.frame(y = 0), mapping = aes(x = y))\n\n\n\ne utilizziamo la funzione stat_function() per creare il plot:\n\n\np + \n  stat_function(fun = f) + \n  xlim(1, 5) +\n  labs(\n    y = \"f(y)\"\n  )\n\n\n\n\nVerifichiamo che l’area sia unitaria:\n\n\nintegrate(\n  f, \n  lower = 1, \n  upper = Inf\n)$value\n\n\n[1] 1\n\nLa probabilità \\(P(a \\leq Y \\leq b)\\)\nCalcoliamo una probabilità, ovvero l’area sottesa alla funzione di densità in un intervallo. Per esempio, calcoliamo la probabilità \\(P(1.5 \\leq Y \\leq 2.5)\\).\nPer trovare l’area sottesa alla funzione di densità nell’intervallo [1.5, 2.5] utilizziamo la funzione integrate() come indicato di seguito:\n\n\nintegrate(\n  f, \n  lower = 1.5, \n  upper = 2.5\n)$value\n\n\n[1] 0.2322963\n\nIl valore atteso\nIl valore atteso di una variabile aleatoria continua \\(Y\\) avente una distribuzione di densità \\(f_Y(y)\\) è definito nel modo seguente:\n\\[\n\\mathbb{E}(Y) = \\mu_Y = \\int_a^b y f_Y(y) dy.\n\\] Anche in questo caso dobbiamo calcolare un integrale. Nel caso dell’esempio, iniziamo con il definire la funzione \\(g\\):\n\n\ng <- function(y) y * f(y)\n\n\n\nIl valore atteso è dato da:\n\n\nEY <- integrate(\n  g,\n  lower = 1,\n  upper = Inf\n)$value\nEY\n\n\n[1] 1.5\n\nLa varianza\nLa varianza di una variabile aleatoria continua \\(Y\\) avente una distribuzione di densità \\(f_Y(y)\\) è definita come\n\\[\nVar(Y) = \\sigma^2_Y = \\int_a^b (y - \\mu_Y)^2 f_Y(y) dy.\n\\]\nNel caso dell’esempio che stiamo discutendo, per calcolare la varianza di \\(Y\\) utilizzeremo la formula seguente:\n\\[\nVar(Y) = \\mathbb{E}(Y^2) - \\mathbb{E}(Y)^2.\n\\] Iniziamo a definire in R la funzione necessaria per calcolare il valore atteso di \\(y^2\\):\n\n\nh <- function(y) y^2 * f(y)\n\n\n\nCalcoliamo poi il valore atteso di \\(y^2\\) e, sottraendo da tale valore il quadrato del valore atteso, otteniamo il risultato desiderato:\n\n\nVarY <- integrate(\n  h,\n  lower = 1,\n  upper = Inf\n)$value - EY^2 \nVarY\n\n\n[1] 0.75\n\nInformazioni sulla sessione di lavoro\n\nSession Info\n\nSono qui fornite le informazioni sulla sessione di lavoro insieme all’elenco dei pacchetti usati. I pacchetti contrassegnati con un asterisco(*) sono stati usati esplicitamente nello script.\n\n─ Session info ─────────────────────────────────────────────────────\n setting  value                       \n version  R version 3.6.3 (2020-02-29)\n os       macOS Mojave 10.14.6        \n system   x86_64, darwin15.6.0        \n ui       X11                         \n language (EN)                        \n collate  it_IT.UTF-8                 \n ctype    it_IT.UTF-8                 \n tz       Europe/Rome                 \n date     2021-03-22                  \n\n─ Packages ─────────────────────────────────────────────────────────\n package     * version    date       lib\n assertthat    0.2.1      2019-03-21 [1]\n backports     1.2.1      2020-12-09 [1]\n broom         0.7.5      2021-02-19 [1]\n bslib         0.2.4      2021-01-25 [1]\n cellranger    1.1.0      2016-07-27 [1]\n cli           2.3.1      2021-02-23 [1]\n colorspace    2.0-0      2020-11-11 [1]\n crayon        1.4.1      2021-02-08 [1]\n DBI           1.1.1      2021-01-15 [1]\n dbplyr        2.1.0      2021-02-03 [1]\n debugme       1.1.0      2017-10-22 [1]\n digest        0.6.27     2020-10-24 [1]\n distill       1.2        2021-01-13 [1]\n downlit       0.2.1      2020-11-04 [1]\n dplyr       * 1.0.5      2021-03-05 [1]\n ellipsis      0.3.1      2020-05-15 [1]\n evaluate      0.14       2019-05-28 [1]\n fansi         0.4.2      2021-01-15 [1]\n farver        2.1.0      2021-02-28 [1]\n fastmap       1.1.0      2021-01-25 [1]\n forcats     * 0.5.1      2021-01-27 [1]\n fs            1.5.0      2020-07-31 [1]\n generics      0.1.0      2020-10-31 [1]\n ggplot2     * 3.3.3      2020-12-30 [1]\n glue          1.4.2      2020-08-27 [1]\n gtable        0.3.0      2019-03-25 [1]\n haven         2.3.1      2020-06-01 [1]\n highr         0.8        2019-03-20 [1]\n hms           1.0.0      2021-01-13 [1]\n htmltools     0.5.1.9000 2021-02-27 [1]\n httpuv        1.5.5      2021-01-13 [1]\n httr          1.4.2      2020-07-20 [1]\n jquerylib     0.1.3      2020-12-17 [1]\n jsonlite      1.7.2      2020-12-09 [1]\n knitr         1.31       2021-01-27 [1]\n labeling      0.4.2      2020-10-20 [1]\n later         1.1.0.1    2020-06-05 [1]\n lattice       0.20-41    2020-04-02 [1]\n lifecycle     1.0.0      2021-02-15 [1]\n lubridate     1.7.10     2021-02-26 [1]\n magrittr      2.0.1      2020-11-17 [1]\n mime          0.10       2021-02-13 [1]\n modelr        0.1.8      2020-05-19 [1]\n munsell       0.5.0      2018-06-12 [1]\n pillar        1.5.1      2021-03-05 [1]\n pkgconfig     2.0.3      2019-09-22 [1]\n promises      1.2.0.1    2021-02-11 [1]\n purrr       * 0.3.4      2020-04-17 [1]\n R6            2.5.0      2020-10-28 [1]\n ragg          1.1.2      2021-03-17 [1]\n Rcpp          1.0.6      2021-01-15 [1]\n readr       * 1.4.0      2020-10-05 [1]\n readxl        1.3.1      2019-03-13 [1]\n reprex        1.0.0      2021-01-27 [1]\n rlang         0.4.10     2020-12-30 [1]\n rmarkdown     2.7.3      2021-03-06 [1]\n rstudioapi    0.13       2020-11-12 [1]\n rvest         1.0.0      2021-03-09 [1]\n sass          0.3.1      2021-01-24 [1]\n scales        1.1.1      2020-05-11 [1]\n sessioninfo   1.1.1      2018-11-05 [1]\n shiny         1.6.0      2021-01-25 [1]\n showtext      0.9-2      2021-01-10 [1]\n showtextdb    3.0        2020-06-04 [1]\n stringi       1.5.3      2020-09-09 [1]\n stringr     * 1.4.0      2019-02-10 [1]\n sysfonts      0.8.3      2021-01-10 [1]\n systemfonts   1.0.1      2021-02-09 [1]\n textshaping   0.3.3      2021-03-16 [1]\n thematic      0.1.1      2021-01-16 [1]\n tibble      * 3.1.0      2021-02-25 [1]\n tidyr       * 1.1.3      2021-03-03 [1]\n tidyselect    1.1.0      2020-05-11 [1]\n tidyverse   * 1.3.0      2019-11-21 [1]\n utf8          1.2.1      2021-03-12 [1]\n vctrs         0.3.6      2020-12-17 [1]\n withr         2.4.1      2021-01-26 [1]\n xfun          0.22       2021-03-11 [1]\n xml2          1.3.2      2020-04-23 [1]\n xtable        1.8-4      2019-04-21 [1]\n yaml          2.2.1      2020-02-01 [1]\n source                            \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n Github (rstudio/htmltools@ac43afe)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n Github (rstudio/rmarkdown@61db7a9)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n\n[1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library\n\n\n\n\n",
    "preview": "posts/2021-03-20-valore-atteso-e-varianza-di-variabili-aleatorie-continue/preview.png",
    "last_modified": "2021-03-22T07:52:11+01:00",
    "input_file": {},
    "preview_width": 608,
    "preview_height": 730
  },
  {
    "path": "posts/2021-03-21-il-teorema-di-bayes-tanto-tempo-fa/",
    "title": "Il teorema di Bayes, tanto tempo fa",
    "description": "Teorema di Bayes e Covid-19.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-21",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\n\nContents\nDetezione degli anticorpi di contrasto al virus\nTeorema di Bayes\n\nPubblico in questo post dei materiali che sono utili per un esercizio sul teorema di Bayes. In realtà, ho scritto questo testo il 22 marzo 2020. Assume un significato diverso oggi.\nDetezione degli anticorpi di contrasto al virus\nRicavo queste informazioni da un articolo tratto da il Giornale del 12/03/2020 che riporto qui con piccole modifiche:\n\na Pechino si sviluppano strumenti diagnostici in grado di rilevare la positività al Covid-19 in tempi record. Stando a quanto riferisce l’agenzia stampa Agenzia Nova, un gruppo di scienziati cinesi avrebbe approntato un test rapido in grado di diagnosticare il coronavirus in soli 15 minuti isolando gli anticorpi di contrasto alla malattia da una goccia di sangue prelevata sulla punta delle dita. Una task force guidata dal pneumologo cinese Zhou Nanshan, agli onori delle cronache per le brillanti intuizioni durante l’epidemia Sars del 2003, ha affermato che il test degli anticorpi mira ad offrire un metodo accurato per identificare rapidamente i pazienti infetti e i portatori asintomatici del nuovo virus. Rispetto al tampone faringeo – test Rt-Pcr dell’acido nucleico – che impiega approssimativamente 3/4 ore per evidenziare la positività al Covid-19, il kit sperimentale riesce a rilevare il gli anticorpi di contrasto al virus entro 15 minuti e con una sensibilità dell’88.6 per cento e una specificità del 90.63 per cento.\n\nTeorema di Bayes\nUtilizziamo queste informazioni per un esercizio su teorema di Bayes. Useremo la seguente notazione:\n\\[\n\\mbox{P}(+ \\mid C=1)=0.8860, \\qquad \\mbox{P}(- \\mid C=0)=0.9063 \n\\]\ndove \\(+\\) significa un test positivo e \\(C\\) indica se il Covid-19 è presente (1) o assente (0).\nLa domanda è la stessa che abbiamo già incontrato in precedenza. Supponiamo di selezionare una persona a caso e di sottoporla al test. Dato che il test rapido dà un risultato positivo, qual è la probabilità che la persona in questione abbia effettivamente contratto il Covid-19? Quello che vogliamo conoscere è \\(\\mbox{P}(C=1 \\mid +)\\).\nPer rispondere a questa domanda, dobbiamo conoscere il tasso di prevalenza del Covid-19 nella popolazione. Ovviamente nessuno conosce questo numero. Inoltre, esso cambia continuamente. Facciamo due ipotesi. Oggi (22 marzo 2020) sappiamo che, in Italia, ci sono 42681 individui positivi. Questo è ovviamente un limite minimo che corrisponde ad una prevalenza all’incirca pari a\n\n\n42681 / 60000000\n\n\n[1] 0.00071135\n\novvero, \\(\\mbox{P}(C=1)=0.00071135\\) e \\(\\mbox{P}(C=0)= 1 - 0.00071135 = 0.9992887\\). Solo per gli scopi di questo esercizio, considereremo il caso secondo il quale la prevalenza è molto maggiore. Ma per ora concentriamoci su questi dati.\nLa risposta che cerchiamo è fornita dal teorema di Bayes:\n\\[\n\\mbox{P}(A \\mid B)  =  \\frac{\\mbox{P}(B \\mid A)\\mbox{P}(A)}{\\mbox{P}(B)} \n\\]\nApplicando il teorema di Bayes alla situazione del problema che stiamo considerando, otteniamo\n\\[\n\\begin{align*}\n\\mbox{P}(C=1 \\mid +) & =  \\frac{P(+ \\mid C=1) \\cdot P(C=1)} {\\mbox{P}(+)} \\\\\n& =  \\frac{\\mbox{P}(+ \\mid C=1)\\cdot P(C=1)} {\\mbox{P}(+ \\mid C=1) \\cdot P(C=1) + \\mbox{P}(+ \\mid C=0) \\mbox{P}(C=0)} \n\\end{align*}\n\\]\nInserendo i numeri del problema nella formula otteniamo:\n\n\nprevalence <- 42681 / 6e7\n\nsensibility <- 0.8860\nspecificity <- 0.9063\n\npr_corona_given_positive <- (sensibility * prevalence) / \n  (sensibility * prevalence + (1 - specificity) * (1 - prevalence))\npr_corona_given_positive\n\n\n[1] 0.006686102\n\nQuindi, se la prevalenza fosse così bassa il test ci darebbe un risultato quasi del tutto inutile.\nSupponiamo ora, per continuare l’esercizio, che la prevalenza sia 100 volte maggiore di quella indicata sopra e ripetiamo i calcoli:\n\n\nprevalence <- 100 * 42681 / 6e7\n\nsensitivity <- 0.8860\nspecificity <- 0.9063\n\npr_corona_given_positive <- (sensitivity * prevalence) / \n  (sensitivity * prevalence + (1 - specificity) * (1 - prevalence))\npr_corona_given_positive\n\n\n[1] 0.420002\n\nI risultati sono migliori ma, anche in questo caso, non tanto buoni. Se si usassero questi dati per determinare chi deve stare in quarantena, data la stima molto alta della prevalenza, allora in più del 50% di casi andrebbero in quarantena delle persone che, in realtà, non hanno il Covid-19.\nMa forse questo non è un prezzo troppo alto da pagare. Quello che vogliamo evitare è che il Covid-19 sia presente quando il risultato è negativo – in quel caso, ovviamente, non si arginerebbe la diffusione del contagio. Per valutare questo, iniziamo a calcolare il valore predittivo del test negativo, ovvero la probabilità che la malattia sia assente in un soggetto con un test negativo. A questo fine, consideriamo la stima più bassa della prevalenza.\n\n\nprevalence <- 42681 / 6e7\n\nsensitivity <- 0.8860\nspecificity <- 0.9063\n\npr_not_corona_given_negative <- (specificity) * (1 - prevalence) / \n  (specificity * (1 - prevalence) + (1 - sensitivity) * prevalence)\npr_not_corona_given_negative\n\n\n[1] 0.9999105\n\nQuesta è evidentemente una probabilità molto alta. Avendo calcolato la probabilità che il Covid-19 sia assente quando il test è negativo, è facile trovare la probabilità dell’evento complementare, ovvero la probabilità che il Covid-19 sia presente quando il test è negativo:\n\n\n1 - pr_not_corona_given_negative\n\n\n[1] 8.953367e-05\n\novvero, quasi zero. Con questo test, dunque, tale probabilità è estremamente bassa. Il che illustra la potenziale utilità di questo test rapido per il Covid-19, se la prevalenza è molto bassa.\nConsideriamo infine una prevalenza maggiore.\n\n\nprevalence <- 100 * 42681 / 6e7\n\nsensitivity <- 0.8860\nspecificity <- 0.9063\n\npr_not_corona_given_negative <- (specificity) * (1 - prevalence) / \n  (specificity * (1 - prevalence) + (1 - sensitivity) * prevalence)\npr_not_corona_given_negative\n\n\n[1] 0.9904589\n\n1 - pr_not_corona_given_negative\n\n\n[1] 0.009541135\n\nAnche in questo caso il test si dimostra utile: meno dell’1% dei casi sono tali per cui il Covid-19 è presente quando si ottiene un risultato negativo.\nInvece, se la prevalenza fosse altissima\n\n\nprevalence <- 1000 * 42681 / 6e7\nprevalence \n\n\n[1] 0.71135\n\nil test considerato si dimostrerebbe di scarsa utilità:\n\n\nsensitivity <- 0.8860\nspecificity <- 0.9063\n\npr_not_corona_given_negative <- (specificity) * (1 - prevalence) / \n  (specificity * (1 - prevalence) + (1 - sensitivity) * prevalence)\n\n1 - pr_not_corona_given_negative\n\n\n[1] 0.2366341\n\nInformazioni sulla sessione di lavoro\n\nSession Info\n\nSono qui fornite le informazioni sulla sessione di lavoro insieme all’elenco dei pacchetti usati. I pacchetti contrassegnati con un asterisco(*) sono stati usati esplicitamente nello script.\n\n─ Session info ─────────────────────────────────────────────────────\n setting  value                       \n version  R version 3.6.3 (2020-02-29)\n os       macOS Mojave 10.14.6        \n system   x86_64, darwin15.6.0        \n ui       X11                         \n language (EN)                        \n collate  it_IT.UTF-8                 \n ctype    it_IT.UTF-8                 \n tz       Europe/Rome                 \n date     2021-03-21                  \n\n─ Packages ─────────────────────────────────────────────────────────\n package     * version    date       lib\n assertthat    0.2.1      2019-03-21 [1]\n bslib         0.2.4      2021-01-25 [1]\n cli           2.3.1      2021-02-23 [1]\n colorspace    2.0-0      2020-11-11 [1]\n crayon        1.4.1      2021-02-08 [1]\n DBI           1.1.1      2021-01-15 [1]\n debugme       1.1.0      2017-10-22 [1]\n digest        0.6.27     2020-10-24 [1]\n distill       1.2        2021-01-13 [1]\n downlit       0.2.1      2020-11-04 [1]\n dplyr         1.0.5      2021-03-05 [1]\n ellipsis      0.3.1      2020-05-15 [1]\n evaluate      0.14       2019-05-28 [1]\n fansi         0.4.2      2021-01-15 [1]\n fastmap       1.1.0      2021-01-25 [1]\n generics      0.1.0      2020-10-31 [1]\n ggplot2       3.3.3      2020-12-30 [1]\n glue          1.4.2      2020-08-27 [1]\n gtable        0.3.0      2019-03-25 [1]\n htmltools     0.5.1.9000 2021-02-27 [1]\n httpuv        1.5.5      2021-01-13 [1]\n jquerylib     0.1.3      2020-12-17 [1]\n jsonlite      1.7.2      2020-12-09 [1]\n knitr         1.31       2021-01-27 [1]\n later         1.1.0.1    2020-06-05 [1]\n lattice       0.20-41    2020-04-02 [1]\n lifecycle     1.0.0      2021-02-15 [1]\n magrittr      2.0.1      2020-11-17 [1]\n mime          0.10       2021-02-13 [1]\n munsell       0.5.0      2018-06-12 [1]\n pillar        1.5.1      2021-03-05 [1]\n pkgconfig     2.0.3      2019-09-22 [1]\n promises      1.2.0.1    2021-02-11 [1]\n purrr         0.3.4      2020-04-17 [1]\n R6            2.5.0      2020-10-28 [1]\n ragg          1.1.2      2021-03-17 [1]\n Rcpp          1.0.6      2021-01-15 [1]\n rlang         0.4.10     2020-12-30 [1]\n rmarkdown     2.7.3      2021-03-06 [1]\n rstudioapi    0.13       2020-11-12 [1]\n sass          0.3.1      2021-01-24 [1]\n scales        1.1.1      2020-05-11 [1]\n sessioninfo   1.1.1      2018-11-05 [1]\n shiny         1.6.0      2021-01-25 [1]\n showtext      0.9-2      2021-01-10 [1]\n showtextdb    3.0        2020-06-04 [1]\n stringi       1.5.3      2020-09-09 [1]\n stringr       1.4.0      2019-02-10 [1]\n sysfonts      0.8.3      2021-01-10 [1]\n systemfonts   1.0.1      2021-02-09 [1]\n textshaping   0.3.3      2021-03-16 [1]\n thematic      0.1.1      2021-01-16 [1]\n tibble        3.1.0      2021-02-25 [1]\n tidyselect    1.1.0      2020-05-11 [1]\n utf8          1.2.1      2021-03-12 [1]\n vctrs         0.3.6      2020-12-17 [1]\n withr         2.4.1      2021-01-26 [1]\n xfun          0.22       2021-03-11 [1]\n xtable        1.8-4      2019-04-21 [1]\n yaml          2.2.1      2020-02-01 [1]\n source                            \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n Github (rstudio/htmltools@ac43afe)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n Github (rstudio/rmarkdown@61db7a9)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n\n[1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library\n\n\n\n\n",
    "preview": "posts/2021-03-21-il-teorema-di-bayes-tanto-tempo-fa/preview.png",
    "last_modified": "2021-03-21T18:27:18+01:00",
    "input_file": {},
    "preview_width": 632,
    "preview_height": 800
  },
  {
    "path": "posts/2021-03-21-un-semplice-esercizio-sul-teorema-di-bayes/",
    "title": "Un semplice esercizio sul teorema di Bayes",
    "description": "Aggiorniamo le nostre credenze usando Bayes.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-21",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\n\nContents\nLa distribuzione a priori\nLa verosimiglianza\nLa distribuzione a posteriori\nL’aggiornamento Bayesiano\n\n\nSupponiamo che uno psicologo sia impegnato in uno studio che richiede la trasposizione digitale di un archivio cartaceo di cartelle cliniche. Supponiamo inoltre che tale lavoro di trascrizione venga eseguito da tre tirocinanti: M., L. e C.\nLa distribuzione a priori\nI tre tirocinanti trascrivono un numero diverso di cartelle cliniche: M. trascrive il 60% delle cartelle, L. il 30% e C. il rimanente 10%. Siano \\(M\\), \\(L\\) e \\(C\\) gli eventi: la trascrizione di una cartella clinica è stata fatta, rispettivamente, da M., L. o C..\nLa verosimiglianza\nPer evitare errori di trascrizione, devono essere fatti dei controlli incrociati. Da tali controlli emerge che c’è una probabilità d’errore diversa per i tre tirocinanti. Supponiamo che, in passato, i tre tirocinanti abbiano già lavorato per lo psicologo e, sulla base di questa esperienza precedente, si può dire che la probabilità di un errore di trascrizione per i tre tirocinanti è la seguente:\nM.\nL.\nC.\n0.003\n0.007\n0.010\nLa distribuzione a posteriori\nSupponiamo che, nello studio considerato, è stato commesso un errore di trascrizione. Ci chiediamo: avendo osservato un errore, qual è la probabilità che a commettere l’errore sia stato M., L. o C.?\nPer rispondere a questa domanda usiamo il teorema di Bayes. Alla luce del fatto che è stato commesso un errore, la probabilità che il responsabile sia M. è:\n\\[\nP(M \\mid E) = \\frac{P(M \\cap E)}{P(E)} = \\frac{P(E \\mid M) P(M)}{P(E \\mid M) P(M) + P(E \\mid L) P(L) + P(E \\mid C) P(C)},\n\\] ovvero\n\\[\n\\begin{align}\nP(M \\mid E) &= \\frac{P(E \\mid M) P(M)}{P(E \\mid M) P(M) + P(E \\mid L) P(L) + P(E \\mid C) P(C)}\\notag\\\\\n&=\\frac{0.003 \\cdot 0.6}{0.003 \\cdot 0.6 + 0.007 \\cdot 0.3 + 0.010 \\cdot 0.1} = 0.367.\n\\end{align}\n\\] È possibile applicare la stessa formula per calcolare le rimanenti due probabilità, cioè \\(P(L \\mid E)\\) e \\(P(C \\mid E)\\). Possiamo però facilitare i calcoli usando R come indicato qui sotto.\nDefiniamo una distribuzione di probabilità a priori che descrive la nostra credenza su chi possa essere il responsabile di un errore, senza avere informazioni ulteriori. Se, ad esempio, sappiamo che M. completa il 99% del lavoro, e sappiamo che c’è un errore, allora, probabilmente, il responsabile sarà M.. Nel caso dell’esercizio, la distribuzione a priori per i tre tirocinanti è:\n\n\nprior <- c(0.6, 0.3, 0.1)\n\n\n\nMa sappiamo anche che la probabilità di un errore varia tra i diversi tirocinanti. La verosimiglianza di osservare un errore per ciascuno dei tirocinanti è:\n\n\nlike <- c(0.003, 0.007, 0.01)\n\n\n\nSi noti che la verosimiglianza, a differenza della distribuzione a priori, non è una distribuzione di probabilità.\nSupponiamo che sia stato commesso un errore di trascrizione. Usando il teorema di Bayes possiamo combinare le informazioni precedenti\n\n\npost <- prior * like\npost/sum(post)\n\n\n[1] 0.3673469 0.4285714 0.2040816\n\nper trovare le probabilità a posteriori \\(P(M \\mid E)\\) (ovvero, la probabilità che il responsabile sia M. dato che è stato osservato un errore), \\(P(L \\mid E)\\) e \\(P(C \\mid E)\\).\nA differenza della verosimiglianza, la distribuzione a posteriori è una distribuzione di probabilità. Infatti\n\n\nsum(post/sum(post))\n\n\n[1] 1\n\nL’aggiornamento Bayesiano\nIl teorema di Bayes è utile soprattutto perché ci consente di eseguire quell’operazione che va sotto il nome di aggiornamento Bayesiano, ovvero l’aggiornamento delle nostre credenze via via che nuove informazioni risultano disponibili. Supponiamo che le nuove informazioni siano le seguenti: sono stati commessi altri 7 errori di trascrizione. Supponiamo inoltre di credere che tutti gli errori siano stati commessi dallo stesso tirocinante. Avendo a disposizione tali nuove informazioni, come si trasformano le probabilità a posteriori?\nMediante l’aggiornamento Bayesiano, le nuove probabilità a posteriori diventano:\n\n\nnewprior <- post\npost <- newprior * like^7 \npost/sum(post)\n\n\n[1] 0.0003355044 0.1473949328 0.8522695627\n\nVediamo ora che il tirocinante che ha la probabilità maggiore di avere commesso un errore di trascrizione in tutti gli otto documenti non è più L. ma diventa C. (non ho spiegato perché ho elevato la verosimiglianza alla settima potenza, ma spero che si capisca).\nInformazioni sulla sessione di lavoro\n\nSession Info\n\nSono qui fornite le informazioni sulla sessione di lavoro insieme all’elenco dei pacchetti usati. I pacchetti contrassegnati con un asterisco(*) sono stati usati esplicitamente nello script.\n\n─ Session info ─────────────────────────────────────────────────────\n setting  value                       \n version  R version 3.6.3 (2020-02-29)\n os       macOS Mojave 10.14.6        \n system   x86_64, darwin15.6.0        \n ui       X11                         \n language (EN)                        \n collate  it_IT.UTF-8                 \n ctype    it_IT.UTF-8                 \n tz       Europe/Rome                 \n date     2021-03-21                  \n\n─ Packages ─────────────────────────────────────────────────────────\n package     * version    date       lib\n assertthat    0.2.1      2019-03-21 [1]\n bslib         0.2.4      2021-01-25 [1]\n cli           2.3.1      2021-02-23 [1]\n colorspace    2.0-0      2020-11-11 [1]\n crayon        1.4.1      2021-02-08 [1]\n DBI           1.1.1      2021-01-15 [1]\n debugme       1.1.0      2017-10-22 [1]\n digest        0.6.27     2020-10-24 [1]\n distill       1.2        2021-01-13 [1]\n downlit       0.2.1      2020-11-04 [1]\n dplyr         1.0.5      2021-03-05 [1]\n ellipsis      0.3.1      2020-05-15 [1]\n evaluate      0.14       2019-05-28 [1]\n fansi         0.4.2      2021-01-15 [1]\n fastmap       1.1.0      2021-01-25 [1]\n generics      0.1.0      2020-10-31 [1]\n ggplot2       3.3.3      2020-12-30 [1]\n glue          1.4.2      2020-08-27 [1]\n gtable        0.3.0      2019-03-25 [1]\n htmltools     0.5.1.9000 2021-02-27 [1]\n httpuv        1.5.5      2021-01-13 [1]\n jquerylib     0.1.3      2020-12-17 [1]\n jsonlite      1.7.2      2020-12-09 [1]\n knitr         1.31       2021-01-27 [1]\n later         1.1.0.1    2020-06-05 [1]\n lattice       0.20-41    2020-04-02 [1]\n lifecycle     1.0.0      2021-02-15 [1]\n magrittr      2.0.1      2020-11-17 [1]\n mime          0.10       2021-02-13 [1]\n munsell       0.5.0      2018-06-12 [1]\n pillar        1.5.1      2021-03-05 [1]\n pkgconfig     2.0.3      2019-09-22 [1]\n promises      1.2.0.1    2021-02-11 [1]\n purrr         0.3.4      2020-04-17 [1]\n R6            2.5.0      2020-10-28 [1]\n ragg          1.1.2      2021-03-17 [1]\n Rcpp          1.0.6      2021-01-15 [1]\n rlang         0.4.10     2020-12-30 [1]\n rmarkdown     2.7.3      2021-03-06 [1]\n rstudioapi    0.13       2020-11-12 [1]\n sass          0.3.1      2021-01-24 [1]\n scales        1.1.1      2020-05-11 [1]\n sessioninfo   1.1.1      2018-11-05 [1]\n shiny         1.6.0      2021-01-25 [1]\n showtext      0.9-2      2021-01-10 [1]\n showtextdb    3.0        2020-06-04 [1]\n stringi       1.5.3      2020-09-09 [1]\n stringr       1.4.0      2019-02-10 [1]\n sysfonts      0.8.3      2021-01-10 [1]\n systemfonts   1.0.1      2021-02-09 [1]\n textshaping   0.3.3      2021-03-16 [1]\n thematic      0.1.1      2021-01-16 [1]\n tibble        3.1.0      2021-02-25 [1]\n tidyselect    1.1.0      2020-05-11 [1]\n utf8          1.2.1      2021-03-12 [1]\n vctrs         0.3.6      2020-12-17 [1]\n withr         2.4.1      2021-01-26 [1]\n xfun          0.22       2021-03-11 [1]\n xtable        1.8-4      2019-04-21 [1]\n yaml          2.2.1      2020-02-01 [1]\n source                            \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n Github (rstudio/htmltools@ac43afe)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n Github (rstudio/rmarkdown@61db7a9)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n\n[1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library\n\n\n\n\n",
    "preview": "posts/2021-03-21-un-semplice-esercizio-sul-teorema-di-bayes/preview.png",
    "last_modified": "2021-03-21T18:48:26+01:00",
    "input_file": {},
    "preview_width": 1132,
    "preview_height": 486
  },
  {
    "path": "posts/2021-03-19-distribuzioni-di-probabilit-discrete-3/",
    "title": "Distribuzioni di probabilità discrete (3)",
    "description": "Usiamo `R` per calcolare la covarianza di due variabili aleatorie discrete.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-20",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\n\nContents\nTL; DR\nLa distribuzioni di probabilità congiunta\nValore atteso\nCovarianza\nCorrelazione\n\nTL; DR\nIn questo post ci porremo il problema di calcolare la covarianza e la correlazione di due variabili aleatorie discrete, utilizzando l’informazione fornita dalla distribuzione di probabilità congiunta. Vedremo come si possa usare R per semplificare i calcoli.\nLa distribuzioni di probabilità congiunta\nCarichiamo i pacchetti necessari:\n\n\nlibrary(\"prob\")\nlibrary(\"distrEx\")\n\n\n\nConsideriamo l’esperimento casuale che corrisponde al lancio di due dadi bilanciati. Creiamo lo spazio campionario di questo esperimento casuale e definiamo su di esso le seguenti variabili aleatorie:\nU: somma dei punti dei due dadi,\nV: 1 se i punti del lancio del primo dado sono minori di 4, 0 altrimenti.\nCome nei post precedenti, otteniamo un elenco di tutti gli eventi elementari dello spazio campionario usando le istruzioni seguenti:\n\n\nS <- rolldie(2, nsides = 6, makespace = TRUE)\nS <- addrv(S, U = X1+X2, V = ifelse(X1 < 4, 1, 0))\nS\n\n\n   X1 X2  U V      probs\n1   1  1  2 1 0.02777778\n2   2  1  3 1 0.02777778\n3   3  1  4 1 0.02777778\n4   4  1  5 0 0.02777778\n5   5  1  6 0 0.02777778\n6   6  1  7 0 0.02777778\n7   1  2  3 1 0.02777778\n8   2  2  4 1 0.02777778\n9   3  2  5 1 0.02777778\n10  4  2  6 0 0.02777778\n11  5  2  7 0 0.02777778\n12  6  2  8 0 0.02777778\n13  1  3  4 1 0.02777778\n14  2  3  5 1 0.02777778\n15  3  3  6 1 0.02777778\n16  4  3  7 0 0.02777778\n17  5  3  8 0 0.02777778\n18  6  3  9 0 0.02777778\n19  1  4  5 1 0.02777778\n20  2  4  6 1 0.02777778\n21  3  4  7 1 0.02777778\n22  4  4  8 0 0.02777778\n23  5  4  9 0 0.02777778\n24  6  4 10 0 0.02777778\n25  1  5  6 1 0.02777778\n26  2  5  7 1 0.02777778\n27  3  5  8 1 0.02777778\n28  4  5  9 0 0.02777778\n29  5  5 10 0 0.02777778\n30  6  5 11 0 0.02777778\n31  1  6  7 1 0.02777778\n32  2  6  8 1 0.02777778\n33  3  6  9 1 0.02777778\n34  4  6 10 0 0.02777778\n35  5  6 11 0 0.02777778\n36  6  6 12 0 0.02777778\n\nLe istruzioni precedenti specificano, in corrispondenza di ogni punto dello spazio campionario (ovvero, in ciascuna riga del data.frame che viene generato), il valore assunto dalle due variabili aleatorie che sono state definite.\nLa descrizione dello spazio campionario fornita da R è corretta: questo esperimento casuale produce infatti 6 \\(\\times\\) 6 esiti (eventi elementari) possibili. Ma si noti che alcune righe del data.frame si ripetono più volte. Possiamo dunque semplificare tale descrizione nel modo seguente:\n\n\nUV <- marginal(S, vars = c(\"U\", \"V\")) \nUV\n\n\n    U V      probs\n1   5 0 0.02777778\n2   6 0 0.05555556\n3   7 0 0.08333333\n4   8 0 0.08333333\n5   9 0 0.08333333\n6  10 0 0.08333333\n7  11 0 0.05555556\n8  12 0 0.02777778\n9   2 1 0.02777778\n10  3 1 0.05555556\n11  4 1 0.08333333\n12  5 1 0.08333333\n13  6 1 0.08333333\n14  7 1 0.08333333\n15  8 1 0.05555556\n16  9 1 0.02777778\n\nL’elenco dei valori che le due variabili aleatorie \\(U\\) e \\(V\\) possono assumere, insieme alla probabilità del loro verificarsi, costituisce la distribuzione di probabilità congiunta delle due variabili aleatorie.\nÈ anche possibile presentare la distribuzione di probabilità congiunta in forma tabulare. A tale fine possiamo utilizzare la funzione xtabs():\n\n\nxtabs(round(probs, 3) ~ U + V, data = UV)\n\n\n    V\nU        0     1\n  2  0.000 0.028\n  3  0.000 0.056\n  4  0.000 0.083\n  5  0.028 0.083\n  6  0.056 0.083\n  7  0.083 0.083\n  8  0.083 0.056\n  9  0.083 0.028\n  10 0.083 0.000\n  11 0.056 0.000\n  12 0.028 0.000\n\nRipetiamo ora il processo precedente chiedendo alla funzione marginal() di calcolare le due distribuzioni marginali univariate:\n\n\npu <- marginal(S, vars = \"U\")\npu\n\n\n    U      probs\n1   2 0.02777778\n2   3 0.05555556\n3   4 0.08333333\n4   5 0.11111111\n5   6 0.13888889\n6   7 0.16666667\n7   8 0.13888889\n8   9 0.11111111\n9  10 0.08333333\n10 11 0.05555556\n11 12 0.02777778\n\ne\n\n\npv <- marginal(S, vars = \"V\")\npv\n\n\n  V probs\n1 0   0.5\n2 1   0.5\n\nIn maniera equivalente, lo stesso risultato si trova applicando le funzioni rowSums() e colSums() all’oggetto creato da xtabs():\n\n\ntemp <- xtabs(probs ~ U + V, data = UV)\nrowSums(temp)\n\n\n         2          3          4          5          6          7 \n0.02777778 0.05555556 0.08333333 0.11111111 0.13888889 0.16666667 \n         8          9         10         11         12 \n0.13888889 0.11111111 0.08333333 0.05555556 0.02777778 \n\ne\n\n\ncolSums(temp)\n\n\n  0   1 \n0.5 0.5 \n\nValore atteso\nCalcoliamo ora il valore atteso delle due variabili aleatorie.\nUtilizziamo la funzione DiscreteDistribution(). Il primo argomento richiede un vettore che specifica il supporto della variabile casuale discreta. Il secondo argomento è un vettore che, per ciasun valore della variabile casuale discreta, specifica il corrispondente valore della funzione di massa di probabilità.\nMediante la funzione DiscreteDistribution() otteniamo il valore atteso di U:\n\n\nU <- DiscreteDistribution(supp = pu$U, prob = pu$probs)\nmu_u <- E(U)\nmu_u\n\n\n[1] 7\n\nInoltre, il valore atteso di V è:\n\n\nV <- DiscreteDistribution(supp = pv$V, prob = pv$probs)\nmu_v <- E(V)\nmu_v\n\n\n[1] 0.5\n\nCovarianza\nUtilizzando le informazioni precedenti possiamo calcolare la covarianza tra \\(U\\) e \\(V\\).\nLa formula della covarianza\n\\[\n\\sigma_{xy} = \\sum_i \\big(x_i - \\mathbb{E}(x)\\big)\\big(y_i - \\mathbb{E}(y)\\big) \\cdot p_i(x,y)\n\\] può essere implementata in R nel modo seguente:\n\n\ns_uv <- sum((UV$U - mu_u) * (UV$V - mu_v) * UV$probs)\ns_uv\n\n\n[1] -0.75\n\nLo stesso risultato si ottiene usando la formula alternativa per il calcolo della covarianza:\n\\[\n\\sigma_{xy} = \\mathbb{E}(xy) -\\mathbb{E}(x)\\mathbb{E}(y)\n\\] ovvero\n\n\nsum((UV$U * UV$V) * UV$probs) - mu_u * mu_v\n\n\n[1] -0.75\n\nCorrelazione\nConoscendo la covarianza è possibile calcolare la correlazione. La correlazione infatti è una covarianza standardizzata:\n\\[\n\\rho_{xy} = \\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}\n\\]\nPer standardizzare dobbiamo dividere per le due deviazioni standard. Iniziamo dunque a calcolare le varianze delle due variabili aleatorie \\(U\\) e \\(V\\):\n\n\nv_u <- sum((pu$U - mu_u)^2 * pu$probs)\nv_u\n\n\n[1] 5.833333\n\nv_v <- sum((pv$V - mu_v)^2 * pv$probs)\nv_v\n\n\n[1] 0.25\n\noppure, in maniera equivalente\n\n\nvar(U)\n\n\n[1] 5.833333\n\nvar(V)\n\n\n[1] 0.25\n\nAbbiamo ora tutte le informazioni necessarie per il calcolo della correlazione:\n\n\ns_uv / sqrt(var(U) * var(V))\n\n\n[1] -0.621059\n\nInformazioni sulla sessione di lavoro\n\nSession Info\n\nSono qui fornite le informazioni sulla sessione di lavoro insieme all’elenco dei pacchetti usati. I pacchetti contrassegnati con un asterisco(*) sono stati usati esplicitamente nello script.\n\n─ Session info ─────────────────────────────────────────────────────\n setting  value                       \n version  R version 3.6.3 (2020-02-29)\n os       macOS Mojave 10.14.6        \n system   x86_64, darwin15.6.0        \n ui       X11                         \n language (EN)                        \n collate  it_IT.UTF-8                 \n ctype    it_IT.UTF-8                 \n tz       Europe/Rome                 \n date     2021-03-21                  \n\n─ Packages ─────────────────────────────────────────────────────────\n package       * version    date       lib\n assertthat      0.2.1      2019-03-21 [1]\n bslib           0.2.4      2021-01-25 [1]\n cli             2.3.1      2021-02-23 [1]\n combinat      * 0.0-8      2012-10-29 [1]\n digest          0.6.27     2020-10-24 [1]\n distill         1.2        2021-01-13 [1]\n distr         * 2.8.0      2019-03-11 [1]\n distrEx       * 2.8.0      2019-03-29 [1]\n downlit         0.2.1      2020-11-04 [1]\n evaluate        0.14       2019-05-28 [1]\n fansi           0.4.2      2021-01-15 [1]\n fAsianOptions * 3042.82    2017-11-17 [1]\n fBasics       * 3042.89.1  2020-03-07 [1]\n fOptions      * 3042.86    2017-11-16 [1]\n glue            1.4.2      2020-08-27 [1]\n htmltools       0.5.1.9000 2021-02-27 [1]\n jquerylib       0.1.3      2020-12-17 [1]\n jsonlite        1.7.2      2020-12-09 [1]\n knitr           1.31       2021-01-27 [1]\n magrittr        2.0.1      2020-11-17 [1]\n MASS            7.3-53.1   2021-02-12 [1]\n prob          * 1.0-1      2018-08-27 [1]\n R6              2.5.0      2020-10-28 [1]\n rlang           0.4.10     2020-12-30 [1]\n rmarkdown       2.7.3      2021-03-06 [1]\n sass            0.3.1      2021-01-24 [1]\n sessioninfo     1.1.1      2018-11-05 [1]\n sfsmisc       * 1.1-8      2021-01-07 [1]\n spatial         7.3-13     2021-01-24 [1]\n startupmsg    * 0.9.6      2019-03-11 [1]\n stringi         1.5.3      2020-09-09 [1]\n stringr         1.4.0      2019-02-10 [1]\n timeDate      * 3043.102   2018-02-21 [1]\n timeSeries    * 3062.100   2020-01-24 [1]\n vctrs           0.3.6      2020-12-17 [1]\n withr           2.4.1      2021-01-26 [1]\n xfun            0.22       2021-03-11 [1]\n yaml            2.2.1      2020-02-01 [1]\n source                            \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n Github (rstudio/htmltools@ac43afe)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n Github (rstudio/rmarkdown@61db7a9)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n\n[1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library\n\n\n\n\n",
    "preview": "posts/2021-03-19-distribuzioni-di-probabilit-discrete-3/preview.png",
    "last_modified": "2021-03-21T15:20:44+01:00",
    "input_file": {},
    "preview_width": 660,
    "preview_height": 1004
  },
  {
    "path": "posts/2021-03-19-distribuzioni-di-probabilit-discrete-2/",
    "title": "Distribuzioni di probabilità discrete (2)",
    "description": "Usiamo `R` per calcolare il valore atteso e la varianza di variabili aleatorie discrete.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-19",
    "categories": [
      "Psicometria",
      "R"
    ],
    "contents": "\nTL; DR\nIn questo post esamineremo nuovamente la nozione di indipendenza nel caso di due variabili aleatorie definite su uno spazio campionario discreto. Di tali variabili aleatorie discrete calcoleremo poi il valore atteso e la varianza.\nDistribuzioni di probabilità marginali\nConsideriamo l’esperimento casuale corrispondente al lancio di due dadi bilanciati.\nCarichiamo il pacchetto prob\n\n\nlibrary(\"prob\")\n\n\n\ne creiamo lo spazio campionario di questo esperimento casuale:\n\n\nS <- rolldie(2, nsides = 6, makespace = TRUE)\nS\n\n\n   X1 X2      probs\n1   1  1 0.02777778\n2   2  1 0.02777778\n3   3  1 0.02777778\n4   4  1 0.02777778\n5   5  1 0.02777778\n6   6  1 0.02777778\n7   1  2 0.02777778\n8   2  2 0.02777778\n9   3  2 0.02777778\n10  4  2 0.02777778\n11  5  2 0.02777778\n12  6  2 0.02777778\n13  1  3 0.02777778\n14  2  3 0.02777778\n15  3  3 0.02777778\n16  4  3 0.02777778\n17  5  3 0.02777778\n18  6  3 0.02777778\n19  1  4 0.02777778\n20  2  4 0.02777778\n21  3  4 0.02777778\n22  4  4 0.02777778\n23  5  4 0.02777778\n24  6  4 0.02777778\n25  1  5 0.02777778\n26  2  5 0.02777778\n27  3  5 0.02777778\n28  4  5 0.02777778\n29  5  5 0.02777778\n30  6  5 0.02777778\n31  1  6 0.02777778\n32  2  6 0.02777778\n33  3  6 0.02777778\n34  4  6 0.02777778\n35  5  6 0.02777778\n36  6  6 0.02777778\n\nDefiniamo le seguenti variabili aleatorie:\nU: somma dei punti dei due dadi,\nV: 1 se i punti del lancio del primo dado sono minori di 4, 0 altrimenti.\n\n\nS <- addrv(S, U = X1+X2, V = ifelse(X1 < 4, 1, 0))\nhead(S)\n\n\n  X1 X2 U V      probs\n1  1  1 2 1 0.02777778\n2  2  1 3 1 0.02777778\n3  3  1 4 1 0.02777778\n4  4  1 5 0 0.02777778\n5  5  1 6 0 0.02777778\n6  6  1 7 0 0.02777778\n\nCalcoliamo la distribuzione marginale di \\(U\\):\n\n\npu <- marginal(S, vars = \"U\")\npu\n\n\n    U      probs\n1   2 0.02777778\n2   3 0.05555556\n3   4 0.08333333\n4   5 0.11111111\n5   6 0.13888889\n6   7 0.16666667\n7   8 0.13888889\n8   9 0.11111111\n9  10 0.08333333\n10 11 0.05555556\n11 12 0.02777778\n\nVerifichiamo:\n\n\nsum(pu$U * pu$probs)\n\n\n[1] 7\n\nLo stesso per \\(V\\):\n\n\npv <- marginal(S, vars = \"V\")\npv\n\n\n  V probs\n1 0   0.5\n2 1   0.5\n\nDistribuzione di probabilità congiunta\nCreiamo ora la distribuzione di probabilità congiunta delle variabili \\(U\\) e \\(V\\):\n\n\npj <- marginal(S, vars = c(\"U\", \"V\"))\npj\n\n\n    U V      probs\n1   5 0 0.02777778\n2   6 0 0.05555556\n3   7 0 0.08333333\n4   8 0 0.08333333\n5   9 0 0.08333333\n6  10 0 0.08333333\n7  11 0 0.05555556\n8  12 0 0.02777778\n9   2 1 0.02777778\n10  3 1 0.05555556\n11  4 1 0.08333333\n12  5 1 0.08333333\n13  6 1 0.08333333\n14  7 1 0.08333333\n15  8 1 0.05555556\n16  9 1 0.02777778\n\nIndipendenza\nChiediamoci: \\(U\\) e \\(V\\) sono indipendenti?\n\n\npu[pu$U == 3, ]$probs * pv[pv$V == 1, ]$probs\n\n\n[1] 0.02777778\n\npj[pj$U == 3 & pj$V == 1, ]$probs\n\n\n[1] 0.05555556\n\nQuindi, la risposta è negativa.\nValore atteso e varianza\nConsideriamo la variabile \\(U\\). Calcoliamo il valore atteso e la varianza. Per fare questo, utilizzeremo le funzioni del pacchetto distrEx.\n\n\nlibrary(\"distrEx\")\n\n\n\nIniziamo con il valore atteso:\n\n\nX <- DiscreteDistribution(supp = pu$U, prob = pu$probs)\nmu <- E(X)\nmu\n\n\n[1] 7\n\nVerifichiamo:\n\n\nsum(pu$U * pu$probs)\n\n\n[1] 7\n\nCalcoliamo ora la varianza:\n\n\nvar(X)\n\n\n[1] 5.833333\n\nVerifichiamo:\n\n\nsum((pu$U - mu)^2 * pu$probs)\n\n\n[1] 5.833333\n\nInfine, la deviazione standard:\n\n\nsd(X)\n\n\n[1] 2.415229\n\nConclusioni\nLe funzioni fornite dai pacchetti prob e distrEx ci consentono di trovare la soluzione dei più comuni problemi “scolastici” relativi alle variabili aletorie discrete. Una descrizione delle funzionalità di questi pacchetti è fornita, per esempio, in questo documento.\nInformazioni sulla sessione di lavoro\n\nSession Info\n\nSono qui fornite le informazioni sulla sessione di lavoro insieme all’elenco dei pacchetti usati. I pacchetti contrassegnati con un asterisco(*) sono stati usati esplicitamente nello script.\n\n─ Session info ─────────────────────────────────────────────────────\n setting  value                       \n version  R version 3.6.3 (2020-02-29)\n os       macOS Mojave 10.14.6        \n system   x86_64, darwin15.6.0        \n ui       X11                         \n language (EN)                        \n collate  it_IT.UTF-8                 \n ctype    it_IT.UTF-8                 \n tz       Europe/Rome                 \n date     2021-03-21                  \n\n─ Packages ─────────────────────────────────────────────────────────\n package       * version    date       lib\n assertthat      0.2.1      2019-03-21 [1]\n bslib           0.2.4      2021-01-25 [1]\n cli             2.3.1      2021-02-23 [1]\n colorspace      2.0-0      2020-11-11 [1]\n combinat      * 0.0-8      2012-10-29 [1]\n crayon          1.4.1      2021-02-08 [1]\n DBI             1.1.1      2021-01-15 [1]\n debugme         1.1.0      2017-10-22 [1]\n digest          0.6.27     2020-10-24 [1]\n distill         1.2        2021-01-13 [1]\n distr         * 2.8.0      2019-03-11 [1]\n distrEx       * 2.8.0      2019-03-29 [1]\n downlit         0.2.1      2020-11-04 [1]\n dplyr           1.0.5      2021-03-05 [1]\n ellipsis        0.3.1      2020-05-15 [1]\n evaluate        0.14       2019-05-28 [1]\n fansi           0.4.2      2021-01-15 [1]\n fAsianOptions * 3042.82    2017-11-17 [1]\n fastmap         1.1.0      2021-01-25 [1]\n fBasics       * 3042.89.1  2020-03-07 [1]\n fOptions      * 3042.86    2017-11-16 [1]\n generics        0.1.0      2020-10-31 [1]\n ggplot2         3.3.3      2020-12-30 [1]\n glue            1.4.2      2020-08-27 [1]\n gtable          0.3.0      2019-03-25 [1]\n htmltools       0.5.1.9000 2021-02-27 [1]\n httpuv          1.5.5      2021-01-13 [1]\n jquerylib       0.1.3      2020-12-17 [1]\n jsonlite        1.7.2      2020-12-09 [1]\n knitr           1.31       2021-01-27 [1]\n later           1.1.0.1    2020-06-05 [1]\n lattice         0.20-41    2020-04-02 [1]\n lifecycle       1.0.0      2021-02-15 [1]\n magrittr        2.0.1      2020-11-17 [1]\n MASS            7.3-53.1   2021-02-12 [1]\n mime            0.10       2021-02-13 [1]\n munsell         0.5.0      2018-06-12 [1]\n pillar          1.5.1      2021-03-05 [1]\n pkgconfig       2.0.3      2019-09-22 [1]\n prob          * 1.0-1      2018-08-27 [1]\n promises        1.2.0.1    2021-02-11 [1]\n purrr           0.3.4      2020-04-17 [1]\n R6              2.5.0      2020-10-28 [1]\n ragg            1.1.2      2021-03-17 [1]\n Rcpp            1.0.6      2021-01-15 [1]\n rlang           0.4.10     2020-12-30 [1]\n rmarkdown       2.7.3      2021-03-06 [1]\n rstudioapi      0.13       2020-11-12 [1]\n sass            0.3.1      2021-01-24 [1]\n scales          1.1.1      2020-05-11 [1]\n sessioninfo     1.1.1      2018-11-05 [1]\n sfsmisc       * 1.1-8      2021-01-07 [1]\n shiny           1.6.0      2021-01-25 [1]\n showtext        0.9-2      2021-01-10 [1]\n showtextdb      3.0        2020-06-04 [1]\n spatial         7.3-13     2021-01-24 [1]\n startupmsg    * 0.9.6      2019-03-11 [1]\n stringi         1.5.3      2020-09-09 [1]\n stringr         1.4.0      2019-02-10 [1]\n sysfonts        0.8.3      2021-01-10 [1]\n systemfonts     1.0.1      2021-02-09 [1]\n textshaping     0.3.3      2021-03-16 [1]\n thematic        0.1.1      2021-01-16 [1]\n tibble          3.1.0      2021-02-25 [1]\n tidyselect      1.1.0      2020-05-11 [1]\n timeDate      * 3043.102   2018-02-21 [1]\n timeSeries    * 3062.100   2020-01-24 [1]\n utf8            1.2.1      2021-03-12 [1]\n vctrs           0.3.6      2020-12-17 [1]\n withr           2.4.1      2021-01-26 [1]\n xfun            0.22       2021-03-11 [1]\n xtable          1.8-4      2019-04-21 [1]\n yaml            2.2.1      2020-02-01 [1]\n source                            \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n Github (rstudio/htmltools@ac43afe)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n Github (rstudio/rmarkdown@61db7a9)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n\n[1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library\n\n\n\n\n",
    "preview": "posts/2021-03-19-distribuzioni-di-probabilit-discrete-2/preview.png",
    "last_modified": "2021-03-21T18:28:53+01:00",
    "input_file": {},
    "preview_width": 682,
    "preview_height": 428
  },
  {
    "path": "posts/2021-03-19-usare-r-per-risolvere-gli-esercizi-sulle-distribuzioni-di-probabilit-discrete/",
    "title": "Distribuzioni di probabilità discrete (1)",
    "description": "Le funzioni contenute del pacchetto `prob` possono facilitare la soluzione di esercizi sulle distribuzioni di probabilità discrete.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-19",
    "categories": [
      "Psicometria",
      "R"
    ],
    "contents": "\n\nContents\nTL; DR\nPacchetto prob\nEventi\n\nProbabilità condizionata\nIndipendenza\n\nTL; DR\nUtilizzeremo qui le funzioni fornite dal pacchetto prob per affrontare alcuni comuni problemi di calcolo delle probabilità nel caso in cui lo spazio campionario è costitutito da un insieme finito di eventi elementari aventi tutti la stessa probabilità di verificarsi.\n\n\nlibrary(\"prob\")\n\n\n\nPacchetto prob\nConsideriamo l’esperimento casuale del lancio di una moneta. I risultati sono H (head) e T (tail). Possiamo esplicitare lo spazio campionario di questo esperimento casuale con la funzione tosscoin():\n\n\nlibrary(\"prob\")\ntosscoin(1)\n\n\n  toss1\n1     H\n2     T\n\nIn maniera equivalente possiamo generare lo spazio campionario dell’esperimento casuale del lancio di tre monete:\n\n\ntosscoin(3)\n\n\n  toss1 toss2 toss3\n1     H     H     H\n2     T     H     H\n3     H     T     H\n4     T     T     H\n5     H     H     T\n6     T     H     T\n7     H     T     T\n8     T     T     T\n\nIl pacchetto prob fornisce anche funzioni per generare lo spazio campionario dell’esperimento casuale che consiste nel lancio di un dado, o dell’estrazione di una carta da un mazzo di carte da poker ben mescolato.\nPer l’esperimento casuale consistente nel lancio di dado bilanciato si usa la funzione rolldie():\n\n\nrolldie(1, makespace = TRUE)\n\n\n  X1     probs\n1  1 0.1666667\n2  2 0.1666667\n3  3 0.1666667\n4  4 0.1666667\n5  5 0.1666667\n6  6 0.1666667\n\nIl valore di default della funzione rolldie() specifica un dado a 6 facce, ma possiamo specificarne altri valori con l’argomento nsides. Ad esempio, l’istruzione rolldie(3, nsides = 4) viene usata per far specificare il caso di un dado a 4 facce lanciato tre volte.\nLa funzione rolldie() crea un dataframe in cui ciascuna riga corrisponde ad un punto dello spazio campionario (cioè un evento elementare). La colonna probs riporta la probabilità associata a ciascun punto dello spazio campionario.\nCon le istruzioni seguenti generiamo lo spazio campionario associato all’esperimento casuale che consiste nel lancio di due dadi bilanciati:\n\n\nS <- rolldie(2, makespace = TRUE)\nS\n\n\n   X1 X2      probs\n1   1  1 0.02777778\n2   2  1 0.02777778\n3   3  1 0.02777778\n4   4  1 0.02777778\n5   5  1 0.02777778\n6   6  1 0.02777778\n7   1  2 0.02777778\n8   2  2 0.02777778\n9   3  2 0.02777778\n10  4  2 0.02777778\n11  5  2 0.02777778\n12  6  2 0.02777778\n13  1  3 0.02777778\n14  2  3 0.02777778\n15  3  3 0.02777778\n16  4  3 0.02777778\n17  5  3 0.02777778\n18  6  3 0.02777778\n19  1  4 0.02777778\n20  2  4 0.02777778\n21  3  4 0.02777778\n22  4  4 0.02777778\n23  5  4 0.02777778\n24  6  4 0.02777778\n25  1  5 0.02777778\n26  2  5 0.02777778\n27  3  5 0.02777778\n28  4  5 0.02777778\n29  5  5 0.02777778\n30  6  5 0.02777778\n31  1  6 0.02777778\n32  2  6 0.02777778\n33  3  6 0.02777778\n34  4  6 0.02777778\n35  5  6 0.02777778\n36  6  6 0.02777778\n\nIn questo caso, il dataframe \\(S\\) è costituito da \\(6 \\times 6\\) righe:\n\n\nnrow(S)\n\n\n[1] 36\n\nEventi\nUn evento \\(A\\) è semplicemente un insieme di risultati dell’esperimento casuale o, in altre parole, un sottoinsieme dello spazio campionario. Dopo che l’esperimento casuale è stato eseguito, diciamo che l’evento \\(A\\) si è verificato se il risultato dell’esperimento appartiene ad \\(A\\). Diciamo che un gruppo di eventi \\(A_1\\), \\(A_2\\), \\(A_3\\),… sono mutuamente esclusivi o disgiunti se \\(Ai ∩ Aj = ∅\\) per ogni coppia distinta \\(A_i \\neq A_j\\). Ad esempio, nell’esperimento del lancio della moneta gli eventi \\(A\\) = {Heads} e \\(B\\) = {Tails} sono mutuamente esclusivi.\nCome possiamo definire un evento usando R?\nRitorniamo allo spazio campionario \\(S\\) dell’esperimento casuale corrispondente al lancio di due dadi bilanciati. Definiamo ora gli eventi \\(A\\) e \\(B\\) in \\(S\\) tali per cui\n\\(A\\): i due lanci producono lo stesso risultato,\n\\(B\\): la somma dei punti dei due lanci è maggiore o uguale a 8.\nPer identificare tali eventi usando R è sufficiente estrarre dal dataframe che descrive S le righe (eventi elementari) che soddisfano le condizioni specificate dalla definizione degli eventi medesimi:\n\n\nA <- subset(S, X1 == X2)\nB <- subset(S, X1 + X2 >= 8)\n\n\n\n\n\nA\n\n\n   X1 X2      probs\n1   1  1 0.02777778\n8   2  2 0.02777778\n15  3  3 0.02777778\n22  4  4 0.02777778\n29  5  5 0.02777778\n36  6  6 0.02777778\n\nSiamo ora nelle condizioni di potere calcolare la probabilità del verificarsi dell’evento \\(A\\), ovvero\n\n\n6 / 36\n\n\n[1] 0.1666667\n\nPer replicare tale risultato in R dobbiamo sommare i valori della colonna probs. Ciò viene ottenuto passando alla funzione Prob() l’oggetto che corrisponde al sottoinsieme di \\(S\\) che abbiamo definito:\n\n\nProb(A)\n\n\n[1] 0.1666667\n\nOvviamente, nel caso dell’esempio che stiamo discutendo ciò è poco utile, in quanto era facile trovare il risultato senza ricorrere ad R. Ma, in generale, il problema può essere complesso e, in tali condizioni, contare è difficile. In quei casi ricorrere ad R facilita la soluzione del problema.\nFacciamo la stessa cosa per \\(B\\):\n\n\nnrow(B)\n\n\n[1] 15\n\nDunque, la probabilità di \\(B\\) è\n\n\n15 / 36\n\n\n[1] 0.4166667\n\novvero\n\n\nProb(B)\n\n\n[1] 0.4166667\n\nUna descrizione approfondita dell’uso delle funzioni presenti nel pacchetto prob per risolvere questo tipo di problemi è fornita nel testo di G. Jay Kerns. Qui di seguito forniamo solo alcuni esempi per fare capire come ci si può usare R per risolvere problemi di questo tipo.\nProbabilità condizionata\nLa probabilità condizionata di \\(A\\) dato \\(B\\) corrisponde alla somma delle probabilità degli eventi elementari in \\(A\\), se consideriamo solo il sottoinsieme \\(B\\). Esaminiamo dunque \\(B\\):\n\n\nB\n\n\n   X1 X2      probs\n12  6  2 0.02777778\n17  5  3 0.02777778\n18  6  3 0.02777778\n22  4  4 0.02777778\n23  5  4 0.02777778\n24  6  4 0.02777778\n27  3  5 0.02777778\n28  4  5 0.02777778\n29  5  5 0.02777778\n30  6  5 0.02777778\n32  2  6 0.02777778\n33  3  6 0.02777778\n34  4  6 0.02777778\n35  5  6 0.02777778\n36  6  6 0.02777778\n\nCi chiediamo quali righe nel dataframe B sono tali per cui X1 == X2:\n\n\nB[B$X1 == B$X2, ]\n\n\n   X1 X2      probs\n22  4  4 0.02777778\n29  5  5 0.02777778\n36  6  6 0.02777778\n\nLa soluzione è dunque data dalla somma dei valori probs nel dataframe riportato sopra, la quale va divisa per la somma dei valori probs contenuti nel dataframe B:\n\n\nsum(B[B$X1 == B$X2, ]$probs) / sum(B$probs)\n\n\n[1] 0.2\n\nTale risultato può essere ottenuto usando la seguente istruzione:\n\n\nProb(A, given = B)\n\n\n[1] 0.2\n\nÈ dunque facile trovare, seguendo la stessa procedura, la probabilità \\(P(B \\mid A)\\):\n\n\nProb(B, given = A)\n\n\n[1] 0.5\n\nIndipendenza\nChiediamoci ora se \\(A\\) e \\(B\\) sono indipendenti.\nPer affrontare questo problema dobbiamo calcolare la probabilità \\(P(A \\cap B)\\). Per fare questo dobbiamo selezionare le righe di \\(S\\) nelle quali la colonna X1 è uguale alla colonna X2 e poi sommare i valori di probabilità contenuti nella colonna probs. Usiamo la funzione intersect():\n\n\nintersect(A, B)\n\n\n   X1 X2      probs\n22  4  4 0.02777778\n29  5  5 0.02777778\n36  6  6 0.02777778\n\nLa probabilità dell’intersezione \\(A \\cap B\\) è dunque uguale a\n\n\n3 / 36\n\n\n[1] 0.08333333\n\novvero:\n\n\nProb(intersect(A, B))\n\n\n[1] 0.08333333\n\nSiamo ora nelle condizioni di decidere se \\(A\\) e \\(B\\) sono o meno indipendenti. Sappiamo che due eventi sono indipendenti se \\(P(A \\cap B) = P(A) P(B)\\). Verifichiamo:\n\n\nProb(A) * Prob(B)\n\n\n[1] 0.06944444\n\nil che non corrisponde al valore 0.0833333. Possiamo dunque concludere che gli eventi \\(A\\) e \\(B\\) non sono indipendenti.\nInformazioni sulla sessione di lavoro\n\nSession Info\n\nSono qui fornite le informazioni sulla sessione di lavoro insieme all’elenco dei pacchetti usati. I pacchetti contrassegnati con un asterisco(*) sono stati usati esplicitamente nello script.\n\n─ Session info ─────────────────────────────────────────────────────\n setting  value                       \n version  R version 3.6.3 (2020-02-29)\n os       macOS Mojave 10.14.6        \n system   x86_64, darwin15.6.0        \n ui       X11                         \n language (EN)                        \n collate  it_IT.UTF-8                 \n ctype    it_IT.UTF-8                 \n tz       Europe/Rome                 \n date     2021-03-21                  \n\n─ Packages ─────────────────────────────────────────────────────────\n package       * version    date       lib\n assertthat      0.2.1      2019-03-21 [1]\n bslib           0.2.4      2021-01-25 [1]\n cli             2.3.1      2021-02-23 [1]\n colorspace      2.0-0      2020-11-11 [1]\n combinat      * 0.0-8      2012-10-29 [1]\n crayon          1.4.1      2021-02-08 [1]\n DBI             1.1.1      2021-01-15 [1]\n debugme         1.1.0      2017-10-22 [1]\n digest          0.6.27     2020-10-24 [1]\n distill         1.2        2021-01-13 [1]\n downlit         0.2.1      2020-11-04 [1]\n dplyr           1.0.5      2021-03-05 [1]\n ellipsis        0.3.1      2020-05-15 [1]\n evaluate        0.14       2019-05-28 [1]\n fansi           0.4.2      2021-01-15 [1]\n fAsianOptions * 3042.82    2017-11-17 [1]\n fastmap         1.1.0      2021-01-25 [1]\n fBasics       * 3042.89.1  2020-03-07 [1]\n fOptions      * 3042.86    2017-11-16 [1]\n generics        0.1.0      2020-10-31 [1]\n ggplot2         3.3.3      2020-12-30 [1]\n glue            1.4.2      2020-08-27 [1]\n gtable          0.3.0      2019-03-25 [1]\n htmltools       0.5.1.9000 2021-02-27 [1]\n httpuv          1.5.5      2021-01-13 [1]\n jquerylib       0.1.3      2020-12-17 [1]\n jsonlite        1.7.2      2020-12-09 [1]\n knitr           1.31       2021-01-27 [1]\n later           1.1.0.1    2020-06-05 [1]\n lattice         0.20-41    2020-04-02 [1]\n lifecycle       1.0.0      2021-02-15 [1]\n magrittr        2.0.1      2020-11-17 [1]\n mime            0.10       2021-02-13 [1]\n munsell         0.5.0      2018-06-12 [1]\n pillar          1.5.1      2021-03-05 [1]\n pkgconfig       2.0.3      2019-09-22 [1]\n prob          * 1.0-1      2018-08-27 [1]\n promises        1.2.0.1    2021-02-11 [1]\n purrr           0.3.4      2020-04-17 [1]\n R6              2.5.0      2020-10-28 [1]\n ragg            1.1.2      2021-03-17 [1]\n Rcpp            1.0.6      2021-01-15 [1]\n rlang           0.4.10     2020-12-30 [1]\n rmarkdown       2.7.3      2021-03-06 [1]\n rstudioapi      0.13       2020-11-12 [1]\n sass            0.3.1      2021-01-24 [1]\n scales          1.1.1      2020-05-11 [1]\n sessioninfo     1.1.1      2018-11-05 [1]\n shiny           1.6.0      2021-01-25 [1]\n showtext        0.9-2      2021-01-10 [1]\n showtextdb      3.0        2020-06-04 [1]\n spatial         7.3-13     2021-01-24 [1]\n stringi         1.5.3      2020-09-09 [1]\n stringr         1.4.0      2019-02-10 [1]\n sysfonts        0.8.3      2021-01-10 [1]\n systemfonts     1.0.1      2021-02-09 [1]\n textshaping     0.3.3      2021-03-16 [1]\n thematic        0.1.1      2021-01-16 [1]\n tibble          3.1.0      2021-02-25 [1]\n tidyselect      1.1.0      2020-05-11 [1]\n timeDate      * 3043.102   2018-02-21 [1]\n timeSeries    * 3062.100   2020-01-24 [1]\n utf8            1.2.1      2021-03-12 [1]\n vctrs           0.3.6      2020-12-17 [1]\n withr           2.4.1      2021-01-26 [1]\n xfun            0.22       2021-03-11 [1]\n xtable          1.8-4      2019-04-21 [1]\n yaml            2.2.1      2020-02-01 [1]\n source                            \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n Github (rstudio/htmltools@ac43afe)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n Github (rstudio/rmarkdown@61db7a9)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n\n[1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library\n\n\n\n\n",
    "preview": "posts/2021-03-19-usare-r-per-risolvere-gli-esercizi-sulle-distribuzioni-di-probabilit-discrete/preview.png",
    "last_modified": "2021-03-21T18:27:55+01:00",
    "input_file": {},
    "preview_width": 1280,
    "preview_height": 586
  },
  {
    "path": "posts/2021-03-18-varianza-e-valore-atteso-una-semplice-simulazione-in-r/",
    "title": "Varianza e valore atteso",
    "description": "Una semplice simulazione in R",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-18",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\n\n\nlibrary(\"tidyverse\", warn.conflicts = FALSE)\n\n\n\nProblema\nSupponiamo di sapere che, sulla base dei dati di un campione molto grande, possiamo stabilire che nella popolazione oncologica, i punteggi di un test psicometrico atto a misurare l’astenia seguono la distribuzione di massa di probabilità indicata qui sotto:\nPunteggio\nProbabilità\n1\n0.1\n2\n0.5\n3\n0.3\n4\n0.1\nLaddove i valori indicano la frequenza con la quale tale sintomo, riduzione di energia dell’individuo, si manifesta. Per esempio, potremmo avere che 4 corrisponde a “molto spesso”, 3 corrisponde a “spesso”, 2 corrisponde a “alle volte” e 1 corrisponde a “raramente o mai”.\nRappresentazione grafica della distribuzione di massa di probabilità\n\n\n# valori della variabile aleatoria\ny <- 1:4\n\n# probabilità\npy <- c(0.1, 0.5, 0.3, 0.1)\n\nd <- data.frame(y, py)\n\nd %>% \n  ggplot(aes(y, py)) +\n  geom_point(size = 4) +\n  geom_linerange(aes(x=y, ymax=py, ymin=0.0)) +\n  ylim(0, 0.55) +\n  labs(\n    title = \"Astenia nella popolazione oncologica\",\n    x = \"Punteggio del test\",\n    y = \"Probabilità\"\n  )\n\n\n\n\nPoniamoci ora il problema di trovare il valore atteso del punteggio di astenia e la varianza del punteggio di astenia di questo ipotetico test. Soprattutto, ci poniamo il problema di assegnare un’interpretazione inuitiva a questi concetti.\nValore atteso\nLa definizione di valore atteso della variabile aleatoria \\(y\\) è\n\\[\n\\mathbb{E}(y) = \\sum_{i=1}^n y_i \\cdot p_i\n\\]\nil che, per i dati del problema, significa\n\\[\n1 \\cdot 0.1 + 2 \\cdot 0.5 + 3 \\cdot 0.3 + 4 \\cdot 0.1 = 2.4\n\\]\n\n\n1 * 0.1 + 2 * 0.5 + 3 * 0.3 + 4 * 0.1\n\n\n[1] 2.4\n\novvero\n\n\nev_y <- sum(y * py)\nev_y\n\n\n[1] 2.4\n\nInterpretazione\nMa che significa, in pratica, il valore atteso? Per rispondere a questa domanda, eseguiamo una simulazione. Ovvero, estraiamo un campione molto grade di osservazioni da una popolazione che ha le caratteristiche descritte, ovvero nella quale il valore 1 è presente nel 10% delle osservazioni, il valore 2 è presente nel 50% delle osservazioni, eccetera.\nImplemento qui di seguito una funzione che consente di estrarre dei campioni casuali, di qualunque ampiezza, da una siffatta popolazione:\n\n\nsample_distr = function(n) { \n  sample(\n    x = 1:4, \n    n, \n    replace = TRUE, \n    prob = c(0.1, 0.5, 0.3, 0.1)\n  ) \n}\n\n\n\nEstraggo ora un campione di ampiezza \\(n\\) = 100,000 e lo chiamo x:\n\n\nx <- sample_distr(1e5)\n\n\n\nCalcolo ora la media di tale campione:\n\n\nmean(x)\n\n\n[1] 2.40157\n\nIl risultato della simulazione mostra che la media di un capione molto grande estratto dalla popolazione specificata è (quasi) uguale al valore atteso della variabile aleatoria. Questo ci consente di assegnare un’inerpretazione intuitiva al concetto di valore atteso:\n\nil valore atteso è la media aritmetica di un numero molto grande di realizzazioni della variabile aleatoria.\n\nVarianza\nLa definizione di varianza di una variabile aleatoria è la seguente:\n\\[\nVar(y) = \\sum_{i=1}^n (y_i - \\mu)^2 \\cdot p_i\n\\]\nImplementiamo la formula in R per i dati del problema:\n\\[\n(1 - 4.4)^2 \\cdot 0.1 + (2 - 4.4)^2 \\cdot 0.5 +(3 - 4.4)^2 \\cdot 0.3 +(4 - 4.4)^2 \\cdot 0.1 = 0.64\n\\]\novvero\n\n\nsum((y - ev_y)^2 * py)\n\n\n[1] 0.64\n\nFormula alternativa\nUsiamo ora la formula alternativa per il calcolo della varianza:\n\\[\nVar(y) = \\mathbb{E}(y^2) - \\mathbb{E}(y)^2= \\sum_{i=1}^n y_i^2\\cdot p_i - \\Bigg(\\sum_{i=1}^n y_i \\cdot p_i\\Bigg)^2\n\\]\n\n\nsum(y^2 * py) - ev_y^2\n\n\n[1] 0.64\n\nInterpretazione\nDi nuovo, ci chiediamo: che cosa è, in pratica, la varianza di una variabile aleatoria? Possiamo rispondere a questa domanda ripetendo il ragionamento fatto sopra.\nAvendo extratto 100,000 valori dalla popolazione di riferimento, calcoliamo la varianza di tali valori, usando la formula della statistica descrittiva:\n\n\nvar(x)\n\n\n[1] 0.643938\n\net voilà! Il valore che abbiamo trovato, utilizzando la formula della statistica descrittiva (anche senza correggere il denominatore, dato che \\(n\\) è molto grande) ci dà un risultato molto simile a quello della varianza della variabile aleatoria.\nQuindi, anche in questo caso, l’interpretazione è semplice:\n\nla varianza di una variabile aleatoria non è altro che la varianza, nel senso della statistica descrittiva, di un numero molto grande di realizzazioni della variabile aleatoria.\n\nCome sempre, è più semplice interpretare la radice quadrata della varianza: la deviazione standard, infatti, è espressa nella stessa unità di misura dei valori grezzi della variabile in esame.\n\n\n\n",
    "preview": "posts/2021-03-18-varianza-e-valore-atteso-una-semplice-simulazione-in-r/varianza-e-valore-atteso-una-semplice-simulazione-in-r_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-03-21T20:19:09+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-13-la-struttura-del-progetto/",
    "title": "La struttura del progetto",
    "description": "Salvare e assegnare un nome ai documenti di un progetto.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-13",
    "categories": [
      "R",
      "Psicometria",
      "tesi"
    ],
    "contents": "\nDi seguito riporto il link ad un breve video-tutorial che cerca di rispondere alle seguenti domande: come organizzo tutto il materiale che fa parte di un progetto? Quali sono i principi che devo seguire per assegnare i nomi ai file? Come devo organizzare i file nelle cartelle? L’obiettivo è quello di immagazzinare sul computer il lavoro che abbiamo fatto in maniera tale che, in futuro, sarà facile recuperare quello che ci serve. E in modo tale che sia facile lavorare al nostro progetto nel presente.\nLe considerazioni che faccio fanno riferimento a delle raccomandazioni che “sono nell’aria”, ovvero, che sono condivise da molte persone. Questo materiale è stato organizzato in una serie di video youtube da Danielle Navarro e io mi limito a commentare brevemente quello che lei dice. Ovviamente, invece di seguire il mio video-tutorial, potete guardare direttamente i video di Danielle Navarro.\nUna cosa che mi sono dimenticato di dire nel mio video, e che invece è presente nei video youtube, è la seguente: dove dobbiamo salvare la cartella che contiene tutto il materiale di un progetto? Pessime risposte a questa domanda sono: il Desktop oppure la cartella di Download. Peggio di così non si può fare. Perché sia il Desktop sia la cartella Download contengono informazioni transienti, ovvero file che butterete via ad un certo punto – presto, si spera. Invece la cartella che contiene il vostro progetto contiene il frutto del vostro lavoro – e certamente non volete cancellarla per sbaglio. Quindi, un’idea molto migliore è quella di salvare la cartella del progetto nella cloud, ovvero, sul vostro comupter in cartelle come OneDrive o Dropbox.\nSpero che questo sia utile.\n\n\n\n",
    "preview": "posts/2021-03-13-la-struttura-del-progetto/preview.png",
    "last_modified": "2021-03-19T21:06:34+01:00",
    "input_file": {},
    "preview_width": 568,
    "preview_height": 912
  },
  {
    "path": "posts/2021-03-11-manipolazione-dei-dati-con-dplyr/",
    "title": "Manipolazione dei dati con dplyr",
    "description": "Tutorial sull'uso delle funzionalità di base di dplyr per la manipolazione dei dati.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-11",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\nAggiungo qui il link ad un video-tutorial che ho preparato relativamente alla manipolaizone dei dati usando le funzioni del pacchetto dplyr.\nNel video non faccio altro che commentare un tutorial predisposto da Allison Horst e disponibile seguendo questo link. Il tutorial di Allison Horst è fatto benissimo e non vedo ragioni di tradurlo o cambiarlo in qualche modo. Inoltre, se andate nella pagina web che ho indicato sopra, potete anche fare degli esercizi che vi consentono di verificare se avete capito come utilizzare in pratica le istruzioni R che vengono discusse – le risposte agli esercizi sono immediatamente disponibili il che rende il tutorial di Allison Horst un utilissimo strumento di apprendimento. Buon divertimento!\n\n\n\n",
    "preview": "posts/2021-03-11-manipolazione-dei-dati-con-dplyr/preview.png",
    "last_modified": "2021-04-01T10:12:20+02:00",
    "input_file": {},
    "preview_width": 1022,
    "preview_height": 554
  },
  {
    "path": "posts/2021-03-10-introduzione-a-r-markdown/",
    "title": "Introduzione a R Markdown",
    "description": "Creare un documento R Markdown con R Studio.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-10",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\nPosto qui il link ad un breve video-tutorial sull’uso di R Markdown.\nLa migliore descrizione del flusso di lavoro (workflow) con R Markdown è fornita in questo capitolo di R for Data Science.\nPer chi ha bisogno di un’introduzione, risultano sicuramente utili anche le slide di Danielle Navarro.\n\n\n\n",
    "preview": "posts/2021-03-10-introduzione-a-r-markdown/preview.png",
    "last_modified": "2021-03-12T06:49:05+01:00",
    "input_file": {},
    "preview_width": 1008,
    "preview_height": 582
  },
  {
    "path": "posts/2021-03-10-notazione-sommatoria/",
    "title": "Notazione sommatoria",
    "description": "Simbolo di somma.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-10",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\n\nIl simbolo \\(\\sum_{i=1}^{\\infty}\\) indica che dobbiamo assegnare al numero intero \\(i\\) tutti i suoi valori \\(1, 2, 3, ...\\) ed eseguire la somma dei termini. (Jean-Baptiste Joseph Fourier)\n\nLe somme si incontrano costantemente in svariati contesti matematici e statistici quindi abbiamo bisogno di una notazione adeguata che ci consenta di gestirle. Si veda, ad esempio, Wikipedia.\nSimbolo di somma (sommatorie)\nLa somma dei primi \\(n\\) numeri interi può essere scritta come \\(1+2+\\dots+(n-1)+n\\), dove `\\(\\dots\\)’ ci dice di completare la sequenza definita dai termini che vengono prima e dopo. Ovviamente, una notazione come \\(1+7+\\dots+73.6\\) non ha alcun senso senza qualche altro tipo di precisazione. In generale, si incontrano delle somme nella forma\n\\[\\begin{equation}\nx_1+x_2+\\dots+x_n,\n\\end{equation}\\]\ndove \\(x_i\\) è un numero che è stato definito altrove. La notazione precedente, che fa uso dei tre puntini di sospensione, è utile in alcuni contesti ma in altri risulta ambigua. Pertanto la notazione di uso corrente è del tipo\n\\[\\begin{equation}\n  \\sum_{i=1}^n x_i\n\\end{equation}\\]\n e si legge ``sommatoria per \\(i\\) che va da \\(1\\) a \\(n\\) di \\(x_i\\).’’ Il simbolo \\(\\sum\\) (lettera sigma maiuscola dell’alfabeto greco) indica l’operazione di somma, il simbolo \\(x_i\\) indica il generico addendo della sommatoria, le lettere \\(1\\) ed \\(n\\) indicano i cosiddetti , ovvero l’intervallo (da \\(1\\) fino a \\(n\\) estremi inclusi) in cui deve variare l’indice \\(i\\) allorché si sommano gli addendi \\(x_i\\). Solitamente l’estremo inferiore è \\(1\\) ma potrebbe essere qualsiasi altri numero \\(m < n\\). Quindi \\[\n  \\sum_{i=1}^n x_i = x_1 + x_{2} + \\dots + x_{n}.\n\\] Per esempio, se i valori \\(x\\) sono \\(\\{3, 11, 4, 7\\}\\), si avrà \\[\n  \\sum_{i=1}^4 x_i = 3+11+4+7 = 25 \n\\] laddove \\(x_1 = 3\\), \\(x_2 = 11\\), eccetera. La quantità \\(x_i\\) nella formula precedente si dice l’ della sommatoria, mentre la variabile \\(i\\), che prende i valori naturali successivi indicati nel simbolo, si dice  della sommatoria.\nLa notazione di sommatoria può anche essere fornita nella forma seguente\n\\[\\begin{equation}\n  \\sum_{P(i)} x_i\n\\end{equation}\\]\ndove \\(P(i)\\) è qualsiasi proposizione riguardante \\(i\\) che può essere vera o falsa. Quando è ovvio che si vogliono sommare tutti i valori di \\(n\\) osservazioni, la notazione può essere semplificata nel modo seguente: \\(\\sum_{i} x_i\\) oppure \\(\\sum x_i\\). Al posto di \\(i\\) si possono trovare altre lettere: \\(k, j, l, \\dots\\),.\nManipolazione di somme\nÈ conveniente utilizzare le seguenti regole per semplificare i calcoli che coinvolgono l’operatore della sommatoria.\nProprietà 1\nLa sommatoria di \\(n\\) valori tutti pari alla stessa costante \\(a\\) è pari a \\(n\\) volte la costante stessa:\n\\[\n  \\sum_{i=1}^{n} a =  \\underbrace{a + a + \\dots + a}_{n~\\text{volte}} = n a.\n  \\]\nProprietà 2 (proprietà distributiva)\nNel caso in cui l’argomento contenga una costante, è possibile riscrivere la sommatoria. Ad esempio con \\[\n  \\sum_{i=1}^{n} a x_i =  a x_1 + a x_2 + \\dots + a x_n\n  \\] è possibile raccogliere la costante \\(a\\) e fare \\(a(x_1 +x_2 + \\dots + x_n)\\). Quindi possiamo scrivere \\[\n  \\sum_{i=1}^{n} a x_i =  a  \\sum_{i=1}^{n} x_i.\n  \\]\nProprietà 3 (proprietà associativa)\nNel caso in cui \\[\n  \\sum_{i=1}^{n} (a + x_i) =  (a + x_1) +  (a + x_1) + \\dots  (a + x_n)\n  \\] si ha che \\[\n  \\sum_{i=1}^{n} (a + x_i) =  n a + \\sum_{i=1}^{n} x_i.\n  \\] È dunque chiaro che in generale possiamo scrivere \\[\n  \\sum_{i=1}^{n} (x_i + y_i) =  \\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} y_i.\n  \\]\nProprietà 4\nSe deve essere eseguita un’operazione algebrica (innalzamento a potenza, logaritmo, ecc.) sull’argomento della sommatoria, allora tale operazione algebrica deve essere eseguita prima della somma. Per esempio,\n\\[\n\\sum_{i=1}^{n} x_i^2 = x_1^2 + x_2^2 + \\dots + x_n^2 \\neq \\left(\\sum_{i=1}^{n} x_i \\right)^2.\n\\]\nProprietà 5\nNel caso si voglia calcolare \\(\\sum_{i=1}^{n} x_i y_i\\), il prodotto tra i punteggi appaiati deve essere eseguito prima e la somma dopo:\n\\[\n\\sum_{i=1}^{n} x_i y_i = x_1 y_1 + x_2 y_2 + \\dots + x_n y_n,\n\\] infatti, \\(a_1 b_1 + a_2 b_2 \\neq (a_1 + a_2)(b_1 + b_2)\\).\nDoppia sommatoria}\nÈ possibile incontrare la seguente espressione in cui figurano una doppia sommatoria e un doppio indice:\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{m} x_{ij}.\n\\]\nLa doppia sommatoria comporta che per ogni valore dell’indice esterno, \\(i\\) da \\(1\\) ad \\(n\\), occorre sviluppare la seconda sommatoria per \\(j\\) da \\(1\\) ad \\(m\\). Quindi,\n\\[\n\\sum_{i=1}^{3}\\sum_{j=4}^{6} x_{ij} = (x_{1, 4} + x_{1, 5} + x_{1, 6}) + (x_{2, 4} + x_{2, 5} + x_{2, 6}) + (x_{3, 4} + x_{3, 5} + x_{3, 6}).\n\\]\nUn caso particolare interessante di doppia sommatoria è il seguente:\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} x_i y_j\n\\]\nSi può osservare che nella sommatoria interna (quella che dipende dall’indice \\(j\\)), la quantità \\(x_i\\) è costante, ovvero non dipende dall’indice (che è \\(j\\)). Allora possiamo estrarre \\(x_i\\) dall’operatore di sommatoria interna e scrivere\n\\[\n\\sum_{i=1}^{n} \\left( x_i \\sum_{j=1}^{n} y_j \\right).\n\\]\nAllo stesso modo si può osservare che nell’argomento della sommatoria esterna la quantità costituita dalla sommatoria in \\(j\\) non dipende dall’indice \\(i\\) e quindi questa quantità può essere estratta dalla sommatoria esterna. Si ottiene quindi\n\\[\n\\sum_{i=1}^{n}\\sum_{j=1}^{n} x_i y_j = \\sum_{i=1}^{n} \\left( x_i \\sum_{j=1}^{n} y_j \\right) = \\sum_{i=1}^{n}\\ x_i \\sum_{j=1}^{n} y_j.\n\\]\nEsercizio\nSi verifichi quanto detto sopra nel caso particolare di \\(x = \\{2, 3, 1\\}\\) e \\(y = \\{1, 4, 9\\}\\), svolgendo prima la doppia sommatoria per poi verificare che quanto così ottenuto sia uguale al prodotto delle due sommatorie.\nSoluzione\n\\[\\begin{align}\n\\sum_{i=1}^3 \\sum_{j=1}^3 x_i y_j &= x_1y_1 + x_1y_2 + x_1y_3 + \nx_2y_1 + x_2y_2 + x_2y_3 + \nx_3y_1 + x_3y_2 + x_3y_3 \\notag\\\\\n&= 2 \\times (1+4+9) + 3 \\times (1+4+9) + 2 \\times (1+4+9) = 84,\\notag\n\\end{align}\\] ovvero \\[\n(2 + 3 + 1) \\times (1+4+9) = 84.\n\\]\nSommatorie con R\nPer evitare errori di calcolo, possiamo usare R per risolvere questo tipo di problemi. I dati a cui facciamo riferimento sono codificati nella forma di un vettore, il che corrisponde semplicemente ad un insieme ordinato di numeri. Solitamente è ciò che chiamiamo variabile, ovvero quello che uno psicologo misura e vuole descrivere o analizzare in qualche modo. Per esempio, una variabile può corrispondere al QI di un insieme di individui. Abbiamo visto che, in R, possiamo definire questo insieme di dati usando la funzione c() (che crea un vettore).\nSupponiamo di avere misurato il QI di 5 persone e di avere ottenuto i risultati seguenti: 102, 98, 122, 109, 89.\nPer manipolare questi dati, dobbiamo prima renderli disponibili nel workspace della sessione di R:\n\n\nx <- c(102, 98, 122, 109, 89)\nx\n\n\n[1] 102  98 122 109  89\n\nSupponiamo di volere sommare questi valori:\n\\[\n\\sum_{i = 1}^5 x_i\n\\]\nSe sviluppiamo la notazione precedente, per i dati dell’esempio avremo\n\\[\n\\sum_{i = 1}^5 x_i = x_1 + x_2 + x_3 + x_4 + x_5\n\\]\nladdove\n\\[\nx_1 = 102 \\quad x_2 = 98 \\quad x_3 = 122 \\quad x_4 = 109 \\quad x_5 = 89.\n\\]\nIn R una sommatoria si svolge utilizzando la funzione sum(). Quindi, nel caso presente, avremo\n\n\nsum(x)\n\n\n[1] 520\n\nLa notazione della sommatoria viene utilizzata, per esempio, nel calcolo della media:\n\\[\n\\bar{x} = \\frac{\\sum_{i = 1}^{n} x_i}{n}\n\\]\nNella formula precedente, sommiamo prima i valori contenuti nel vettore x e poi dividiamo il risultato ottenuto per n = 5, ovvero 520 / 5 = 104.\nIn R abbiamo\n\n\nsum(x) / length(x)\n\n\n[1] 104\n\nladdove la funzione length(x) ci restituisce i numero di elementi che costituiscono il vettore x, ovvero 5 nel caso dell’esempio.\nIn maniera equivalente, per le proprietà delle sommatorie presentate sopra, la formula della media può essere scritta come\n\\[\n\\bar{x} = \\frac{1}{n}\\sum_{i = 1}^{n} x_i = \\sum_{i = 1}^{n}\\frac{1}{n} x_i,\n\\]\nil che significa che possiamo prima moltiplicare ciascun elemento di x per 1/n e poi sommare, come indicato qui di seguito:\n\\[\n(1/5 \\times 102) + (1/5 \\times 98) + (1/5 \\times  122) + (1/5 \\times  109) + (1/5 \\times  89)\n\\]\nIn R questo diventa\n\n\nsum(1/5 * x)\n\n\n[1] 104\n\nperché\n\n\n1/5 * x\n\n\n[1] 20.4 19.6 24.4 21.8 17.8\n\ncorrisponde al vettore\n\\[\n(1/5 \\times 102) + (1/5 \\times 98) + (1/5 \\times  122) + (1/5 \\times  109) + (1/5 \\times  89)\n\\]\ndopodiché sommiamo utilizzando la funzione sum().\nLa cosa importante da ricordare è che le operazioni algebriche (in questo caso moltiplicare per 1/5), quando vengono applicate ad un vettore (nel nostro caso (102, 98, 122, 109, 89)) si calcolano elemento per elemento. Ovvero, a ciascun elemento viene applicata l’operazione algebrica indicata, cioè il valore 102 viene moltiplicato per 1/5, il valore 98 viene moltiplicato per 1/5, ecc.\n\n\n\n",
    "preview": "posts/2021-03-10-notazione-sommatoria/preview.png",
    "last_modified": "2021-03-12T06:51:12+01:00",
    "input_file": {},
    "preview_width": 1226,
    "preview_height": 736
  },
  {
    "path": "posts/2021-03-08-introduzione-a-r-1/",
    "title": "Introduzione a R (1)",
    "description": "Il primo di una serie di post che presentano la sintassi di base di R.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-08",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\n\nContents\nSintassi\nR: linguaggio + ambiente\nRStudio: Ambiente di sviluppo integrato (IDE) per R\n\nAssegnazione\nR usato come un calcolatore\nEsecuzione di comandi in R\nTre modi per eseguire i comandi in R\n\nOggetti\nClassi di oggetti\n\nVettori\nFunzioni\nFunzione mean()\nFunzione length()\n\nData type di un vettore\nData type character\nData type logical\nI vettori sono omogenei\n\nSequenze\nAritmetica vettorializzata\nDati di tipo data.frame\nChe cosa c’è da capire su vettori e data.frame?\nSelezione di elementi\n\n\nSintassi\nÈ difficile sapere da dove iniziare quando si deve imparare un nuovo linguaggio di programmazione. Questi file Rmd hanno lo scopo di fornire qualche informazione di base che spero possa dare un’idea generale di come R viene usato.\nCi sono approcci diversi per imparare un linguaggio di programmazione come R. L’approccio meno efficiente è quello che ci porta a cercare le informazioni a proposito della sintassi di R quando ci servono per risolvere un problema specifico. Questo approccio è motivato dal fatto che non è molto divertente studiare la sintassi di un linguaggio di programmazione in termini astratti. Questo è l’approccio che seguo io. Per cui, secondo me, il modo migliore per imparare la sintassi di R è quello di… usare R e cercare con Google la soluzione di ciascuno specifico problema di sintassi, quando esso si presenta.\nL’approccio più tradizionale (e molto più efficiente) è invece quello di leggere un manuale in cui tutto viene presentato in maniera sistematica. È facilissimo trovare in rete un enorme numero di guide all’utilizzo dell’ambiente statistico R. Uno dei punti di entrata in questo mondo è sicuramente il testo scritto da Hadley Wickham: R for Data Science. Può anche essere utile e/o divertente leggere i messaggi twitter che utilizzano l’hashtag #rstats.\nR: linguaggio + ambiente\nR è un port al linguaggio S, sviluppata presso i Bell Labs. R è open source e gratuito da utilizzare e distribuire. Può essere installato e utilizzato sui principali sistemi operativi.\nR può essere inteso come un linguaggio e un ambiente integrati che sono stati progettati avendo come finalità il calcolo statistico e l’analisi dei dati. A tal fine, la sua struttura rappresenta un compromesso tra una base di codice he è stata ottimizzata per le procedure matematiche sopra la quale sono state sviluppate funzionalità di alto livello che possono essere utilizzate in modo interattivo. In altre parole, è un ottimo strumento per lavorare in modo interattivo con dati quantitativi.\nLe funzionalità di R si stanno estendendo sempre più attraverso pacchetti forniti dagli utenti. Useremo un certo numero di pacchetti in questo insegnamento, soprattutto quelli sviluppati all’interno della famiglia tidyverse.\nRStudio: Ambiente di sviluppo integrato (IDE) per R\nRStudio fa quasi tutto ciò che è può essere fatto con R, in maniera più semplice. Quindi è un ottimo programma completo per l’utilizzo di R. Lo consiglio vivamente.\nAssegnazione\nIn R, qualcunque cosa è un oggetto. Gli oggetti sono come scatole in cui possiamo mettere le cose: dati, funzioni e persino altri oggetti.\nPrima di discutere i tipi di dati e le strutture, il primo argomento che presenteremo relativamente alla sintassi di R è come assegnare valori agli oggetti. In R, il mezzo principale di assegnazione è la freccia, <-, che è un simbolo minore di <, seguito da un trattino, -.\n\n\nx <- 3\nx\n\n\n[1] 3\n\ne anche\n\n\ny <- \"estate\"\ny\n\n\n[1] \"estate\"\n\nR usato come un calcolatore\nR può essere usato come qualunque calcolatore portatile. Avendo a disposizione un computer è ridicolo usare un calcolatore portatile. All’esame, molti studenti si ostinano ad usare uno smartphone per fare i calcoli. Credetemi, è molto più facile fare i calcoli usando R su un computer!\nLe operazioni algebriche si svolgono nel modo seguente:\n\n\n5 + 2\n\n\n[1] 7\n\n7 - 3\n\n\n[1] 4\n\n10 * 3\n\n\n[1] 30\n\n21 / 3\n\n\n[1] 7\n\nx^2\n\n\n[1] 9\n\nsqrt(9)\n\n\n[1] 3\n\nEsecuzione di comandi in R\nTre modi per eseguire i comandi in R\nUsare la console.\nDigitare / copiare i comandi direttamente nella “console” (non una buona idea, a meno che facciamo qualcosa di estremamente semplice e non ci interessa salvare il procedimento).\nUsare uno script R (file .R).\nUno script R può essere creato in RStudio mediante il menu a tendina File > New File > R Script. Può essere salvato come un qualunque altro file e può essere riutilizzato e modificato in un secondo momento. È possibile eseguire un comando alla volta, più comandi alla volta o l’intero script. - Cmd / Ctrl + Invio: esegue le linee evidenziate - Cmd / Ctrl + Maiusc + Invio (senza evidenziare alcuna riga): esegue l’intero script.\nUsare un file RMarkdown (file .Rmd).\nI file RMarkdown possono essere creati in RStudio mediante il menu a tendina File > New File > R Notebook. Un file RMarkdown è costituito da diversi “code chunks”. È possibile eseguire un comando alla volta, un blocco alla volta o eseguire “knit” sull’intero documento. - Cmd / Ctrl + Invio: esegue le linee evidenziate all’interno del blocco - Cmd / Ctrl + Shift + k: “knit” per l’intero documento\nOggetti\nR è un linguaggio di programmazione “object-oriented” (come Python). Ma cos’è un “oggetto”?\nIntuitivamente, possiamo pensare che gli oggetti siano qualcosa a cui vengono assegnati dei valori.\n\n\na <- 5\na\n\n\n[1] 5\n\nb <- \"eilà!\"\nb\n\n\n[1] \"eilà!\"\n\n\nObjects are like boxes in which we can put things: data, functions, and even other objects.\n\nSi noti che per assegnare un numero all’oggetto a abbiamo digitato il numero a sinistra dell’operatore di assegnazione. Nel caso di una stringa, invece, è stato necessario scrivere la serie di simboli alfanumerici tra virgolette.\nNon c’è limite al numero di oggetti che R può contenere (tranne la memoria).\nClassi di oggetti\nGli oggetti possono essere categorizzati in base alle categorie “type” e “class”.\nAd esempio, una data è un oggetto con type numerico e una class “date”;\nun dataset è un oggetto con specifiche caratteristiche type e class.\nVettori\nL’oggetto fondamentale in R è il vettore\nUn vettore è un insieme di valori.\nI singoli valori all’interno di un vettore sono chiamati “elementi”.\nI valori in un vettore possono essere numerici o possono essere delle stringhe alfanumeriche (ad esempio “Pomodoro”) o appartenenti ad altre classi type.\nDi seguito utilizzeremo la funzione di concatenazione c() per creare un vettore.\nIl file di aiuto dice che la funzione c() “concatena i suoi argomenti in un vettore o in un elenco”.\n\n\n?c\n\n\n\nCreiamo ora un vettore in cui gli elementi sono dei numeri\n\n\nx <- c (4, 7, 9)\nx\n\n\n[1] 4 7 9\n\noppure un vettore in cui gli elementi sono delle stringhe\n\n\nanimali <- c(\"leoni\", \"tigri\", \"orsi\", \"Aaaargh!\")\nanimali\n\n\n[1] \"leoni\"    \"tigri\"    \"orsi\"     \"Aaaargh!\"\n\nFunzioni\nLe “funzioni” R applicano diversi tipi di trasformazioni a oggetti con diverse caratteristiche type e class.\nFunzione mean()\nAd esempio, la funzione mean() calcola la media per oggetti con type = numerico e class = vector; la funzione mean() non può essere applicata ad oggetti con type = character (ad es. “eilà!”).\n\n\nx\n\n\n[1] 4 7 9\n\nmean(x)\n\n\n[1] 6.666667\n\nanimali\n\n\n[1] \"leoni\"    \"tigri\"    \"orsi\"     \"Aaaargh!\"\n\nmean(animali)\n\n\n[1] NA\n\nFunzione length()\nLa funzione `length() ritorna il numero di elementi di un vettore.\n\n\nx\n\n\n[1] 4 7 9\n\nlength(x)\n\n\n[1] 3\n\nanimali\n\n\n[1] \"leoni\"    \"tigri\"    \"orsi\"     \"Aaaargh!\"\n\nlength(animali)\n\n\n[1] 4\n\nUn singolo numero (o una singola stringa / carattere) è un vettore con length==1\n\n\nz <- 5\nlength(z)\n\n\n[1] 1\n\nlength(\"Maria\")\n\n\n[1] 1\n\nData type di un vettore\nIl “type” di un vettore descrive la classe a cui appartengono gli elementi del vettore. Si possono definire sei diversi “types” di elementi dei vettori, ma qui ne considereremo solo tre:\nnumeric:\n“integer” (e.g., 5)\n“double” (e.g., 5.5)\n\ncharacter (e.g., “ozan”)\nlogical (e.g., TRUE, FALSE)\nLa funzione typeof() viene usata per esaminare il “type” di un vettore:\n\n\nx\n\n\n[1] 4 7 9\n\ntypeof(x)\n\n\n[1] \"double\"\n\nanimali\n\n\n[1] \"leoni\"    \"tigri\"    \"orsi\"     \"Aaaargh!\"\n\ntypeof(animali)\n\n\n[1] \"character\"\n\nData type character\nA differenza dei tipi di dati “numeric” utilizzati per memorizzare i numeri, il tipo di dati “character” viene utilizzato per memorizzare stringhe di testo.\nLe stringhe possono contenere qualsiasi combinazione di numeri, lettere, simboli, ecc.\nI vettori di tipo “character” sono a volte indicati come vettori di tipo alfanumerico.\nQuando si crea un vettore in cui gli elementi hanno type == character (o quando si fa riferimento al valore di una stringa), si pongano le virgolette singole o doppie attorno al testo.\nIl testo tra virgolette è chiamato stringa.\n\n\nc1 <- c(\"cane\", 'felicità', 'cioccolata', 'tranquillità')\nc1\n\n\n[1] \"cane\"         \"felicità\"     \"cioccolata\"   \"tranquillità\"\n\ntypeof(c1)\n\n\n[1] \"character\"\n\nlength(c1)\n\n\n[1] 4\n\nI valori numerici possono anche essere memorizzati come stringhe\n\n\nc2 <- c(\"1\", \"2\", \"3\")\nc2\n\n\n[1] \"1\" \"2\" \"3\"\n\ntypeof(c2)\n\n\n[1] \"character\"\n\nIn tale caso però non possiamo applicare ad essi alcuna operazione aritmetica:\n\n\nmean(c2)\n\n\n[1] NA\n\nData type logical\nI vettori logici possono assumere tre possibili valori: TRUE, FALSE, NA.\nTRUE, FALSE, NA sono delle parole chiave; hanno un significato diverso dalle stringhe \"TRUE\", \"FALSE\", \"NA\"\nLa parola chiave NA viene utilizzata per codificare i dati mancanti.\n\n\ntypeof(TRUE)\n\n\n[1] \"logical\"\n\ntypeof(\"TRUE\")\n\n\n[1] \"character\"\n\ntypeof(c(TRUE,FALSE,NA))\n\n\n[1] \"logical\"\n\ntypeof(c(TRUE,FALSE,NA,\"FALSE\"))\n\n\n[1] \"character\"\n\nlog <- c(TRUE,TRUE,FALSE,NA,FALSE)\ntypeof(log)\n\n\n[1] \"logical\"\n\nlength(log)\n\n\n[1] 5\n\nI vettori sono omogenei\nTutti gli elementi di un vettore devono appartenere allo stesso tipo di dati.\nSe un vettore contiene elementi di diverso tipo, il vettore sarà della classe type dell’elemento più “complesso”,\nI tipi di vettori atomici, dal più semplice al più complesso, seguono la seguente gerarchia:\nlogical < integer < double < character\n\n\ntypeof(c(TRUE, TRUE, NA))\n\n\n[1] \"logical\"\n\ntypeof(c(TRUE, TRUE, NA, 1L)) \n\n\n[1] \"integer\"\n\ntypeof(c(TRUE, TRUE, NA, 1.5))\n\n\n[1] \"double\"\n\ntypeof(c(TRUE, TRUE, NA, 1.5, \"come va?\"))\n\n\n[1] \"character\"\n\nSequenze\nDefinizione: una sequenza è un insieme di numeri in ordine ascendente o discendente\nUn vettore contenente una “sequenza” di numeri (ad es. 1, 2, 3) può essere creato usando l’operatore : con la notazione start : end\n\n\n-5:5\n\n\n [1] -5 -4 -3 -2 -1  0  1  2  3  4  5\n\n5:-5\n\n\n [1]  5  4  3  2  1  0 -1 -2 -3 -4 -5\n\ns <- 1:10 \ns\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nlength(s)\n\n\n[1] 10\n\nSu possono creare sequenze usando la funzione seq():\n\n\nseq(from = 1, to = 1, by = 1)\n\n\n[1] 1\n\n\n\nseq(10, 15)\n\n\n[1] 10 11 12 13 14 15\n\nseq(from = 10, to = 15, by = 1)\n\n\n[1] 10 11 12 13 14 15\n\nseq(from = 100, to = 150, by = 10)\n\n\n[1] 100 110 120 130 140 150\n\nAritmetica vettorializzata\nLe operazioni aritmetiche sui vettori vengono applicate ``elemento per elemento’’.\nQuesto è un punto estremamente importante a cui è necessario prestare particolare attenzione.\nAd esempio, se un singolo valore viene sommato ad un vettore, tale valore verrà sommato a ciascun elemento del vettore:\n\n\n1:3\n\n\n[1] 1 2 3\n\n1:3 + 0.5\n\n\n[1] 1.5 2.5 3.5\n\n(1:3) * 2\n\n\n[1] 2 4 6\n\nAnche le operazioni aritmetiche che coinvolgono due vettori della stessa lunghezza vengono applicate ``elemento per elemento’’.\nPer la somma di due vettori, ad esempio, R esegue quanto segue: somma il primo elemento del vettore 1 al primo elemento del vettore 2; somma il secondo elemento del vettore 1 al secondo elemento del vettore 2; ecc.\nLo stesso vale per le altre operazioni algebriche sui vettori.\n\n\nc(1, 1, 1) + c(1, 0, -2)\n\n\n[1]  2  1 -1\n\nc(1, 1, 1) * c(1, 0, -2)\n\n\n[1]  1  0 -2\n\nDati di tipo data.frame\nLa matrice dei dati può essere codificata in R usando quella struttura di dati che va sotto il nome di data.frame.\nLa maggior parte delle volte i data.frame vengono importati da fonti esterne. Tuttavia, è anche possibile crearli direttamente in R. A questo fine usiamo la funzione data.frame():\n\n\ndf <- data.frame(\n  col_a = c(1, 2, 3, 4),\n  col_b = c(5, 6, 7, 8),\n  col_c = c(9, 10, 11, 12)\n)\n\n\n\nStampiamo il contenuto di df sulla console:\n\n\ndf\n\n\n  col_a col_b col_c\n1     1     5     9\n2     2     6    10\n3     3     7    11\n4     4     8    12\n\n\n\n## check\nis.data.frame(df)\n\n\n[1] TRUE\n\n\n\nhead(df) # stampa le prime 6 righe\n\n\n  col_a col_b col_c\n1     1     5     9\n2     2     6    10\n3     3     7    11\n4     4     8    12\n\nI data.frame possiedono vari attributi:\n\n\nnames(df)\n\n\n[1] \"col_a\" \"col_b\" \"col_c\"\n\ndim(df) \n\n\n[1] 4 3\n\nstr(df)\n\n\n'data.frame':   4 obs. of  3 variables:\n $ col_a: num  1 2 3 4\n $ col_b: num  5 6 7 8\n $ col_c: num  9 10 11 12\n\nLe colonne di un data.frame sono le variabili;\nLe variabili sono vettori.\nCiascuna riga del data.frame corrisponde ad un’osservazione (per esempio, un soggetto).\nEstraiamo da df la variabile col_a:\n\n\ndf$col_a\n\n\n[1] 1 2 3 4\n\nEsaminiamo la variabile col_a con più attenzione:\n\n\nlength(df$col_a) # length=numbero di righe/osservazioni\n\n\n[1] 4\n\nstr(df$col_a)\n\n\n num [1:4] 1 2 3 4\n\nChe cosa c’è da capire su vettori e data.frame?\nStruttura di base\nVettori: sono oggetti di type logical, integer, double, character.\ngli elementi di un vettore devono tutti appartere alla stessa classe type.\n\ndata.frame: sono collezioni di oggetti.\nGli elementi di un data.frame possono avere classi diverse tra loro (es., vettori numerici e vettori i cui elementi sono alfanumerici).\n\nBuona pratica: eseguire semplici diagnostiche su qualsiasi nuovo oggetto:\nlength() : quanti elementi ci sono in nell’oggetto?\ntypeof() : a che type di dati appatiene l’oggetto?\nstr() : mostra la struttura gerarchica dell’oggetto.\n\nSelezione di elementi\nR dispone di un sistema di notazione che consente di estrarre singoli elementi dagli oggetti. Per estrarre un valore da un data.frame, per esempio, dobbiamo scrivere il nome del data.frame seguito da una coppia di parentesi quadre:\n\ndf[ , ]\n\nAll’interno delle parentesi quadre ci sono due indici separati da una virgola. R usa il primo indice per selezionare un sottoinsieme di righe del data.frame e il secondo indice per selezionare un sottoinsieme di colonne. Ad esempio:\n\n\ndf\n\n\n  col_a col_b col_c\n1     1     5     9\n2     2     6    10\n3     3     7    11\n4     4     8    12\n\ndf[1, 1]\n\n\n[1] 1\n\ndf[, 1]\n\n\n[1] 1 2 3 4\n\ndf[2, ]\n\n\n  col_a col_b col_c\n2     2     6    10\n\ndf[1:2, 3]\n\n\n[1]  9 10\n\nPer estrarre un’intera colonna, è anche possibile usare l’operatore $ mediante la sintassi descritta in precedenza. Possiamo anche estrarre più di una colonna alla volta:\n\n\ndf[, c(\"col_b\", \"col_c\")]\n\n\n  col_b col_c\n1     5     9\n2     6    10\n3     7    11\n4     8    12\n\nSession Info:\n\nR version 3.6.3 (2020-02-29) Platform: x86_64-apple-darwin15.6.0 (64-bit) Running under: macOS Mojave 10.14.6\nMatrix products: default BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib\nlocale: [1] it_IT.UTF-8/it_IT.UTF-8/it_IT.UTF-8/C/it_IT.UTF-8/it_IT.UTF-8\nattached base packages: [1] stats graphics grDevices utils datasets methods\n[7] base\nloaded via a namespace (and not attached): [1] fansi_0.4.2 digest_0.6.27 R6_2.5.0\n[4] jsonlite_1.7.2 magrittr_2.0.1 evaluate_0.14\n[7] stringi_1.5.3 rlang_0.4.10 jquerylib_0.1.3\n[10] bslib_0.2.4 vctrs_0.3.6 rmarkdown_2.7.3\n[13] distill_1.2 tools_3.6.3 stringr_1.4.0\n[16] xfun_0.22 yaml_2.2.1 compiler_3.6.3\n[19] htmltools_0.5.1.9000 knitr_1.31 downlit_0.2.1\n[22] sass_0.3.1\n\n\n\n\n",
    "preview": "posts/2021-03-08-introduzione-a-r-1/preview.png",
    "last_modified": "2021-03-20T11:40:12+01:00",
    "input_file": {},
    "preview_width": 954,
    "preview_height": 1428
  },
  {
    "path": "posts/2021-03-08-introduzione-a-r-2/",
    "title": "Introduzione a R (2)",
    "description": "Il secondo post sulla sintassi di base di R.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-08",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\nFunzioni\nR offre la possibilità di utilizzare molteplici funzioni che permettono di svolgere svariate operazioni, più o meno complicate.\nSintassi di base\nPer esempio lancio di un dado può essere simulato da R con la funzione sample(). La funzione sample() prende due argomenti, il nome di un vettore e un numero chiamato size.\nLa funzione sample() ritorna un numero di elementi del vettore pari a size:\n\n\n# definiamo i valori che possono essere ottenuti dal lancio di un dado\ndie <- 1:6\ndie\n\n\n[1] 1 2 3 4 5 6\n\n# simuliamo il lancio di due dadi (o di due lanci di un dado)\nsample(die, 2)\n\n\n[1] 2 3\n\nCome si fa a sapere quanti e quali argomenti sono richiesti da una funzione? Tale informazione viene fornita dalla funzione args(). Per esempio,\n\n\nargs(sample)\n\n\nfunction (x, size, replace = FALSE, prob = NULL) \nNULL\n\nci informa che il primo argomento della funzione sample() è un vettore chiamato x, il secondo argomento è chiamato size ed ha il significato descritto sopra, il terzo argomento, replace, specifica se il campionamento è eseguito con o senza reimmissione, e il quarto argomento, prob, assegna delle probabilità agli elementi del vettore.\nSi noti che gli argomenti sono contenuti all’interno di parentesi tonde.\nIl significato degli argomenti viene spiegato nel file di help della funzione. Si noti che agli ultimi due argomenti sono stati assegnati dei valori, detti di default. Ciò significa che, se l’utilizzatore non li cambia, verranno usati da R. replace = FALSE significa che il campionamento viene eseguito senza reimmissione. Se desideriamo un campionamento con reimmissione, basta specificare replace = TRUE (nel caso di una singola estrazione è ovviamente irrilevante). Ad esempio, l’istruzione seguente simula i risultati di 10 lanci indipendenti di un dado:\n\n\nsample(die, 10, replace = TRUE)\n\n\n [1] 4 3 1 6 4 2 5 5 5 5\n\nLo stesso risultato può essere ottenuto in lanci diversi. Infine, prob = NULL specifica che non viene alterata la probabilità di estrazione degli elementi del vettore. Manipolando questo argomento è possibile simulare i risultati di un dado ``truccato’’, ovvero nel quale i numeri hanno probabilità diversa di essere osservati.\nIn generale, gli argomenti di una funzione, possono essere oggetti come vettori, matrici, altre funzioni, parametri o operatori logici.\nScrivere proprie funzioni\nAbbiamo visto in precedenza come sia possibile simulare il risultati prodotti dai lanci di due dadi. Possiamo replicare questo processo digitando ripetutamente le stesse istruzioni nella console. Otterremo ogni volta risultati diversi perché, ad ogni ripetizione, il generatore di numeri pseudo-casuali di R dipende dal valore ottenuto dal clock interno della macchina. Tuttavia, questa procedura è praticamente difficile da perseguire se il numero di ripetizioni è alto. In tal caso è vantaggioso scrivere una funzione contenente il codice che specifica il numero di ripetizioni. In questo modo, per trovare il risultato cercato basterà chiamare la funzione una sola volta.\nLe funzioni di R sono costituite da tre elementi: il nome, il blocco del codice e una serie di argomenti. Per creare una funzione è necessario immagazzinare in R questi tre elementi e function() consente di ottenere tale risultato usando la sintassi seguente:\n\n\nnome_funzione <- function(arg1, arg2, ...) {\n  espressione1\n  espressione2\n  return(risultato)\n}\n\n\n\nLa formattazione del codice mediante l’uso di spazi e rientri non è necessaria ma è altamente raccomandata per minimizzare la probabilità di compiere errori.\nUna chiamata di funzione è poi eseguita nel seguente modo:\n\n\nnome_funzione(arg1, arg2, ...)\n\n\n\nPer potere essere utilizzata, una funzione deve essere presente nella memoria di lavoro di R. Le funzioni salvate in un file possono essere richiamate utilizzando la funzione source(), ad esempio,\n\n\nsource(\"file_funzioni.R\")\n\n\n\nConsideriamo ora la funzione che ritorna la somma dei punti prodotti dal lancio di due dadi onesti:\n\n\nroll <- function () {\n  die <- 1:6\n  dice <- sample(die, size = 2, replace = TRUE) \n  return(sum(dice))\n}\n\n\n\nLa funzione roll() crea il vettore die che contiene sei elementi: i numeri da 1 a 6. La funzione sample() con l’argomento replace = TRUE e l’argomento size = 2 trova i numeri usciti dal lancio di due dadi. Tramite l’operatore di attribuzione questi due numeri sono immagazzinati nel vettore dice. La funzione sum() somma i gli elementi del vettore dice. Infine, la funzione return() (opzionale) ritorna il risultato trovato.\nInvocando la funzione roll() si ottiene dunque un punteggio che è uguale alla somma dei valori dei due dadi lanciati. In generale, si trova un risultato diverso ogni volta che la funzione viene usata. La funzione set.seed() ci permette di replicare esattamente i risultati della generazione di numeri casuali. Per ottenere questo risultato, basta assegnare al seed un numero arbitrario:\n\n\nset.seed(12345)\nroll()\n\n\n[1] 9\n\nPacchetti\nLe funzioni di R sono organizzate in pacchetti, i più importanti dei quali sono già disponibili quando si accede al programma.\nIstallazione e upgrade dei pacchetti\nAlcuni pacchetti non sono presenti nella release di base di R. Per installare un pacchetto non presente è sufficiente scrivere nella console:\n\n\ninstall.packages(\"nome_pacchetto\") \n\n\n\nAd esempio,\n\n\ninstall.packages(\"ggplot2\") \n\n\n\nLa prima volta che si usa questa funzione durante una sessione di lavoro si dovrà anche selezionare da una lista il sito mirror da cui scaricare il pacchetto. Il R Core Development Team lavora continuamente per migliorare le prestazioni di R, per correggere errori e per consentire l’uso di R con nuove tecnologie. Di conseguenza, periodicamente vengono rilasciate nuove versioni di R. Informazioni a questo proposito sono fornite sulla pagina web https://www.r-project.org/. Per installare una nuova versione di R si segue la stessa procedura che si è seguita per installare la versione corrente. Anche gli autori dei pacchetti periodicamente rilasciano nuove versioni dei loro pacchetti che contengono miglioramenti di varia natura. Per eseguire l’upgrade dei pacchetti ggplot2 e dplyr, ad esempio, si usa la seguente istruzione:\n\n\nupdate.packages(c(\"ggplot2\", \"dplyr\"))\n\n\n\nPer eseguire l’upgrade di tutti i pacchetti l’istruzione è\n\n\nupdate.packages()\n\n\n\nCaricare un pacchetto in R\nL’istallazione dei pacchetti non rende immediatamente disponibili le funzioni in essi contenute. L’istallazione di un pacchetto semplicemente copia il codice sul disco rigido della macchina in uso. Per potere usare le funzioni contenute in un pacchetto installato è necessario caricare il pacchetto in R. Ciò si ottiene con il comando:\n\n\nlibrary(\"ggplot2\")\n\n\n\nse si vuole caricare il pacchetto ggplot2. A questo punto diventa possibile usare le funzioni contenute in ggplot2.\nPer sapere quali sono i pacchetti già presenti nella release di R con cui si sta lavorando, basta digitare:\n\n\nsessionInfo()\n\n\nR version 3.6.3 (2020-02-29)\nPlatform: x86_64-apple-darwin15.6.0 (64-bit)\nRunning under: macOS Mojave 10.14.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] it_IT.UTF-8/it_IT.UTF-8/it_IT.UTF-8/C/it_IT.UTF-8/it_IT.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nloaded via a namespace (and not attached):\n [1] fansi_0.4.2          digest_0.6.27        R6_2.5.0            \n [4] jsonlite_1.7.2       magrittr_2.0.1       evaluate_0.14       \n [7] stringi_1.5.3        rlang_0.4.10         jquerylib_0.1.3     \n[10] bslib_0.2.4          vctrs_0.3.6          rmarkdown_2.7.3     \n[13] distill_1.2          tools_3.6.3          stringr_1.4.0       \n[16] xfun_0.21            yaml_2.2.1           compiler_3.6.3      \n[19] htmltools_0.5.1.9000 knitr_1.31           downlit_0.2.1       \n[22] sass_0.3.1          \n\nGiocare a poker con R\nPer fare un esempio che ci consenta di applicare le nozioni discusse in precedenza ad una situazione concreta, consideriamo il gioco del poker.\nDefiniamo un data.frame che codifica le carte di un mazzo di 52 carte:\n\n\ndeck <- data.frame(\n  face = c(\n  \"king\", \"queen\", \"jack\", \"ten\", \" nine\", \"eight\",\"seven\", \"six\", \n  \"five\", \"four\", \"three\", \"two \", \"ace\", \"king\", \"queen\", \"jack\", \n  \"ten\", \"nine\", \"eight\", \" seven\", \"six\", \"five\", \"four\", \"three\", \n  \"two\", \"ace\", \"king\" , \"queen\", \"jack\", \"ten\", \"nine\", \"eight\", \n  \"seven\", \"six\", \" five\", \"four\", \"three\", \"two\", \"ace\", \"king\", \n  \"queen\", \"jack \", \"ten\", \"nine\", \"eight\", \"seven\", \"six\", \"five\", \n  \"four\", \" three\", \"two\", \"ace\"\n  ), \n  suit = c(\n    \"spades\", \"spades\", \"spades\" , \"spades\", \"spades\", \"spades\", \n    \"spades\", \"spades\", \"spades\" , \"spades\", \"spades\", \"spades\", \n    \"spades\", \"clubs\", \"clubs\", \"clubs\", \"clubs\", \"clubs\", \"clubs\",\n    \"clubs\", \"clubs\", \"clubs \", \"clubs\", \"clubs\", \"clubs\", \"clubs\",\n    \"diamonds\", \"diamonds \", \"diamonds\", \"diamonds\", \"diamonds\",\n    \"diamonds\", \"diamonds \", \"diamonds\", \"diamonds\", \"diamonds\",\n    \"diamonds\", \"diamonds \", \"diamonds\", \"hearts\", \"hearts\", \n    \"hearts\", \"hearts\", \" hearts\", \"hearts\", \"hearts\", \"hearts\", \n    \"hearts\", \"hearts\", \" hearts\", \"hearts\", \"hearts\"\n  ), \n  value = c(\n    13, 12, 11, 10, 9, 8 , 7, 6, 5, 4, 3, 2, 1, 13, 12, 11, 10, \n    9, 8, 7, 6, 5, 4, 3, 2, 1, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, \n    3, 2, 1, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1\n  )\n)\n\n\n\nCosì facendo abbiamo ottenuto il seguente risultato:\n\n\ndeck\n\n\n     face      suit value\n1    king    spades    13\n2   queen    spades    12\n3    jack    spades    11\n4     ten    spades    10\n5    nine    spades     9\n6   eight    spades     8\n7   seven    spades     7\n8     six    spades     6\n9    five    spades     5\n10   four    spades     4\n11  three    spades     3\n12   two     spades     2\n13    ace    spades     1\n14   king     clubs    13\n15  queen     clubs    12\n16   jack     clubs    11\n17    ten     clubs    10\n18   nine     clubs     9\n19  eight     clubs     8\n20  seven     clubs     7\n21    six     clubs     6\n22   five    clubs      5\n23   four     clubs     4\n24  three     clubs     3\n25    two     clubs     2\n26    ace     clubs     1\n27   king  diamonds    13\n28  queen diamonds     12\n29   jack  diamonds    11\n30    ten  diamonds    10\n31   nine  diamonds     9\n32  eight  diamonds     8\n33  seven diamonds      7\n34    six  diamonds     6\n35   five  diamonds     5\n36   four  diamonds     4\n37  three  diamonds     3\n38    two diamonds      2\n39    ace  diamonds     1\n40   king    hearts    13\n41  queen    hearts    12\n42  jack     hearts    11\n43    ten    hearts    10\n44   nine    hearts     9\n45  eight    hearts     8\n46  seven    hearts     7\n47    six    hearts     6\n48   five    hearts     5\n49   four    hearts     4\n50  three    hearts     3\n51    two    hearts     2\n52    ace    hearts     1\n\nPoniamoci il problema di mescolare il mazzo di carte e di estrarre a caso alcune carte dal mazzo.\nL’istruzione\n\n\ndeck[1:52, ]\n\n\n     face      suit value\n1    king    spades    13\n2   queen    spades    12\n3    jack    spades    11\n4     ten    spades    10\n5    nine    spades     9\n6   eight    spades     8\n7   seven    spades     7\n8     six    spades     6\n9    five    spades     5\n10   four    spades     4\n11  three    spades     3\n12   two     spades     2\n13    ace    spades     1\n14   king     clubs    13\n15  queen     clubs    12\n16   jack     clubs    11\n17    ten     clubs    10\n18   nine     clubs     9\n19  eight     clubs     8\n20  seven     clubs     7\n21    six     clubs     6\n22   five    clubs      5\n23   four     clubs     4\n24  three     clubs     3\n25    two     clubs     2\n26    ace     clubs     1\n27   king  diamonds    13\n28  queen diamonds     12\n29   jack  diamonds    11\n30    ten  diamonds    10\n31   nine  diamonds     9\n32  eight  diamonds     8\n33  seven diamonds      7\n34    six  diamonds     6\n35   five  diamonds     5\n36   four  diamonds     4\n37  three  diamonds     3\n38    two diamonds      2\n39    ace  diamonds     1\n40   king    hearts    13\n41  queen    hearts    12\n42  jack     hearts    11\n43    ten    hearts    10\n44   nine    hearts     9\n45  eight    hearts     8\n46  seven    hearts     7\n47    six    hearts     6\n48   five    hearts     5\n49   four    hearts     4\n50  three    hearts     3\n51    two    hearts     2\n52    ace    hearts     1\n\nritorna tutte le righe e tutte e le colonne del data.frame deck. Le righe sono identificate dal primo indice che, nel caso presente, va da 1 a 52.\nPermutare in modo casuale l’indice delle righe equivale a mescolare il mazzo di carte. Per fare questo, utilizziamo la funzione sample() ponendo replace=FALSE e size uguale alla dimensione del vettore che contiene gli indici da 1 a 52:\n\n\nrandom <-\n  sample(\n    1:52,\n    size = 52, \n    replace = FALSE\n  )\nrandom\n\n\n [1] 16 26 28 24 29 11 32 49  2 22 47 38 39 30 10 17 46 40  1 12 20  8\n[23] 51 33  3  9 14 13 36 41 52 43 44 27  4 31 42  6 19  5 50 34 21 25\n[45] 45 37 48 15 23 35 18  7\n\nUtilizzando ora il vettore random di indici permutati otteniamo il risultato cercato:\n\n\ndeck_shuffled <- deck[random, ] \nhead(deck_shuffled)\n\n\n    face      suit value\n16  jack     clubs    11\n26   ace     clubs     1\n28 queen diamonds     12\n24 three     clubs     3\n29  jack  diamonds    11\n11 three    spades     3\n\nPossiamo ora scrivere una funzione che include le istruzioni precedenti:\n\n\nshuffle <- function(cards) {\n  random <- sample(1:52, size = 52, replace = FALSE)\n  return(cards[random, ])\n}\n\n\n\nInvocando la funzione shuffle() possiamo dunque generare un data.frame che rappresenta un mazzo di carte mescolato:\n\n\ndeck_shuffled <- shuffle(deck)\n\n\n\nSe immaginiamo di distribuire le carte di questo mazzo a due giocatori di poker, per il primo giocatore avremo:\n\n\ndeck_shuffled[c(1, 3, 5, 7, 9), ]\n\n\n    face     suit value\n10  four   spades     4\n27  king diamonds    13\n44  nine   hearts     9\n45 eight   hearts     8\n42 jack    hearts    11\n\ne per il secondo:\n\n\ndeck_shuffled[c(2, 4, 6, 8, 10), ]\n\n\n    face     suit value\n43   ten   hearts    10\n34   six diamonds     6\n35  five diamonds     5\n22  five   clubs      5\n23  four    clubs     4\n\nEsercizi\n1. Sia x = c(2, 1, 6, 4). Si calcoli la media di x.\nSi trovi la soluzione utilizzando le semplici operazioni algebriche di somma e divisione, specificando l’ordine corretto con il quale vengono eseguite le operazioni mediante le partentesi tonde.\nLa funzione mean() ritorna la media di un vettore. Si trovi la soluzione utilizzando la funzione mean().\nSi trovi la soluzione utilizzando le funzioni base di R, quando viene implementata la formula \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\). Per applicare questa formula, possiamo utilizzare le seguenti ulteriori funzioni R:\nlength(), la quale ritorna il numero degli elementi del vettore che viene passato come argomento alla funzione;\nsum(), la quale ritorna la somma degli elementi del vettore che viene passato come argomento alla funzione.\n2. Sia x = c(2, 1, 6, 4). Si scriva una funzione che calcola la media di x divisa per valore massimo di x. Si trovi il risultato cercato utilizzando la funzione così definita. In R, le funzioni min() e max() ritornano il minimo e il massimo del vettore che viene passato come argomento.\n3. Sia x = c(2, 1, 6, 4). Si verifichi che la somma degli scarti degli elementi del vettore x dalla media \\(\\bar{x}\\) è uguale a zero. [Suggerimento: si trovi il vettore di scargi di ciascun elemento x da \\(\\bar{x}\\). Si sommino gli elementi del vettore che contiene gli scarti dalla media.]\nSession Info:\n\nR version 3.6.3 (2020-02-29) Platform: x86_64-apple-darwin15.6.0 (64-bit) Running under: macOS Mojave 10.14.6\nMatrix products: default BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib\nlocale: [1] it_IT.UTF-8/it_IT.UTF-8/it_IT.UTF-8/C/it_IT.UTF-8/it_IT.UTF-8\nattached base packages: [1] stats graphics grDevices utils datasets methods\n[7] base\nloaded via a namespace (and not attached): [1] fansi_0.4.2 digest_0.6.27 R6_2.5.0\n[4] jsonlite_1.7.2 magrittr_2.0.1 evaluate_0.14\n[7] stringi_1.5.3 rlang_0.4.10 jquerylib_0.1.3\n[10] bslib_0.2.4 vctrs_0.3.6 rmarkdown_2.7.3\n[13] distill_1.2 tools_3.6.3 stringr_1.4.0\n[16] xfun_0.21 yaml_2.2.1 compiler_3.6.3\n[19] htmltools_0.5.1.9000 knitr_1.31 downlit_0.2.1\n[22] sass_0.3.1\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-08T08:43:42+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-08-introduzione-a-r-3/",
    "title": "Introduzione a R (3)",
    "description": "Il terzo post sulla sintassi di base di R: strutture di controllo.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-08",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\nIn R esistono strutture di controllo specifiche per regolare il flusso di esecuzione di un programma. Le istruzioni di controllo if … else consentono di selezionare tra diversi blocchi di codice. I loop permettono di ripetere ciclicamente blocchi di istruzioni per un numero prefissato di volte o fino a che una determinata condizione logica viene soddisfatta. Questo li rende utili per la programmazione di simulazioni numeriche.\nBlocchi di istruzioni\nUn blocco di istruzioni è formato da tutte le linee di programma che sono comprese entro una coppia di parentesi graffe, apera e chiusa. La formazione di un blocco serve principalmente per isolare un gruppo di istruzioni che costituiscono una parte ben definita di programma. La struttura generale di un blocco è la seguente:\n\n\n{\n  istruzioni\n}\n\n\n\nEsecuzione selettiva di blocchi\nÈ frequente scrivere programmi che eseguono azioni diverse in corrispondenza di condizioni diverse – il caso più ovvio è il valore di una particolare variabile. Situazioni del genere possono ben essere gestite mediante una successione di istruzioni if … else if … else.\nIn R l’istruzione condizionale if possiede la seguente sintassi:\n\nif (espressione) \n  istruzioni da eseguire se la espressione == TRUE\n\nAd esempio\n\n\nx <- 30\nif (x > 18) \n  print(\"il voto è sufficiente\")\n\n\n[1] \"il voto è sufficiente\"\n\n\n\nx <- 17\nif (x > 18) \n  print(\"il voto è sufficiente\")\n\n\n\nIn questo secondo caso, la condizione non è soddisfatta quindi non viene stampato nulla.\nCi possono però essere molteplici condizioni da valutare. Per fare questo usiamo la sintassi if () else if () else:\n\nif (espressione1) {\n  istruzioni da eseguire se la espressione1 == TRUE\n}\nelse if (espressione2) {\n  istruzioni da eseguire se la espressione2 == TRUE\n} else {\n  istruzioni da eseguire se nessuna delle espressioni è verificata\n}\n\nAd esempio,\n\n\ntemp_eval <- function(x) {\n  if (x > 40) {\n    \"è troppo caldo\"\n  } else if (x > 30) {\n    \"è molto caldo\"\n  } else if (x > 20) {\n    \"ottimo!\"\n  } else {\n    \"fa troppo freddo\"\n  }\n}\n\ntemp_eval(15)\n\n\n[1] \"fa troppo freddo\"\n\nFunzione if vettorializzata\nLa funzione if può essere applicata ad una sola condizione TRUE o FALSE. Che fare quando abbiamo a disposizione un vettore di valori logici? Di questo si occupa la funzione ifelse. Tale funzione prende tre argomenti: il primo argomento è una condizione da valutare; il secondo argomento è l’output se la condizione è vera; il terzo argomento specifica l’output se la condizione è fals. Se ad ifelse viene passato un vettore, allora la condizione del specificata dal primo argomento viene valutata per ciascun elemento del vettore. Si avrà in output un vettore della stessa lughezza di quello in input; ogni elemento di tale vettore sarà dato dalla scelta fatta da ifelse tra il secondo e terzo argomento, a seconda che l’espressione specificata nel primo argomento sia o meno verificata.\nPer esempio\n\n\nx <- 1:10\nifelse(x <= 5, \"valore piccolo\", \"valore grande\")\n\n\n [1] \"valore piccolo\" \"valore piccolo\" \"valore piccolo\"\n [4] \"valore piccolo\" \"valore piccolo\" \"valore grande\" \n [7] \"valore grande\"  \"valore grande\"  \"valore grande\" \n[10] \"valore grande\" \n\nIl ciclo for\nIl ciclo for è una struttura di controllo iterativa che opera sugli elementi di un vettore. Ha la seguente struttura di base:\n\n\nfor (indice in vettore) { \n  esegui_azione \n}\n\n\n\novvero, esegui_azione viene eseguito una volta per ciascun elemento di vettore, incrementando il valore di indice dopo ciascuna iterazione.\nPer esempio, il seguente ciclo for non fa altro che stampare il valore della variabile contatore i in ciascuna esecuzione del ciclo:\n\n\nfor (i in 1:3) { \n  print(i)\n}\n\n\n[1] 1\n[1] 2\n[1] 3\n\nUn esempio (leggermente) più complicato è il seguente:\n\n\nx <- seq(1, 9, by = 2) \nx\n\n\n[1] 1 3 5 7 9\n\nsum_x <- 0\nfor (i in seq_along(x)) {\n  sum_x <- sum_x + x[i]\n  cat(\"L'indice corrente e'\", i, \"\\n\")\n  cat(\"La frequenza cumulata e'\", sum_x, \"\\n\")\n}\n\n\nL'indice corrente e' 1 \nLa frequenza cumulata e' 1 \nL'indice corrente e' 2 \nLa frequenza cumulata e' 4 \nL'indice corrente e' 3 \nLa frequenza cumulata e' 9 \nL'indice corrente e' 4 \nLa frequenza cumulata e' 16 \nL'indice corrente e' 5 \nLa frequenza cumulata e' 25 \n\nUn esempio più complicato è il seguente. Chiadiamoci quanti numeri pari sono contenuti in un vettore. Per rispondere a questa domanda scriviamo la funzione count_even_numbers():\n\n\ncount_even_numbers <- function(x) {\n  count <- 0\n  for (i in seq_along(x)) {\n    if (x[i] %% 2 == 0) \n      count <- count + 1\n  }\n  count\n}\n\n\n\nNella funzione count_even_numbers() abbiamo inizializzato la variabile count a zero. Prima dell’esecuzione del ciclo for, dunque, count vale zero. Il ciclo for viene eseguito tante volte quanti sono gli elementi che costituiscono il vettore x. L’indice i dunque assume valori compresi tra 1 e il valore che corrisponde al numero di elementi di x. Infatti\n\n\nx\n\n\n[1] 1 3 5 7 9\n\nseq_along(x)\n\n\n[1] 1 2 3 4 5\n\nL’operazione modulo, indicato con %% dà come risultato il resto della divisione euclidea del primo numero per il secondo. Intuitivamente, la divisione euclidea è quell’operazione che si fa quando si suddivide un numero a di oggetti in gruppi di b oggetti ciascuno e si conta quanti gruppi sono stati formati e quanti oggetti sono rimasti. Per esempio, 9 %% 2 dà come risultato 1 perché questo è il resto della divisione [9/2] = 4, quindi 9 - (2 * 4) = 1 e dunque il resto è 1. L’operazione modulo dà come risultato 0 per tutti i numeri pari.\nIn ciascuna esecuzione del ciclo for l’operazione modulo viene eseguita, successivamente, su uno degli elementi di x. Se l’operazione modulo dà 0 come risultato, ovvero se il valore considerato è un numero pari, allora la variabile count viene incrementata di un’unità.\nL’ultima istruzione prima della parentesi graffa chiusa riporta ciò che viene ritornato dalla funzione.\nPer esempio:\n\n\ncount_even_numbers(x)\n\n\n[1] 0\n\n\n\ncount_even_numbers(c(x, 24))\n\n\n[1] 1\n\n\n\ncount_even_numbers(c(2, 4, 6, 8))\n\n\n[1] 4\n\nSimulazione del gioco d’azzardo Sopra-Sotto 7\nNel gioco d’azzardo con due dadi Sopra-Sotto 7 (Under-Over 7) vengono accettati tre tipi di scommesse:\nuna scommessa alla pari che il totale sarà inferiore a 7;\nuna scommessa alla pari che il totale sarà superiore a 7;\nuna scommessa che il totale sarà proprio 7, pagata 4 a 1.\nPoniamoci il problema di valutare se è conveniente la scommessa che il totale sarà 7 (vedremo in seguito come sia possibile risolvere questo problema in maniera formale, senza eseguire una simulazione). Iniziamo con il definire il vettore die che contiene ciascuno dei risultati possibili del lancio di un dado:\n\n\ndie <- c(1, 2, 3, 4, 5, 6) \ndie\n\n\n[1] 1 2 3 4 5 6\n\nLa funzione expand.grid() elenca tutte le possibili combinazioni degli elementi di 𝑛 vettori. Con essa possiamo dunque creare tutte le possibili combinazioni possibili con due dadi: dato che ogni dado ha sei facce, le combinazioni possibili sono 6 × 6 = 36:\n\n\nrolls <- expand.grid(die, die) \nrolls\n\n\n   Var1 Var2\n1     1    1\n2     2    1\n3     3    1\n4     4    1\n5     5    1\n6     6    1\n7     1    2\n8     2    2\n9     3    2\n10    4    2\n11    5    2\n12    6    2\n13    1    3\n14    2    3\n15    3    3\n16    4    3\n17    5    3\n18    6    3\n19    1    4\n20    2    4\n21    3    4\n22    4    4\n23    5    4\n24    6    4\n25    1    5\n26    2    5\n27    3    5\n28    4    5\n29    5    5\n30    6    5\n31    1    6\n32    2    6\n33    3    6\n34    4    6\n35    5    6\n36    6    6\n\nLa funzione expand.grid() può anche essere usata con più di due vettori; per esempio, potremmo usarla per elencare tutte le possibili combinazioni possibili con tre dadi, quattro dadi, ecc.\nCalcoliamo ora la somma dei due lanci. Per fare questo sommiamo due vettori, dato che la somma di vettori viene eseguita in R elemento per elemento:\n\n\nrolls$value <- rolls$Var1 + rolls$Var2 \nhead(rolls, n = 3)\n\n\n  Var1 Var2 value\n1    1    1     2\n2    2    1     3\n3    3    1     4\n\nLa somma è contenuta nella colonna value del data.frame rolls. Passiamo ora il data.frame rolls alla funzione roll_dice(). Questa fun- zione sceglie una riga a caso del data.frame e valuta il valore della variabile value. Se value (la somma dei due lanci) è uguale a 7, la funzione ritorna una vincita di 4; altrimenti ritorna una vincita di 0.\n\n\nroll_dice <- function(df) {\n  random <- sample(1:nrow(df), size = 1)\n  if (df[random, 3] == 7) {\n    return(4)\n  } else {\n    return(0)\n  }\n}\n\n\n\nSi noti che il valore del data.frame all’intero della funzione roll_dice è df, mentre, quando invochiamo la funzione usiamo come argomento di roll_dice l’oggetto rolls. Questo succede perché, quando eseguiamo l’istruzione roll_dice(rolls), comunichiamo ad R che vogliamo che venga eseguita la funzione roll_dice() e che passiamo alla funzione l’oggetto rolls. L’oggetto rolls è l’argomento che passiamo alla funzione roll_dice(). La funzione è stata definita in modo tale che il suo argomento, localmente, si chiama df. All’interno della funzione, dunque, le operazioni sull’argomento che è stato passato alla funzione verranno eseguite sull’oggetto df.\nQuesto significha, in generale, che gli oggetti che vengono manipolati all’interno di una funzione hanno un’esistenza locale, ovvero, non esistono al di fuori della funzione. Si dice: What Happens in Vegas, stays in Vegas. Lo stesso vale per le funzioni: qualsiasi cosa succeda dentro ad una funzione, fuori non si vede.\nUsiamo ora un ciclo for per ripetere 100,000 volte una scommessa unitaria.\n\n\nn_bets <- 1e5\noutcome <- rep(NA, n_bets)\nfor (i in 1:n_bets) {\n  outcome[i] <- roll_dice(rolls)\n}\n\n\n\nLo scalare n_bets specifica il numero di ripetizioni del ciclo for. L’istruzione\n\n\noutcome <- rep(NA, n_bets)\n\n\n\ncrea un vettore vuoto, chiamato outcome, dove verranno salvati i risultati calcolati ad ogni esecuzione del ciclo. Si noti l’utilizzo delle parentesi quadre. L’istruzione [i] significa che facciamo riferimento all’elemento i-esimo di outcome. La prima volta che si entra nel ciclo, il contatore ì vale 1. Dunque, la vincita della prima scommessa sarà salvata nel primo elemento del vettore outcome. Eseguite tutte le istruzioni contenute nel blocco del ciclo, il contatore assume il valore 2 e le istruzioni contenute nel corpo del ciclo vengono eseguite una seconda volta.\nLa funzione roll_dice() ritornerà la vincita (in generale, diversa da quella precedente) per una seconda scommessa e questo risultato verrà salvato nel secondo elemento del vettore outcome. Questo processo viene ripetuto n_bets volte. Il fatto che la variabile contatore assuma il valore finale previsto (nel nostro caso n_bets = 100000) è la condizione che fa terminare il ciclo.\nUna volta completata l’esecuzione del ciclo, il vettore outcome conterrà le medie di 100000 scommesse. La media di questi 100,000 numeri è quello che ci possiamo aspettare di guadagnare, o di perdere, per ogni scommessa unitaria. Se tale valore è uguale a 1, questo vuol dire che, a lungo andare, ci aspettiamo né di vincere né di perdere, ma di ricevere una somma uguale alla posta versata. Se tale valore è minore di 1 ci aspettiamo a lungo andare di perdere una proporzione della posta versata pari a 1 meno il valore trovato dalla simulazione. Se tale valore è maggiore di 1 ci aspettiamo a lungo andare di vincere una proporzione della posta versata pari al valore trovato dalla simulazione meno 1. Nel caso presente, il risultato della simulazione è 0.669 (contro un risultato teorico di 0.667). Ciò significa che, puntando 100 euro, a lungo andare ci aspettiamo di perdere 33.1 euro.\nEsercizi\n1. Utilizzando un ciclo for, si stampi il valore del contatore del ciclo, quando il ciclo for viene eseguito 10 volte. Si usi la funzione print() per stampare il risultato desiderato in ciscuna iterazione.\n2. Sia x = c(3, 1, 7, 9). Utilizzando un ciclo for, si crei un nuovo vettore y i cui elementi hanno un valore doppio rispetto a x.\n3. Si usi un ciclo for per stampare, ad ogni iterazione, il numero 1 un numero di volte uguale al contatore del ciclo. Per esempio, la terza volta che il ciclo viene eseguito va stampata la sequenza 1 1 1.\n\n\n\n4. Si carichi il data.frame iris fornito da R. Si trovi il numero di elementi distinti in ciascuna colonna del data.frame. Per trovare la soluzione si utilizzi un ciclo for. La soluzione è riportata qui sotto.\n[Suggerimento. I data.frame forniti da R si leggono nel workspace mediante la funzione data(). Per risolvere il problema dobbiamo prima definire un vettore vuoto dove salveremo i risultati. Poi dobbiamo trovare il modo per fare riferimento a ciascuna colonna del data.frame all’interno del ciclo for. Un modo per fare questo è di fare riferimento a ciascuna colonna i-esima del data.frame df con la sintassi df[[i]]. Infine, dobbiamo trovare un modo per contare il numero di elementi distinti di un vettore. La funzione unique() fa proprio questo.\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n          35           23           43           22            3 \n\nSession Info:\n\nR version 3.6.3 (2020-02-29) Platform: x86_64-apple-darwin15.6.0 (64-bit) Running under: macOS Mojave 10.14.6\nMatrix products: default BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib\nlocale: [1] it_IT.UTF-8/it_IT.UTF-8/it_IT.UTF-8/C/it_IT.UTF-8/it_IT.UTF-8\nattached base packages: [1] stats graphics grDevices utils datasets methods\n[7] base\nloaded via a namespace (and not attached): [1] fansi_0.4.2 digest_0.6.27 R6_2.5.0\n[4] jsonlite_1.7.2 magrittr_2.0.1 evaluate_0.14\n[7] stringi_1.5.3 rlang_0.4.10 jquerylib_0.1.3\n[10] bslib_0.2.4 vctrs_0.3.6 rmarkdown_2.7.3\n[13] distill_1.2 tools_3.6.3 stringr_1.4.0\n[16] xfun_0.21 yaml_2.2.1 compiler_3.6.3\n[19] htmltools_0.5.1.9000 knitr_1.31 downlit_0.2.1\n[22] sass_0.3.1\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-08T12:09:48+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-08-introduzione-a-r-4/",
    "title": "Introduzione a R (4)",
    "description": "Il quarto post sulla sintassi di base di R: input/output.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-08",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\nLeggere i dati\nCaricare i dati in R è una delle cose più importanti che si devono fare. Se non puoi importare i dati in R, allora non importa quali trucchetti potresti avere imparato: nulla può essere fatto. Importare i dati è anche una delle cose più frustranti - non solo in R, ma in generale. Se sei fortunato, i dati che stai cercando di caricare in R sono stati salvati come file .csv. L’estensione .csv sta per “Valori separati da virgola” e significa che i dati sono memorizzati in righe con virgole che separano le variabili. In questo caso, è facile importare i dati in R.\nOra, anche se sei fortunato e i tuoi dati sono in formato .csv, se sei un novizio del computer, allora avrai ancora qualche frustrazione nell’importare il file in R. Infatti, è necessario comunicare ad R dove sul computer è stato salvato il file da importare.\nÈ possibile risolvere tale problema specificando il percorso completo o il percorso relativo al file che si desidera caricare. In generale, è sempre meglio utilizzare un percorso relativo, in maniera tale che, se si copia la cartella di un progetto da un computer all’altro, tutto continua a funzionare.\nÈ una buona pratica impostare una determinata struttura di file con dati memorizzati in una cartella separata per ciascun progetto. Vediamo come si possa fare questo.\nCreare un progetto con RStudio\nCome indicato nell’esercizio 1 qui sotto, creiamo una cartella chiamata psicometria, dove salveremo tutti gli scritp Rmd che creeremo in questo corso. Una volta creata questa cartella, apriamo RStudio e creiamo un nuovo progetto. Per creare un progetto, da RStudio utilizziamo il percorso File/New Project... Quello che facciamo in questo modo è di comunicare ad R che questo specifico progetto è situato nella cartella di lavoro psicometria. L’indirizzo di questa cartella può essere ottenuto con l’istruzione getwd().\nPossiamo creare altre cartelle, dentro psicometria. Per esempio, creiamo da cartella data, dove salveremo i file di dati. Per fare un esercizio, utilizzeremo il file di dati wais.csv che può essere scaricato da Moodle. I dati in questo file corrispondono ai punteggi di quattro sottoscale della WAIS-IV completate da 90 studenti del corso di Psicometria dell’AA 2015/2016. Per fare un confronto, sappiamo che, nella popolazione, i punteggi di ciascuna sottoscala si distribuiscono normalmente con media 10 e deviazione standard 3.\nSalviamo questo file nella cartella data e poniamoci il problema di importare tali dati in R.\nImportare i dati in formato .csv\nPer leggere i dati contenuti in un file esterno, dobbiamo usare una funzione R capace di leggere i dati in quel formato e dobbiamo specificare dove si trova il file di dati che vogliamo importare. I dati sono conenuti del file wais.csv; dunque il file è in formato .csv. Per leggere i dati userò la funzione read_csv() e, per usare tale funzione, prima devo caricare il boundle di pacchetti tidyverse.\nDove si trova il file dei dati?\nOra si pone il problema di specificare dove si trova il file sul computer. È facile risolvere questo problema se abbiamo definito con R un progetto che identifica la cartella di lavoro che ci interessa.\nAvendo già creato il progetto psicometria, per specificare dove si trova il file che vogliamo importare possiamo utilizzare la funzione here::here(). Ovvero, rispetto alla cartella di lavoro che abbiamo definito, basta specificare in maniera relativa la cartella in cui abbiamo inserito il file. La sintassi da usare è la seguente:\n\n\nlibrary(\"here\")\nhere(\"data\", \"wais.csv\")\n\n\n[1] \"/Users/corrado/OneDrive - unifi.it/blog/data/wais.csv\"\n\nIn questo modo viene precisato l’indirizzo del file che voglio importare.\nPer importare i dati usiamo l’indirizzo specificato come indicato sopra:\n\n\nlibrary(\"tidyverse\")\ndf <- read_csv(here(\"data\", \"wais.csv\"))\n\n\n\nEsaminare i dati con glimpse()\nPossiamo esaminare il contenuto di df nel modo seguente:\n\n\nglimpse(df)\n\n\nRows: 90\nColumns: 7\n$ personal_code <chr> \"160996FMT784\", \"310795FMV569\", \"131096FSL143\"…\n$ MC            <dbl> 11, 9, 7, 5, 3, 7, 4, 8, 6, 13, 11, 9, 9, 9, 1…\n$ RA            <dbl> 11, 6, 11, 8, 9, 11, 7, 10, 3, 11, 9, 12, 9, 1…\n$ RS            <dbl> 12, 12, 12, 8, 9, 11, 12, 10, 14, 14, 11, 14, …\n$ CR            <dbl> 16, 14, 11, 10, 11, 13, 11, 14, 12, 17, 14, 12…\n$ wrkn_mem      <dbl> 22, 15, 18, 13, 12, 18, 11, 18, 9, 24, 20, 21,…\n$ pr_speed      <dbl> 28, 26, 23, 18, 20, 24, 23, 24, 26, 31, 25, 26…\n\nL’output di glimpse() ci comunica che ci sono 90 osservazioni e 7 variabili. I primi valori di ciascuna variabile vengono stampati. La funzione glimpse() ci dice anche quale classe di dati appartengono le variabili. Per esempio, personal_code appartiene alla classe char. Ciò vuol dire che le modalità di tale variabile sono delle stringhe alfanumeriche. Le altre varibili sono di type dbl, il che vuol dire che sono dei numeri reali.\nPossiamo anche esaminare i dati con la funzione head()\n\n\nhead(df)\n\n\n# A tibble: 6 x 7\n  personal_code    MC    RA    RS    CR wrkn_mem pr_speed\n  <chr>         <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n1 160996FMT784     11    11    12    16       22       28\n2 310795FMV569      9     6    12    14       15       26\n3 131096FSL143      7    11    12    11       18       23\n4 230195FER172      5     8     8    10       13       18\n5 190396FCP941      3     9     9    11       12       20\n6 170896FMO879      7    11    11    13       18       24\n\noppure con summary()\n\n\nsummary(df)\n\n\n personal_code            MC               RA              RS       \n Length:90          Min.   : 3.000   Min.   : 3.00   Min.   : 3.00  \n Class :character   1st Qu.: 8.000   1st Qu.: 8.00   1st Qu.: 9.00  \n Mode  :character   Median : 9.000   Median : 9.00   Median :11.00  \n                    Mean   : 8.793   Mean   : 9.39   Mean   :10.91  \n                    3rd Qu.:10.000   3rd Qu.:11.00   3rd Qu.:12.00  \n                    Max.   :14.000   Max.   :17.00   Max.   :19.00  \n                    NA's   :8        NA's   :8       NA's   :8      \n       CR           wrkn_mem        pr_speed    \n Min.   : 7.00   Min.   : 9.00   Min.   :14.00  \n 1st Qu.:11.00   1st Qu.:16.00   1st Qu.:20.00  \n Median :12.00   Median :18.00   Median :23.00  \n Mean   :12.34   Mean   :18.18   Mean   :23.26  \n 3rd Qu.:14.00   3rd Qu.:21.00   3rd Qu.:25.75  \n Max.   :17.00   Max.   :28.00   Max.   :36.00  \n NA's   :8       NA's   :8       NA's   :8      \n\nFormati diversi\nI dati possono essere in vari formati (Excel, SPSS, SAS, solo testo, RDS, …). Il pacchetto rio si pone l’obiettivo di semplificare il processo di importazione dei dati in R e l’esportazione dei dati da R. Dopo avere installato il pacchetto, install.packages(\"rio\"), lo carichiamo:\n\n\nlibrary(\"rio\")\n\n\n\nPer garantire che rio sia completamente funzionante, la prima volta che si usa rio eseguiamo la seguente istruzione:\n\n\ninstall_formats()\n\n\n\nA questo punto, importare i dati è molto semplice\n\n\ndf1 <- rio::import(here(\"data\", \"wais.csv\"))\n\n# confirm identical\nall.equal(df, df1, check.attributes = FALSE)\n\n\n[1] TRUE\n\nLeggiamo un file di dati in formato Excel:\n\n\ndf2 <- rio::import(here(\"data\", \"Jordan_etal.xls\"))\nglimpse(df2)\n\n\nRows: 747\nColumns: 50\n$ goodinfo           <dbl> 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,…\n$ condemn            <dbl> 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,…\n$ qAnswered          <dbl> 16, 16, 16, 16, 16, 16, 4, 16, 16, 4, 16,…\n$ totalCorrect       <dbl> 16, 16, 16, 16, 16, 16, 3, 16, 16, 2, 16,…\n$ percCorrect        <dbl> 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.75,…\n$ ansall             <dbl> 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,…\n$ dowork             <dbl> 4, 5, 2, 4, 3, 6, NA, 2, 2, NA, 1, NA, 4,…\n$ doromantic         <dbl> 2, 4, 1, 5, 5, 3, NA, 2, 2, NA, 1, NA, 4,…\n$ doacademic         <dbl> 2, 4, 5, 4, 3, 2, 3, 3, 2, NA, 2, NA, 4, …\n$ dodrugs            <dbl> 2, 5, 3, 3, 2, 3, NA, 3, 2, 4, 1, 4, 5, N…\n$ trustswork         <dbl> 5, 4, 5, 4, 5, 3, NA, 6, 6, NA, 7, NA, 5,…\n$ trustsromantic     <dbl> 3, 4, 6, 5, 4, 4, NA, 5, 5, NA, 7, NA, 5,…\n$ trustsacademic     <dbl> 5, 1, 5, 4, 5, 5, 5, 3, 5, NA, 5, NA, 6, …\n$ trustsdrugs        <dbl> 5, 5, 5, 4, 5, 5, NA, 5, 5, 3, 7, 4, 5, N…\n$ trustgwork         <dbl> 4, 4, 5, 4, 5, 2, NA, 5, 5, NA, 7, NA, 6,…\n$ trustgromantic     <dbl> 4, 5, 5, 4, 5, 2, NA, 5, 5, NA, 6, NA, 5,…\n$ trustgacademic     <dbl> 5, 3, 3, 4, 5, 5, 5, 3, 5, NA, 6, NA, 5, …\n$ trustgdrugs        <dbl> 5, 4, 3, 4, 5, 5, NA, 5, 5, 3, 5, 4, 4, N…\n$ likework           <dbl> 5, 4, 6, 4, 4, 3, NA, 5, 5, NA, 6, NA, 5,…\n$ likeromantic       <dbl> 3, 4, 7, 3, 5, 4, NA, 5, 5, NA, 7, NA, 5,…\n$ likeacademic       <dbl> 5, 4, 6, 4, 5, 6, 5, 4, 5, NA, 3, NA, 6, …\n$ likedrugs          <dbl> 5, 4, 4, 4, 4, 6, NA, 4, 5, 3, 6, 4, 4, N…\n$ compq1work         <dbl> 2, 1, 2, 1, 1, 1, NA, 2, 1, NA, 2, NA, 2,…\n$ compq1romantic     <dbl> 2, 1, 2, 1, 1, 1, NA, 2, 1, NA, 2, NA, 2,…\n$ compq1academic     <dbl> 2, 1, 2, 1, 1, 1, 2, 2, 1, NA, 2, NA, 2, …\n$ compq1drugs        <dbl> 2, 1, 2, 1, 1, 1, NA, 2, 1, 3, 2, 1, 2, N…\n$ compq2work         <dbl> 1, 1, 1, 1, 1, 1, NA, 1, 1, NA, 1, NA, 1,…\n$ compq2romantic     <dbl> 1, 1, 1, 1, 1, 1, NA, 1, 1, NA, 1, NA, 1,…\n$ compq2academic     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, NA, 1, …\n$ compq2drugs        <dbl> 1, 1, 1, 1, 1, 1, NA, 1, 1, 2, 1, 1, 1, N…\n$ compq3work         <dbl> 3, 3, 3, 3, 3, 3, NA, 3, 3, NA, 3, NA, 3,…\n$ compq3romantic     <dbl> 3, 3, 3, 3, 3, 3, NA, 3, 3, NA, 3, NA, 3,…\n$ compq3academic     <dbl> 3, 3, 3, 3, 3, 3, 1, 3, 3, NA, 3, NA, 3, …\n$ compq3drugs        <dbl> 3, 3, 3, 3, 3, 3, NA, 3, 3, 3, 3, 1, 3, N…\n$ compq4work         <dbl> 1, 2, 1, 1, 2, 2, NA, 2, 1, NA, 1, NA, 1,…\n$ compq4romantic     <dbl> 1, 2, 1, 1, 2, 2, NA, 2, 1, NA, 1, NA, 1,…\n$ compq4academic     <dbl> 1, 2, 1, 1, 2, 2, 2, 2, 1, NA, 1, NA, 1, …\n$ compq4drugs        <dbl> 1, 2, 1, 1, 2, 2, NA, 2, 1, 1, 1, 2, 1, N…\n$ sumdo              <dbl> 5.50, 3.50, 5.25, 4.00, 4.75, 4.50, 5.00,…\n$ sumtrusts          <dbl> 4.50, 3.50, 5.25, 4.25, 4.75, 4.25, 5.00,…\n$ sumtrustg          <dbl> 4.50, 4.00, 4.00, 4.00, 5.00, 3.50, 5.00,…\n$ sumlike            <dbl> 4.50, 4.00, 5.75, 3.75, 4.50, 4.75, 5.00,…\n$ sumtotal           <dbl> 4.7500, 3.7500, 5.0625, 4.0000, 4.7500, 4…\n$ age                <dbl> 47, NA, 35, 38, 24, NA, NA, 29, 34, NA, 2…\n$ gender             <dbl> 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1,…\n$ edu                <dbl> 6, NA, 5, 4, 5, NA, NA, 6, 4, NA, 6, NA, …\n$ income             <dbl> 1, NA, 7, 7, 3, NA, NA, 4, 5, NA, 7, NA, …\n$ generaltrust       <dbl> 5, NA, 6, 1, 5, NA, NA, 6, 5, NA, 5, NA, …\n$ reason             <chr> \"Tried to balance or qualify the facts an…\n$ previousexperience <dbl> 3, NA, 2, 3, 2, NA, NA, 4, 3, NA, 1, NA, …\n\nEsportare i dati\nÈ possibile salvare i dati che abbiamo manipolato in R in un file esterno.\n\n\nrio::export(df2, \"jordan.csv\")\nrio::export(df2, \"jordan.rds\")\n\n\n\nSe adesso guardiamo nella cartella di lavoro dove stiamo lavorando, vedremo che sono stati creati i file jordan.csv e jordan.rds.\nSession Info:\n\nR version 3.6.3 (2020-02-29) Platform: x86_64-apple-darwin15.6.0 (64-bit) Running under: macOS Mojave 10.14.6\nMatrix products: default BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib\nlocale: [1] it_IT.UTF-8/it_IT.UTF-8/it_IT.UTF-8/C/it_IT.UTF-8/it_IT.UTF-8\nattached base packages: [1] stats graphics grDevices utils datasets methods\n[7] base\nother attached packages: [1] rio_0.5.26 forcats_0.5.1 stringr_1.4.0 dplyr_1.0.5\n[5] purrr_0.3.4 readr_1.4.0 tidyr_1.1.3 tibble_3.1.0\n[9] ggplot2_3.3.3 tidyverse_1.3.0 here_1.0.1\nloaded via a namespace (and not attached): [1] Rcpp_1.0.6 lubridate_1.7.10 assertthat_0.2.1\n[4] rprojroot_2.0.2 digest_0.6.27 utf8_1.1.4\n[7] R6_2.5.0 cellranger_1.1.0 backports_1.2.1\n[10] reprex_1.0.0 evaluate_0.14 httr_1.4.2\n[13] pillar_1.5.1 rlang_0.4.10 curl_4.3\n[16] readxl_1.3.1 rstudioapi_0.13 data.table_1.14.0\n[19] jquerylib_0.1.3 rmarkdown_2.7.3 foreign_0.8-75\n[22] munsell_0.5.0 broom_0.7.5 compiler_3.6.3\n[25] modelr_0.1.8 xfun_0.21 pkgconfig_2.0.3\n[28] htmltools_0.5.1.9000 downlit_0.2.1 tidyselect_1.1.0\n[31] fansi_0.4.2 crayon_1.4.1 dbplyr_2.1.0\n[34] withr_2.4.1 grid_3.6.3 jsonlite_1.7.2\n[37] gtable_0.3.0 lifecycle_1.0.0 DBI_1.1.1\n[40] magrittr_2.0.1 scales_1.1.1 zip_2.1.1\n[43] cli_2.3.1 stringi_1.5.3 debugme_1.1.0\n[46] fs_1.5.0 xml2_1.3.2 bslib_0.2.4\n[49] ellipsis_0.3.1 generics_0.1.0 vctrs_0.3.6\n[52] openxlsx_4.2.3 distill_1.2 tools_3.6.3\n[55] glue_1.4.2 hms_1.0.0 yaml_2.2.1\n[58] colorspace_2.0-0 rvest_1.0.0 knitr_1.31\n[61] haven_2.3.1 sass_0.3.1\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-10T22:05:39+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-08-introduzione-a-r-5/",
    "title": "Introduzione a R (5)",
    "description": "Il quinto post sulla sintassi di base di R: `tidyverse`.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-08",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\nManipolazione dei dati\nSi chiamano “dati grezzi” quelli che provengono dal mondo circostanze, i dati raccolti per mezzo degli strumenti usati negli esperimenti, per mezzo di interviste, di questionari, ecc. Questi dati raramente vengono forniti con una struttura logica precisa. Per poterli elaborare mediante dei software dobbiamo prima trasformarli in maniera tale che abbiano una struttura logica organizzata. La struttura che solitamente si utilizza è quella tabellare (matrice dei dati), ovvero si dispongono i dati in una tabella nella quale a ciascuna riga corrisponde ad un’osservazione e ciascuna colonna corrisponde ad una variabile rilevata. In R una tale struttura è chiamata* data fram*e.\nIl pacchetto dplyr, che è al momento uno dei pacchetti più utilizzati per la gestione dei dati, offre una serie di funzionalità che consentono di ottenere il risultato descritto in precedenza e consente inoltre di eseguire le operazioni più comuni di manipolazione dei dati in maniera più semplice rispetto a quanto succeda quando usiamo le funzioni base di R.\ndplyr si fonda su cinque funzioni base:\nfilter(),\nselect(),\nmutate(),\narrange(),\nsummarise().\nA questi cinque comandi di base si aggiungono\nil pipe %>% che serve a concatenare più operazioni,\ngroup_by che viene utilizzato per il subsetting.\nIn particolare, considerando una matrice osservazioni per variabili (colonne), select() e mutate() si occupano di organizzare le variabili, filter() e arrange() i casi (righe), e summarise() i gruppi.\nPer introdurre le funzionalità di base di dplyr, usiamo i dati contenuti nel file sheffield.csv che può essere scaricato da Moodle. Carichiamo i pacchetti tidyverse e here:\n\n\nlibrary(\"tidyverse\")\nlibrary(\"here\")\n\n\n\nCreiamo l’oggetto df_sheffield leggendo da il file .csv nella working directory:\n\n\nlibrary(here)\ndf_sheffield <- read_csv(here(\"data\", \"sheffield.csv\"))\n\n\n\nCon l’istruzione here(\"data\", \"sheffield.csv\") ho specificato che, rispetto alla cartella di lavoro di default del progetto, il file si trova nella cartella data.\nI dati sono stati raccolti in una ricerca che ho eseguito un po’ di tempo fa sul problema della menzogna (Caudek, Lorenzino, & Liperoti, 2017). La teoria del carico cognitivo della menzogna ipotizza che mentire richieda un maggior carico cognitivo che dire la verità. In linea con questa ipotesi è stato dimostrato che le risposte menzognere sono tipicamente associate ad un innalzamento delle latenze di risposta rispetto alle risposte veritiere. Lo Sheffild Lie Test è una procedura comuterizzata che richiede ai soggetti di rispondere ad una serie di domande presentate sullo schermo in un ordine randomizzato. In metà delle prove ai soggetti viene chiesto di dire la verità e nell’altra metà di mentire. Il risultato tipico è che i tempi di reazione tendono ad essere maggiori quando ai soggetti viene chiesto di mentire piuttosto che di dire la verità.\nI dati contenuti nel file sheffield.csv riportano i tempi di reazione medi di 75 soggetti. Nel file sono contenute le seguenti variabili: lie indica se la consegna era di mentire o di dire la verità, self distingue le domande riferite al soggetto dalle domande riferite ad un estraneo, sex riporta il genere, age riporta l’età dei soggetti, pr_speed riporta i punteggi della scala WAIS relativa alla velocità di elaborazione, wrkn_mem riporta i punteggi della scala WAIS relativa alla memoria di lavoro, mrt riporta la media dei tempi di reazione.\nFiltrare le righe del data.frame con filter()\nLa funzione filter() consente di selezionare un sottoinsieme di osservazioni in un dataset. Per esempio, possiamo selezionare tutte le osservazioni nella variabile lie contrassegnate come yes e tutte le osservazioni nella variabile self contrassegnate come no:\n\n\ndf_sheffield %>% \n  dplyr::filter(lie == \"yes\", self == \"no\")\n\n\n# A tibble: 75 x 8\n      id lie   self  sex     age pr_speed wrkn_mem   mrt\n   <dbl> <chr> <chr> <chr> <dbl>    <dbl>    <dbl> <dbl>\n 1     1 yes   no    F        19       26       15 1454.\n 2     2 yes   no    F        20       23       18 1984.\n 3     3 yes   no    M        20       24       18 2760.\n 4     4 yes   no    F        19       23       11 1848.\n 5     5 yes   no    F        19       24       18 1810.\n 6     6 yes   no    F        19       26        9 2036.\n 7     7 yes   no    F        19       31       24 2292.\n 8     8 yes   no    F        20       25       20 1558.\n 9     9 yes   no    M        20       26       21 1589.\n10    10 yes   no    F        19       23       18 2136.\n# … with 65 more rows\n\nSelezionare le colonne del data.frame con select()\nLa funzione select() consente di selezionare un sottoinsieme di variabili in un dataset. Per esempio, possiamo selezionare solo le variabili id e mrt:\n\n\ndf_sheffield %>% \n  dplyr::select(id, mrt)\n\n\n# A tibble: 300 x 2\n      id   mrt\n   <dbl> <dbl>\n 1     1 1340.\n 2     1 1064.\n 3     1 1454.\n 4     1 1112.\n 5     2 1918.\n 6     2 1527.\n 7     2 1984.\n 8     2 1665.\n 9     3 2492.\n10     3 2186.\n# … with 290 more rows\n\nAggiungere una colonna al data.frame con mutate()\nTalvolta vogliamo creare una nuova variabile in uno stesso dataset ad esempio sommando o dividendo due variabili, oppure calcolandone la media. A questo scopo si usa la funzione mutate(). Per esempio, la somma dei punteggi di velocità di elaborazione pr_speed e di memoria di lavoro wrkn_mem della WAIS si trova come:\n\n\ndf <- df_sheffield %>%\n  mutate(\n    wais_sub = pr_speed + wrkn_mem\n  ) %>%\n  select(pr_speed, wrkn_mem, wais_sub)\nhead(df)\n\n\n# A tibble: 6 x 3\n  pr_speed wrkn_mem wais_sub\n     <dbl>    <dbl>    <dbl>\n1       26       15       41\n2       26       15       41\n3       26       15       41\n4       26       15       41\n5       23       18       41\n6       23       18       41\n\nOrdinare i dati con arrange()\nLa funzione arrange() serve a ordinare i dati in base ai valori di una o più variabili. Per esempio, possiamo ordinare la variabile mrt dal valore più alto al più basso in questo modo:\n\n\ndf_sheffield %>% \n  arrange(desc(mrt))\n\n\n# A tibble: 300 x 8\n      id lie   self  sex     age pr_speed wrkn_mem   mrt\n   <dbl> <chr> <chr> <chr> <dbl>    <dbl>    <dbl> <dbl>\n 1     3 yes   no    M        20       24       18 2760.\n 2     3 no    no    M        20       24       18 2492.\n 3     3 yes   yes   M        20       24       18 2389.\n 4    17 yes   no    F        23       21       15 2333.\n 5     7 yes   no    F        19       31       24 2292.\n 6    29 yes   no    F        19       22       18 2187.\n 7     3 no    yes   M        20       24       18 2186.\n 8    25 yes   no    F        19       28       19 2141.\n 9    10 yes   no    F        19       23       18 2136.\n10    73 yes   no    F        20       24       17 2082.\n# … with 290 more rows\n\nRaggruppare i dati con group_by()\nLa funzione group_by() serve a raggruppare insieme i valori in base a una o più variabili. La vedremo in uso in seguito insieme a summarise().\nSommario dei dati con summarise()\nLa funzione summarise() collassa il dataset in una singola riga dove viene riportato il risultato della statistica richiesta. Per esempio, la media dei tempi di reazione è\n\n\ndf_sheffield %>% \n  summarise(\n    y = mean(mrt, na.rm = TRUE) \n  )\n\n\n# A tibble: 1 x 1\n      y\n  <dbl>\n1 1443.\n\nCalcoliamo ora la media dei tempi di reazione in funzione delle variabili self e lie, ma solo per le femmine:\n\n\ndf_sheffield %>% \n  dplyr::filter(sex == \"F\") %>% \n  group_by(self, lie) %>% \n  summarise(\n    y = mean(mrt, na.rm = TRUE) \n  )\n\n\n# A tibble: 4 x 3\n# Groups:   self [2]\n  self  lie       y\n  <chr> <chr> <dbl>\n1 no    no    1445.\n2 no    yes   1694.\n3 yes   no    1197.\n4 yes   yes   1414.\n\nEsercizi\n1. Utilizziamo nuovamente i dati sulla menzogna discussi in precedenza.\nSi crei sul proprio computer una cartella chiamata psicometria.\nSi apra RStudio e si crei un nuovo progetto che ha psicometria come cartella di lavoro.\nNella cartella psicometria si creino delle altre cartelle. Una sarà chiamata data e in essa verranno salvati i file contenti i dati che utilizzeremo in questo corso. Altre cartelle avranno, ciascuna, il nome dell’argomento considerato. Ad esempio, se l’argomento trattato è probabilità potremmo creare una cartella con questo nome. Tuttavia, vi incoraggio a non usare mai caratteri speciali nel nome dei file e delle cartelle. Invece dello spazio potete usare il trattino basso. Gli accenti vanno eliminati. È più semplice (ed elegante) scrivere in inglese, così il problema non si pone. Quindi la cartella può essere chiamata probability. Considerate che R è sensibile alle maiuscole: probability e Probability non sono a stessa cosa!\nSi salvi il file sheffield.csv nella cartella data.\nUtilizzando la funzione here() del pacchetto here si specifichi dov’è il file sheffield.csv e si leggano i dati in R.\nSi generi un istogramma della variabile mrt che riporta la media dei tempi di reazione per ciascun soggetto in ciascuna condizione.\n2. Utilizzando gli stessi dati, si determini\nse in questo campione le femmine, in media, hanno valori più alti o più bassi dei maschi sulla variabile che codifica la velocità di elaborazione nella WAIS (pr_speed);\nse in questo campione le femmine, in media, hanno valori più alti o più bassi dei maschi sulla variabile che codifica la memoria di lavoro nella WAIS (wrkn_mem).\nSession Info:\n\nR version 3.6.3 (2020-02-29) Platform: x86_64-apple-darwin15.6.0 (64-bit) Running under: macOS Mojave 10.14.6\nMatrix products: default BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib\nlocale: [1] it_IT.UTF-8/it_IT.UTF-8/it_IT.UTF-8/C/it_IT.UTF-8/it_IT.UTF-8\nattached base packages: [1] stats graphics grDevices utils datasets methods\n[7] base\nother attached packages: [1] here_1.0.1 forcats_0.5.1 stringr_1.4.0 dplyr_1.0.4\n[5] purrr_0.3.4 readr_1.4.0 tidyr_1.1.2 tibble_3.1.0\n[9] ggplot2_3.3.3 tidyverse_1.3.0\nloaded via a namespace (and not attached): [1] tidyselect_1.1.0 xfun_0.21 bslib_0.2.4\n[4] haven_2.3.1 colorspace_2.0-0 vctrs_0.3.6\n[7] generics_0.1.0 htmltools_0.5.1.9000 yaml_2.2.1\n[10] utf8_1.1.4 rlang_0.4.10 jquerylib_0.1.3\n[13] pillar_1.5.1 withr_2.4.1 glue_1.4.2\n[16] DBI_1.1.1 dbplyr_2.1.0 modelr_0.1.8\n[19] readxl_1.3.1 lifecycle_1.0.0 munsell_0.5.0\n[22] gtable_0.3.0 cellranger_1.1.0 rvest_0.3.6\n[25] evaluate_0.14 knitr_1.31 fansi_0.4.2\n[28] broom_0.7.5 Rcpp_1.0.6 backports_1.2.1\n[31] scales_1.1.1 debugme_1.1.0 jsonlite_1.7.2\n[34] fs_1.5.0 distill_1.2 hms_1.0.0\n[37] digest_0.6.27 stringi_1.5.3 rprojroot_2.0.2\n[40] grid_3.6.3 cli_2.3.1 tools_3.6.3\n[43] magrittr_2.0.1 sass_0.3.1 crayon_1.4.1\n[46] pkgconfig_2.0.3 downlit_0.2.1 ellipsis_0.3.1\n[49] xml2_1.3.2 reprex_1.0.0 lubridate_1.7.9.2\n[52] rstudioapi_0.13 assertthat_0.2.1 rmarkdown_2.7.3\n[55] httr_1.4.2 R6_2.5.0 compiler_3.6.3\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-08T12:21:33+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-08-introduzione-a-r-6/",
    "title": "Introduzione a R (6)",
    "description": "Il sesto post sulla sintassi di base di R: `ggplot()`.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-08",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\nCreare grafici con ggplot2\nLa visualizzazione si pone l’obiettivo di rappresentare i dati tramite linee, forme, colori e così via. Esiste una relazione strutturata, una mappatura, tra le variabili nei dati e la loro rappresentazione nel grafico visualizzato sullo schermo o sulla pagina. La funzione ggplot() fornisce una serie di strumenti per mappare i dati sugli elementi visivi del grafico, per specificare il tipo di grafico che si desidera ottenere e per controllare in modo preciso i dettagli di come le informazioni verranno visualizzate. Un ottimo approfondimento delle tematiche della visualizzazione si può trovare in Data visualizazion: a practical introduction di Kieran Healy.\nIl pacchetto ggplot2 fornisce un’implementazione della cosiddetta grammatica della grafica (grammar of graphics) di Wilkinson. Tale grammatica consiste in uno schema generale da applicare alla visualizzazione dei dati e permette di organizzare un grafico attraverso la combinazione di componenti semantiche distinte, come oggetti geometrici, scale e coordinate. Per questa ragione, in ggplot2 un grafico è costituito da una serie di strati (layers).\nNella visualizzazione è necessario specificare le connessioni tra le variabili nei dati e i colori, i punti e le forme che vengono rappresentate sullo schermo. In ggplot(), queste connessioni logiche tra i dati e gli elementi del grafico sono chiamate mappature estetiche, o semplicemente estetiche, e vengono specificate dalla funzione aes().\nLa funzione da cui si parte per inizializzare un grafico è ggplot(). Tale funzione richiede due argomenti: il primo è il data frame che contiene i dati da visualizzare; il secondo è la funzione aes() che specifica come le variabili nei dati si associano logicamente alle mappature estetiche del grafico.\nLa funzione aes() richiede di specificare x e y, ovvero i nomi delle colonne del data frame che rappresentano le variabili da porre rispettivamente sugli assi orizzontale e verticale.\nÈ poi necessario specificare il tipo di grafico che vogliamo costruire (ad esempio un grafico a dispersione, un grafico a scatola o un grafico a barre), aggiungendo all’oggetto creato da ggplot() tutte le componenti necessarie. In ggplot() la tipologia del grafico da costruire è chiamata geom(). Funzioni del tipo geom_...() vengono quindi usate per aggiungere al livello di base barre, linee, punti, e così via. Nello specifico, la tipologia del grafico viene specificata dalle seguenti funzioni:\ngeom_bar(): crea un grafico a barre;\ngeom_point(): crea un diagramma a dispersione;\ngeom_line(): crea un layer con una linea retta;\ngeom_histogram(): crea un istogramma;\ngeom_boxplot(): crea un box-plot;\ngeom_errorbar(): crea un grafico a barre che rappresenta gli intervalli di confidenza.\nInfine, tramite altre funzioni, ad esempio labs(), sarà possibile definire i dettagli più fini.\nUna generica istruzione ha la seguente forma:\n\n\nmy_graph <- \n  ggplot(my_data, aes(x_var, y_var)) +\n  geom_...()\n\n\n\nLa prima volta che si usa il pacchetto ggplot2 è necessario installarlo. Per fare questo possiamo installare tidyverse che, oltre a contenere ggplot2, fornisce altre utili funzioni per l’analisi dei dati. Per attivare il pacchetto tidyverse si usa l’istruzione:\n\n\nlibrary(\"tidyverse\")\n\n\n\nOgni volta che si inizia una sessione R è necessario attivare i pacchetti che si vogliono usare, ma non è necessario istallarli una nuova volta.  \nDiagramma a dispersione con la funzione geom_point()\nA titolo eseplificativo utilizzeremo il dataset msleep fornito dal pacchetto ggplot2 che descrive le ore di sonno medie di 83 specie di mammiferi. Poniamoci il problema di rappresentare graficamente la relazione tra il numero medio di ore di sonno giornaliero (sleep_total) e il peso dell’animale (bodywt) (Savage e West, 2007).\nInnanzitutto carichiamo i dati in R:\n\n\ndata(msleep)\n\n\n\nGeneriamo un diagramma a dispersione con le impostazioni di default di ggplot2:\n\n\np <- msleep %>%\n  ggplot(\n    aes(x = bodywt, y = sleep_total)\n  ) +\n  geom_point()\n\nprint(p)\n\n\n\n\nTramite la mappatura estetica geom_point() viene creato un diagramma a dispersione.\nPer visualizzare maggiori informazioni, coloriamo in maniera diversa i punti che rappresentano animali carnivori, erbivori, ecc. Tale risultato si ottiene specificando l’argomento col = nella funzione aes():\n\n\np <- msleep %>%\n  ggplot(\n    aes(\n      x = bodywt, y = sleep_total, col = vore\n    )\n  ) +\n  geom_point()\nprint(p)\n\n\n\n\nÈ chiaro, anche senza fare alcuna analisi statistica, che la relazione tra le due variabili non è lineare. Trasformando in maniera logaritmica i valori dell’asse x la relazione si linearizza.\n\n\np <- msleep %>%\n  ggplot(\n    aes(\n      x = log(bodywt), y = sleep_total, col = vore\n    )\n  ) +\n  geom_point()\nprint(p)\n\n\n\n\nCambiamo ora il tema del grafico, aggiungiamo le etichette sugli assi e il titolo. Per cambiare il tema del grafico, usiamo il cowplot con l’istruzione theme_set():\n\n\nlibrary(\"cowplot\")\ntheme_set(theme_cowplot())\n\nmsleep %>%\n  ggplot(\n    aes(x = log(bodywt), y = sleep_total, col = vore)\n  ) +\n  geom_point(size = 2) +\n  theme(legend.title = element_blank()) +\n  labs(\n    x = \"Log Peso Corporeo\",\n    y = \"Totale Ore di Sonno\",\n    title = \"Il sonno in 83 specie di mammiferi\", subtitle = \"Savage e West (2007)\"\n  )\n\n\n\n\nIstogramma con la funzione geom_histogram()\nCreiamo ora un istogramma che rappresenta la distribuzione del (logaritmo del) peso medio del cervello delle 83 specie di mammiferi considerate da Savage e West (2007).\n\n\nmsleep %>%\n  ggplot(\n    aes(log(brainwt))\n  ) +\n  geom_histogram(aes(y = ..density..)) +\n  labs(\n    x = \"Log Peso Cervello\",\n    y = \"Frequenza Relativa\"\n  ) +\n  theme(legend.title = element_blank())\n\n\n\n\nL’argomento aes(y=..density..) in geom_histogram() produce le frequenze relative. L’opzione di default (senza questo argomento) porta ggplot() a rappresentare le frequenze assolute.\nEsercizio con dati WAIS\nUtilizzando i dati contenuti nel file wais.csv, l’obiettivo è di esaminare la distribuzine dei punteggi Ricerca di simboli, separatamente per maschi e femmine. Tali distribuzioni condizionate possono essere visualizzate con un box plot. Prima di fare questo, però, è necessario trasformare e ricodificare i dati.\nLeggere i dati\nI valori riportati corrispondono ai punteggi di quattro sottoscale della WAIS-IV completate da 72 studenti del corso di Psicometria dell’AA 2015/2016. Nella popolazione, i punteggi di ciascuna sottoscala si distribuiscono normalmente con media 10 e deviazione standard 3.\nIniziamo a leggere i dati in R. Sul mio computer, i dati wais.csv sono contenuti della cartella data.\n\n\nlibrary(\"here\")\nlibrary(\"tidyverse\")\n\ntemp <- read_csv(here(\"data\", \"wais.csv\"))\nsummary(temp)\n\n\n personal_code            MC               RA              RS       \n Length:90          Min.   : 3.000   Min.   : 3.00   Min.   : 3.00  \n Class :character   1st Qu.: 8.000   1st Qu.: 8.00   1st Qu.: 9.00  \n Mode  :character   Median : 9.000   Median : 9.00   Median :11.00  \n                    Mean   : 8.793   Mean   : 9.39   Mean   :10.91  \n                    3rd Qu.:10.000   3rd Qu.:11.00   3rd Qu.:12.00  \n                    Max.   :14.000   Max.   :17.00   Max.   :19.00  \n                    NA's   :8        NA's   :8       NA's   :8      \n       CR           wrkn_mem        pr_speed    \n Min.   : 7.00   Min.   : 9.00   Min.   :14.00  \n 1st Qu.:11.00   1st Qu.:16.00   1st Qu.:20.00  \n Median :12.00   Median :18.00   Median :23.00  \n Mean   :12.34   Mean   :18.18   Mean   :23.26  \n 3rd Qu.:14.00   3rd Qu.:21.00   3rd Qu.:25.75  \n Max.   :17.00   Max.   :28.00   Max.   :36.00  \n NA's   :8       NA's   :8       NA's   :8      \n\nElimino le osservazioni contenenti dati mancanti (codificati in R con NA).\n\n\ndf_wais <- temp[complete.cases(temp), ]\nsummary(df_wais)\n\n\n personal_code            MC               RA              RS       \n Length:82          Min.   : 3.000   Min.   : 3.00   Min.   : 3.00  \n Class :character   1st Qu.: 8.000   1st Qu.: 8.00   1st Qu.: 9.00  \n Mode  :character   Median : 9.000   Median : 9.00   Median :11.00  \n                    Mean   : 8.793   Mean   : 9.39   Mean   :10.91  \n                    3rd Qu.:10.000   3rd Qu.:11.00   3rd Qu.:12.00  \n                    Max.   :14.000   Max.   :17.00   Max.   :19.00  \n       CR           wrkn_mem        pr_speed    \n Min.   : 7.00   Min.   : 9.00   Min.   :14.00  \n 1st Qu.:11.00   1st Qu.:16.00   1st Qu.:20.00  \n Median :12.00   Median :18.00   Median :23.00  \n Mean   :12.34   Mean   :18.18   Mean   :23.26  \n 3rd Qu.:14.00   3rd Qu.:21.00   3rd Qu.:25.75  \n Max.   :17.00   Max.   :28.00   Max.   :36.00  \n\nPosso ora rimuovere l’oggetto temp:\n\n\nrm(temp)\n\n\n\nDefinisco la variabile ‘genere’\nDefinisco la variabile sex (genere) estraendo il settimo carattere da personal_code. Per fare questo uso la funzione substr(). La variabile sex viene poi trasformata in un fattore in quanto rappresenta una variabile qualitativa. Usando la funzione summary vediamo che ci sono 73 femmine e 9 maschi.\n\n\ndf_wais$sex <- substr(df_wais$personal_code, 7, 7)\ndf_wais$sex <- factor(df_wais$sex)\nsummary(df_wais$sex)\n\n\n F  M \n73  9 \n\nStatistiche descrittive condizionate alle modalità di una variabile categoriale\nEsaminiamo la media e la deviazione standard della variabile RS separatamente per maschi e femmine. Per fare questo, uso l’operatore “pipe” %>% che prende l’output di una funzione e lo trasforma nell’input della funzione successiva. Così, l’istruzione df_wais %>% significa: prendi il data frame df_wais (ovvero quello che viene prodotto quando la stringa df_wais viene digitata sullo schermo) e usalo come input per la funzione group_by. Tale funzione raggruppa i dati in funzione delle modalità della variabile sex, nel caso presente. Ovvero, tutte le manipolazioni che verranno effettuate sul data frame in seguito saranno fatte separatamente per le due modalità di sex. La pipe successiva passa questo input alla funzione summarise(), la quale consente di calcolare delle statistiche descrittive. Nel caso presente, la media mean() e la deviazione standard sd(), entrambe per la variabile RS. Nell’output che verrà prodotto, le medie dei punteggi RS per maschi e femmine costituiranno gli elementi di un vettore chiamato m; le deviazioni standard dei punteggi RS per maschi e femmine costituiranno gli elementi di un vettore chiamato std.\n\n\ndf_wais %>%\n  group_by(sex) %>%\n  summarise(\n    m = mean(RS),\n    std = sd(RS)\n  )\n\n\n# A tibble: 2 x 3\n  sex       m   std\n* <fct> <dbl> <dbl>\n1 F      10.8  2.87\n2 M      11.6  2.40\n\nBox plot con la funzione geom_boxplot()\nIl box plot fornisce una rappresetazione grafica della distribuzione di una variabile. In particolare, descrive visivamente la forma della distribuzione e la tendenza centrale (mediana). Nel grafico seguente viene riportato un boxplot separato per maschi e femmine. Usiamo ggplot() con la geom_boxplot():\n\n\np1 <- df_wais %>%\n  ggplot(\n    aes(x = sex, y = RS)\n  ) +\n  geom_boxplot()\nprint(p1)\n\n\n\n\nSovrapponiamo ai due boxplot i dati grezzi.\n\n\np1 <- df_wais %>%\n  ggplot(\n    aes(sex, RS)\n  ) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, col = \"gray46\")\nprint(p1)\n\n\n\n\nAggiungiamo le etichette a ciascun asse.\n\n\np1 <- df_wais %>%\n  ggplot(\n    aes(sex, RS)\n  ) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, col = \"gray46\") +\n  labs(\n    title = \"Sottoscala RS della WAIS-IV in funzione del genere\",\n    subtitle = \"Campione di studenti di Psicometria\",\n    x = \"Genere\",\n    y = \"Ricerca di Simboli\"\n  )\nprint(p1)\n\n\n\n\nPoniamoci ora il problema di modificare le modalità della variabile sex. Per fare questo uso la funzione fct_recode contenuta nel pacchetto forcats. Si noti che la modalità della variabile che vogliamo cambiare si trova a destra dell’uguale, tra virgolette; la nuova modalità si trova a sinistra del segno di uguale, anch’essa tra virgolette.\n\n\nlibrary(\"forcats\")\ndf_wais <- df_wais %>%\n  mutate(\n    sex = fct_recode(\n      sex,\n      \"Femmmine\" = \"F\",\n      \"Maschi\" = \"M\"\n    )\n  )\n\n\n\nRifaccio ora il grafico dopo avere modificato il data frame.\n\n\np1 <- df_wais %>%\n  ggplot(\n    aes(sex, RS)\n  ) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, col = \"gray46\") +\n  labs(\n    title = \"Sottoscala RS della WAIS-IV in funzione del genere\",\n    subtitle = \"Campione di studenti di Psicometria\",\n    x = \"Genere\",\n    y = \"Ricerca di Simboli\"\n  ) \n\nprint(p1)\n\n\n\n\nPossiamo colorare il boxplot usando l’argomento fill = nella funzione aes():\n\n\np1 <- df_wais %>%\n  ggplot(\n    aes(sex, RS, fill = sex)\n  ) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, col = \"gray46\") +\n  labs(\n    title = \"Sottoscala RS della WAIS-IV in funzione del genere\",\n    subtitle = \"Campione di studenti di Psicometria\",\n    x = \"Genere\",\n    y = \"Ricerca di Simboli\"\n  ) +\n  theme(legend.position = \"none\")\n\nprint(p1)\n\n\n\n\nSession Info:\n\nR version 3.6.3 (2020-02-29) Platform: x86_64-apple-darwin15.6.0 (64-bit) Running under: macOS Mojave 10.14.6\nMatrix products: default BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib\nlocale: [1] it_IT.UTF-8/it_IT.UTF-8/it_IT.UTF-8/C/it_IT.UTF-8/it_IT.UTF-8\nattached base packages: [1] stats graphics grDevices utils datasets methods\n[7] base\nother attached packages: [1] here_1.0.1 cowplot_1.1.1 forcats_0.5.1 stringr_1.4.0\n[5] dplyr_1.0.4 purrr_0.3.4 readr_1.4.0 tidyr_1.1.2\n[9] tibble_3.1.0 ggplot2_3.3.3 tidyverse_1.3.0\nloaded via a namespace (and not attached): [1] Rcpp_1.0.6 lubridate_1.7.9.2 assertthat_0.2.1\n[4] rprojroot_2.0.2 digest_0.6.27 utf8_1.1.4\n[7] R6_2.5.0 cellranger_1.1.0 backports_1.2.1\n[10] reprex_1.0.0 evaluate_0.14 httr_1.4.2\n[13] highr_0.8 pillar_1.5.1 rlang_0.4.10\n[16] readxl_1.3.1 rstudioapi_0.13 jquerylib_0.1.3\n[19] rmarkdown_2.7.3 labeling_0.4.2 munsell_0.5.0\n[22] broom_0.7.5 compiler_3.6.3 modelr_0.1.8\n[25] xfun_0.21 pkgconfig_2.0.3 htmltools_0.5.1.9000 [28] downlit_0.2.1 tidyselect_1.1.0 fansi_0.4.2\n[31] crayon_1.4.1 dbplyr_2.1.0 withr_2.4.1\n[34] grid_3.6.3 jsonlite_1.7.2 gtable_0.3.0\n[37] lifecycle_1.0.0 DBI_1.1.1 magrittr_2.0.1\n[40] scales_1.1.1 cli_2.3.1 stringi_1.5.3\n[43] debugme_1.1.0 farver_2.1.0 fs_1.5.0\n[46] xml2_1.3.2 bslib_0.2.4 ellipsis_0.3.1\n[49] generics_0.1.0 vctrs_0.3.6 distill_1.2\n[52] tools_3.6.3 glue_1.4.2 hms_1.0.0\n[55] yaml_2.2.1 colorspace_2.0-0 rvest_0.3.6\n[58] knitr_1.31 haven_2.3.1 sass_0.3.1\n\n\n\n\n",
    "preview": "posts/2021-03-08-introduzione-a-r-6/introduzione-a-r-6_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-03-08T12:27:20+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-03-08-istogramma/",
    "title": "Istogramma",
    "description": "La rappresentazione grafica della distribuzione dei dati.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-08",
    "categories": [
      "R",
      "Psicometria"
    ],
    "contents": "\nObiettivi di questo tutorial\nIn questo tutorial ci poniamo il problema di costruire un istrogramma utilizzando la funzione ggplot() del pacchetto ggplot2 in R. Vedremo quali sono i limiti degli istogrammi. Concluderemo introducendo una rappresentazione alternativa, la densità della frequenza dei dati, la quale attenua i limiti degli istogrammi.\nCarichiamo i pacchetti necessari\n\n\nsuppressPackageStartupMessages(library(\"tidyverse\")) \ntheme_set(bayesplot::theme_default(base_size=14))\nsuppressPackageStartupMessages(library(\"knitr\"))\nsuppressPackageStartupMessages(library(\"kableExtra\"))\nsuppressPackageStartupMessages(library(\"patchwork\"))\ntable_nums <- captioner::captioner(prefix = \"Tavola\")\nfigure_nums <- captioner::captioner(prefix = \"Figura\")\nknitr::opts_chunk$set(fig.align = \"center\", fig.width=7, fig.height=5)\n\n\n\nIntroduzione\nIniziamo con delle considerazioni di base.\nL’intervallo (a, b] si dice aperto a sinistra e chiuso a destra. Significa\n\\[\na < x \\leq b,\n\\]\novvero, descrive la situazione nella quale i valori che coincidono con il limite superiore dell’intervallo verranno inclusi nell’intervallo, mentre i valori che coincidono con il limte inferiore dell’intervanno non verranno inclusi nell’intervallo.\nConsideriamo questi dati:\n\n\nx <- c(1, 2, 1, 1, 5, 3, 2, 6)\n\n\n\nSupponiamo di volere i seguenti intervalli aperti a sinistra e chiusi a destra:\n(0, 2]\n(2, 4]\n(4, 6]\n(6, 8]\nPer la variabile x, la distribuzione di frequenze assolute diventa:\nIntervallo\nfrequenza\n(0, 2]\n5\n(2, 4]\n1\n(4, 6]\n2\n(6, 8]\n0\nSe invece consideriamo gli intervalli chiusi a sinistra e aperti a destra, [a, b), ovvero\n\\[\na \\leq x < b,\n\\]\nallora otteniamo una diversa distribuzione di frequenze assolute:\nIntervallo\nfrequenza\n[0, 2)\n3\n[2, 4)\n3\n[4, 6)\n1\n[6, 8)\n1\nUsiamo ora R per ottenere i risultati precedenti.\nUno dei modi possibili per calcolare le frequenze assolute è quello di usare la funzione cut(). Mediante tal funzione è possibile dividere il campo di variazione (ovvero, la differenza tra il valore massimo di una distribuzione ed il valore minimo) di una variabile continua x in intervalli e codificare ciascun valore x nei termini dell’intervallo a cui appartiene.\nIniziamo con gli intervalli aperti a sinistra e chiusi a destra:\n\n\nx_cat <- cut(\n  x,\n  breaks = c(0, 2, 4, 6, 8),\n  right = TRUE\n)\n\n\n\nPossiamo ora usare la funzione table() la quale ritorna una tabella con le frequenze assolute di ciascuna modalità della variabile in input.\n\n\ntable(x_cat)\n\n\nx_cat\n(0,2] (2,4] (4,6] (6,8] \n    5     1     2     0 \n\nLa distribuzione di frequenze per intervalli chiusi a sinistra e aperti a destra è:\n\n\nx_cat <- cut(\n  x,\n  breaks = c(0, 2, 4, 6, 8),\n  right = FALSE\n)\n\n\n\n\n\ntable(x_cat)\n\n\nx_cat\n[0,2) [2,4) [4,6) [6,8) \n    3     3     1     1 \n\nIstogramma\nCreiamo ora un istogramma usando i valori x.\nQual è l’altezza della barra in corrispondeza dell’intervallo (0,2]?\nLa base è pari a 2 e l’area è 5/8. Dunque l’altezza è\n\n\n(5/8) / 2\n\n\n[1] 0.3125\n\nUsiamo ggplot()\n\n\nx %>% \n  as.data.frame() %>% \n  ggplot(aes(x = x)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = c(0, 2, 4, 6, 8) \n  ) \n\n\n\n\nQuesto ci conferma che, di default, ggplot() usa intervalli chiusi a destra.\nCambiamo ora il default e specifichiamo intervalli chiusi a sinistra:\n\n\nx %>% \n  as.data.frame() %>% \n  ggplot(aes(x = x)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = c(0, 2, 4, 6, 8),\n    closed = \"left\"\n  ) \n\n\n\n\nManipolazione dei dati (importazione e pulizia)\nConsidereremo ora i dati di Zetsche, Bürkner, & Renneberg (2020) e ci poniamo il problema di descrivere la distribuzione dei punteggi BDI-II dei 67 partecipanti. Uno di essi non ha risposto e quindi c’è un dato mancante.\nCreiamo la varibile bdi che contiene i valori del valore BDI-II dei 66 soggetti:\n\n\nbysubj <- data.frame(\nbdi <- c(\n  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n  1,  1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3, 3,  5,  7,  9, 12, 19,\n  22, 22, 24, 25, 25, 26, 26, 26, 27, 27, 28, 28, 30, 30, 30, 31, 31, 33,\n  33, 34, 35, 35, 35, 36, 39, 41, 43, 43, 44\n  )\n)\n\n\n\nCalcolo delle frequenze assolute\nI seguenti cut-off vengono usati per interpretare il BDI‐II:\ndepressione minima = 0 – 13,\ndepressione lieve = 14 – 19,\ndepressione moderata = 20 – 28,\ndepressione severa = 29 – 63.\nCalcoliamo le frequenze assolute per i seguenti intervalli aperti a destra: [0, 13.5), [13.5, 19.5), [19.5, 28.5), [28.5, 63). Esaminando i dati, vediamo che 36 soggetti cadono nella prima classe. Dobbiamo però eseguire quest’operazione di conteggio utilizzando R.\n\n\nbysubj$bdi_level <- cut(\n  bysubj$bdi,\n  breaks = c(0, 13.5, 19.5, 28.5, 63),\n  include.lowest = TRUE,\n  labels = c(\n    \"minimal\", \"mild\", \"moderate\", \"severe\"\n  )\n)\n\nbysubj$bdi_level\n\n\n [1] minimal  minimal  minimal  minimal  minimal  minimal  minimal \n [8] minimal  minimal  minimal  minimal  minimal  minimal  minimal \n[15] minimal  minimal  minimal  minimal  minimal  minimal  minimal \n[22] minimal  minimal  minimal  minimal  minimal  minimal  minimal \n[29] minimal  minimal  minimal  minimal  minimal  minimal  minimal \n[36] minimal  mild     moderate moderate moderate moderate moderate\n[43] moderate moderate moderate moderate moderate moderate moderate\n[50] severe   severe   severe   severe   severe   severe   severe  \n[57] severe   severe   severe   severe   severe   severe   severe  \n[64] severe   severe   severe  \nLevels: minimal mild moderate severe\n\n\n\ntable(bysubj$bdi_level)\n\n\n\n minimal     mild moderate   severe \n      36        1       12       17 \n\nPer ottenere le frequenze relative è sufficiente dividere ciascuna frequenza assoluta per il numero totale di osservazioni:\n\n\ntable(bysubj$bdi_level) / sum(table(bysubj$bdi_level))\n\n\n\n   minimal       mild   moderate     severe \n0.54545455 0.01515152 0.18181818 0.25757576 \n\nIn questo modo abbiamo ottenuto le distribuzioni di frequenza assoluta e relativa.\nLimiti delle classi\nFrequenza assoluta\nFrequenza relativa\n[0, 13.5)\n36\n36/66\n[13.5, 19.5)\n1\n1/66\n[19.5, 28.5)\n12\n12/66\n[28.5, 63]\n17\n17/66\nIstogramma\nL’istogramma delle frequenze assolute disegna un rettangolo sopra ogni intervallo specificato, la cui altezza corrisponde alla frequenza assoluta della classe. Per esempio, alla classe [0, 13.5] abbiamo associato la frequenza assoluta di 36. Nell’istogramma delle frequenze assolute l’altezza del primo rettangolo sarà dunque uguale a 36.\nNell’istogramma delle frequenze relative viene invece rappresentata la frequenza relativa delle classi: l’area di ogni rettangolo è proporzionale alla frequenza relativa della classe. Come si trova l’altezza delle barre dell’istogramma in tali circostanze? Per la classe [0, 13.5), ad esempio, la frequenza relativa è 36/66. Tale valore corrisponde all’area del rettangolo. Dato che la base del rettangolo è 13.5, l’altezza sarà 36/66 / 13.5, ovvero {r 36/66 / 13.5}. E così via per le altre barre dell’istogramma.\nVisualizzazione con ggplot()\n\n\n\n\n\np1 <- bysubj %>% \n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = c(0, 13.5, 19.5, 28.5, 44.1) # il valore BDI-II massimo è 44\n  ) +\n  scale_x_continuous(breaks=c(0, 13.5, 19.5, 28.5, 44.1)) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequenza\"\n  )\np1\n\n\n\n\nFigure 1: Figura 1: Istogramma delle frequenze relative creato con ggplot().\n\n\n\nÈ più comune, però, utilizzare classi di ampiezza uguale.\n\n\n\n\n\np2 <- bysubj %>%\n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..),\n    breaks = seq(0, 44.1, length.out = 7)\n  ) +\n  scale_x_continuous(breaks=c(0.00,  7.35, 14.70, 22.05, 29.40, 36.75, 44.10)) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequenza\",\n    caption = \"Fonte: Zetsche, Buerkner, & Renneberg (2020)\"\n  )\n\n\n\n\n\np1 + p2\n\n\n\n\nLimite dell’istogramma\nCome abbiamo notato sopra, uno dei limiti degli istogrammi è che il profilo dell’istogramma è arbitrario: a seconda del numero e dei limiti delle classi che vengono scelte, cambiano sia il numero che la forma delle barre dell’istogramma.\nLa densità della frequenza dei dati\nIl problema precedente può essere alleviato utilizzando una rappresentazione alternativa della distribuzione di frequenza, ovvero la stima della densità della frequenza dei dati. Un modo semplice per pensare a tale rappresentazione, che in inglese va sotto il nome di density plot, è quello di immaginare un grande campione di dati, in modo che diventi possibile definire un enorme numero di classi di equivalenza di ampiezza molto piccola, le quali non risultino vuote. In tali circostanze, la funzione di densità empirica non è altro che il profilo `lisciato’ dell’istogramma. La stessa idea si applica anche quando il campione è più piccolo. Un esempio è fornito nella figura seguente.\n\n\n\n\n\np3 <- bysubj %>% \n  ggplot(aes(x = bdi)) +\n  geom_histogram(\n    aes(y = ..density..), \n    breaks = seq(0, 44.1, length.out = 7)\n  ) +\n  geom_density(\n    aes(x = bdi), \n    adjust = 0.5, \n    size = 0.8, \n    fill = \"steelblue3\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequenza\"\n  )\np3\n\n\n\n\nFigure 2: Figura 3: Istogramma delle frequenze relative con sovrapposta la stima della densità della frequenza dei dati.\n\n\n\nGuardando il grafico della densità della frequenza dei dati possiamo notare che ci sono due valori che tendono a ricorrere più spesso nella distribuzione dei punteggi del BDI-II. Il primo valore tipico è di poco superiore allo zero. Il secondo valore tipico è all’incirca uguale a 25. Tali valori tipici si chiamano mode. Nel caso presente è sensato che una moda della distribuzione corrisponda ad un valore BDI-II molto basso, dato che il campione include 36 soggetti sani, e che una moda corrisponda ad un valore BDI-II di depressione moderata/severa, in quanto il campione include 30 soggetti clinicamente depressi. Ovviamente, se vogliamo rappresentare solo la densità della frequenza dei dati (senza l’istogramma) procediamo come indicato qui sotto.\n\n\n\n\n\np4 <- bysubj %>% \n  ggplot(aes(x = bdi)) +\n  geom_density(\n    aes(x = bdi), \n    adjust = 0.5, \n    size = 0.8, \n    fill = \"steelblue3\", \n    alpha = 0.5\n  ) +\n  labs(\n    x = \"BDI-II\",\n    y = \"Densità di frequenza\",\n    caption = \"Fonte: Zetsche, Buerkner, & Renneberg (2020)\"\n  )\np4\n\n\n\n\nFigure 3: Figura 4: Grafico della stima della densità della frequenza dei dati.\n\n\n\n\n\np3 + p4\n\n\n\n\nConclusioni\nL’area totale di un istogramma è pari a 1.0.\nQuando guardiamo un istogramma dobbiamo pensare che l’area di ciascuna barra è uguale alla frequenza relativa (ovvero alla proporzione) dei casi che cadono in quella classe.\nL’istogramma ci dice come si distribuiscono (proporzionalmente) le osservazioni di un campione nelle classi che sono state definite.\nIl fatto che l’area totale dell’istogramma sia unitaria significa che essa è la somma di tutte le proporzioni rappresentate dalla barre dell’istogramma.\nIl grafico della funzione di densità empirica attenua l’arbitrarietà della scelta del numero e della dimensione delle classi dell’istogramma e ci fornisce una rappresentazione grafica maggiormente comprensibile.\nInformazioni sulla sessione di lavoro\n\nSession Info\n\nSono qui fornite le informazioni sulla sessione di lavoro insieme all’elenco dei pacchetti usati. I pacchetti contrassegnati con un asterisco(*) sono stati usati esplicitamente nello script.\n\n─ Session info ─────────────────────────────────────────────────────\n setting  value                       \n version  R version 3.6.3 (2020-02-29)\n os       macOS Mojave 10.14.6        \n system   x86_64, darwin15.6.0        \n ui       X11                         \n language (EN)                        \n collate  it_IT.UTF-8                 \n ctype    it_IT.UTF-8                 \n tz       Europe/Rome                 \n date     2021-03-21                  \n\n─ Packages ─────────────────────────────────────────────────────────\n package     * version    date       lib\n assertthat    0.2.1      2019-03-21 [1]\n backports     1.2.1      2020-12-09 [1]\n bayesplot     1.8.0      2021-01-10 [1]\n broom         0.7.5      2021-02-19 [1]\n bslib         0.2.4      2021-01-25 [1]\n captioner     2.2.3      2015-07-16 [1]\n cellranger    1.1.0      2016-07-27 [1]\n cli           2.3.1      2021-02-23 [1]\n colorspace    2.0-0      2020-11-11 [1]\n crayon        1.4.1      2021-02-08 [1]\n DBI           1.1.1      2021-01-15 [1]\n dbplyr        2.1.0      2021-02-03 [1]\n debugme       1.1.0      2017-10-22 [1]\n digest        0.6.27     2020-10-24 [1]\n distill       1.2        2021-01-13 [1]\n downlit       0.2.1      2020-11-04 [1]\n dplyr       * 1.0.5      2021-03-05 [1]\n ellipsis      0.3.1      2020-05-15 [1]\n evaluate      0.14       2019-05-28 [1]\n fansi         0.4.2      2021-01-15 [1]\n farver        2.1.0      2021-02-28 [1]\n fastmap       1.1.0      2021-01-25 [1]\n forcats     * 0.5.1      2021-01-27 [1]\n fs            1.5.0      2020-07-31 [1]\n generics      0.1.0      2020-10-31 [1]\n ggplot2     * 3.3.3      2020-12-30 [1]\n ggridges      0.5.3      2021-01-08 [1]\n glue          1.4.2      2020-08-27 [1]\n gtable        0.3.0      2019-03-25 [1]\n haven         2.3.1      2020-06-01 [1]\n highr         0.8        2019-03-20 [1]\n hms           1.0.0      2021-01-13 [1]\n htmltools     0.5.1.9000 2021-02-27 [1]\n httpuv        1.5.5      2021-01-13 [1]\n httr          1.4.2      2020-07-20 [1]\n jquerylib     0.1.3      2020-12-17 [1]\n jsonlite      1.7.2      2020-12-09 [1]\n kableExtra  * 1.3.4      2021-02-20 [1]\n knitr       * 1.31       2021-01-27 [1]\n labeling      0.4.2      2020-10-20 [1]\n later         1.1.0.1    2020-06-05 [1]\n lattice       0.20-41    2020-04-02 [1]\n lifecycle     1.0.0      2021-02-15 [1]\n lubridate     1.7.10     2021-02-26 [1]\n magrittr      2.0.1      2020-11-17 [1]\n mime          0.10       2021-02-13 [1]\n modelr        0.1.8      2020-05-19 [1]\n munsell       0.5.0      2018-06-12 [1]\n patchwork   * 1.1.1      2020-12-17 [1]\n pillar        1.5.1      2021-03-05 [1]\n pkgconfig     2.0.3      2019-09-22 [1]\n plyr          1.8.6      2020-03-03 [1]\n promises      1.2.0.1    2021-02-11 [1]\n purrr       * 0.3.4      2020-04-17 [1]\n R6            2.5.0      2020-10-28 [1]\n ragg          1.1.2      2021-03-17 [1]\n Rcpp          1.0.6      2021-01-15 [1]\n readr       * 1.4.0      2020-10-05 [1]\n readxl        1.3.1      2019-03-13 [1]\n reprex        1.0.0      2021-01-27 [1]\n rlang         0.4.10     2020-12-30 [1]\n rmarkdown     2.7.3      2021-03-06 [1]\n rstudioapi    0.13       2020-11-12 [1]\n rvest         1.0.0      2021-03-09 [1]\n sass          0.3.1      2021-01-24 [1]\n scales        1.1.1      2020-05-11 [1]\n sessioninfo   1.1.1      2018-11-05 [1]\n shiny         1.6.0      2021-01-25 [1]\n showtext      0.9-2      2021-01-10 [1]\n showtextdb    3.0        2020-06-04 [1]\n stringi       1.5.3      2020-09-09 [1]\n stringr     * 1.4.0      2019-02-10 [1]\n svglite       2.0.0      2021-02-20 [1]\n sysfonts      0.8.3      2021-01-10 [1]\n systemfonts   1.0.1      2021-02-09 [1]\n textshaping   0.3.3      2021-03-16 [1]\n thematic      0.1.1      2021-01-16 [1]\n tibble      * 3.1.0      2021-02-25 [1]\n tidyr       * 1.1.3      2021-03-03 [1]\n tidyselect    1.1.0      2020-05-11 [1]\n tidyverse   * 1.3.0      2019-11-21 [1]\n utf8          1.2.1      2021-03-12 [1]\n vctrs         0.3.6      2020-12-17 [1]\n viridisLite   0.3.0      2018-02-01 [1]\n webshot       0.5.2      2019-11-22 [1]\n withr         2.4.1      2021-01-26 [1]\n xfun          0.22       2021-03-11 [1]\n xml2          1.3.2      2020-04-23 [1]\n xtable        1.8-4      2019-04-21 [1]\n yaml          2.2.1      2020-02-01 [1]\n source                            \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n Github (rstudio/htmltools@ac43afe)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n Github (rstudio/rmarkdown@61db7a9)\n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.3)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.2)                    \n CRAN (R 3.6.0)                    \n CRAN (R 3.6.0)                    \n\n[1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library\n\n\n\n\n",
    "preview": "posts/2021-03-08-istogramma/preview.png",
    "last_modified": "2021-03-21T18:49:22+01:00",
    "input_file": {},
    "preview_width": 1020,
    "preview_height": 762
  },
  {
    "path": "posts/2021-03-07-bibliografia/",
    "title": "Le citazioni",
    "description": "Bibliografia della prova finale e della tesi di laurea magistrale.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-07",
    "categories": [
      "tesi"
    ],
    "contents": "\nHo creato un breve video con le istruzioni per creare la bibliografia delle tesi di laurea. Ci sono tanti tutorial sul web per affrontare questo problema: le semplici considerazioni che fornisco qui sono un possibile punto di partenza.\nAl di là delle cose che dico qui, consiglio fortemente tutti i laureandi, triennali e magistrali, di scrivere la tesi di laurea in R Markdown, usando le indicazioni fornite in un altro post e, soprattutto, di utilizzare bibtex per la bibliografia, in modo tale essere sicuri di ottenere il risultato corretto senza doversi preoccupare di applicare le (complicate) regole APA – i ricercatori fanno così quando scrivono un articolo.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-07T21:30:05+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-07-istruzioniprovafinalel24/",
    "title": "Istruzioni per la prova finale L-24",
    "description": "Tutto quello che avreste sempre voluto sapere sulla stesura dell'elaborato finale e non avete mai osato chiedere.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-07",
    "categories": [
      "tesi"
    ],
    "contents": "\nMediante questo link potete accedere ad un video in cui rispondo a tutte le possibili domande che potreste avere su questo argomento. Descriverò la procedura che consiglio per realizzare l’elaborato finale e per preparare la presentazione orale. Una volta scritto l’elaborato finale seguendo queste istruzioni, potete iniziare a lavorare alla presentazione orale. Sulla presentazione orale riceverete poi altri feedback negli incontri settimanali con i laureandi che saranno specificamente dedicati a questo tema.\n\n\n\n",
    "preview": "posts/2021-03-07-istruzioniprovafinalel24/preview.png",
    "last_modified": "2021-03-13T12:01:37+01:00",
    "input_file": {},
    "preview_width": 1034,
    "preview_height": 736
  },
  {
    "path": "posts/2021-03-07-pagina-facebook/",
    "title": "Pagina facebook",
    "description": "Social media.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-07",
    "categories": [
      "social media"
    ],
    "contents": "\nÈ attiva la pagina facebook del laboratorio. Contiene informazioni su progetti in corso e presentazioni dei laureandi. È una creazione dei torocinanti del laboratorio.\n\n\n\n",
    "preview": "posts/2021-03-07-pagina-facebook/preview.png",
    "last_modified": "2021-03-13T12:06:19+01:00",
    "input_file": {},
    "preview_width": 522,
    "preview_height": 840
  },
  {
    "path": "posts/2021-03-07-scrivere-la-tesi-con-r-markdown/",
    "title": "Scrivere la tesi con R Markdown",
    "description": "Video tutorial sull'uso di R Markdown per la tesi di laurea.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-07",
    "categories": [
      "R",
      "tesi"
    ],
    "contents": "\nSeguendo questo link potete trovare un video-tutorial sull’uso di R Markdown per la scrittura della tesi di laurea. Il materiale che ho predisposto può essere scaricato selezionando questo link. Buon lavoro!\n\n\n\n",
    "preview": "posts/2021-03-07-scrivere-la-tesi-con-r-markdown/preview.png",
    "last_modified": "2021-03-13T12:11:55+01:00",
    "input_file": {},
    "preview_width": 664,
    "preview_height": 644
  },
  {
    "path": "posts/welcome/",
    "title": "L'inizio",
    "description": "Benvenuti nel mio blog.",
    "author": [
      {
        "name": "Corrado Caudek",
        "url": "https://caudekblog.netlify.app"
      }
    ],
    "date": "2021-03-07",
    "categories": [
      "social media"
    ],
    "contents": "\nIl blog è scritto in R Markdown usando le funzioni del pacchetto distill. Ho seguito le istruzioni contenute in Building a blog with distill e Creating a Blog.\n\n\n\n",
    "preview": "posts/welcome/preview.png",
    "last_modified": "2021-03-13T12:14:13+01:00",
    "input_file": {},
    "preview_width": 596,
    "preview_height": 422
  }
]
